{"config":{"lang":["en","zh"],"separator":"[\\s\\u200b\\-]","pipeline":["stemmer"]},"docs":[{"location":"","title":"MultiMolecule","text":"<ul> <li>zyc date: 2024-05-04 00:00:00</li> </ul>"},{"location":"#multimolecule","title":"MultiMolecule","text":""},{"location":"#introduction","title":"Introduction","text":"<p>Welcome to MultiMolecule (\u200b\u6d66\u539f\u200b), a foundational library designed to accelerate Scientific Research with Machine Learning. MultiMolecule aims to provide a comprehensive yet flexible set of tools for researchers who wish to leverage AI in their work.</p> <p>We understand that AI4Science is a broad field, with researchers from different disciplines employing various practices. Therefore, MultiMolecule is designed with low coupling in mind, meaning that while it offers a full suite of functionalities, each module can be used independently. This allows you to integrate only the components you need into your existing workflows without adding unnecessary complexity. The key functionalities that MultiMolecule provides include:</p> <ul> <li><code>data</code>: Efficient data handling and preprocessing capabilities to streamline the ingestion and transformation of scientific datasets.</li> <li><code>modules</code>: Modular components designed to provide flexibility and reusability across various machine learning tasks.</li> <li><code>models</code>: State-of-the-art model architectures optimized for scientific research applications, ensuring high performance and accuracy.</li> <li><code>tokenisers</code>: Advanced tokenization methods to effectively handle complex scientific text and data representations.</li> <li><code>downstream</code>: Tools and utilities for the seamless integration of machine learning results into practical scientific workflows and applications.</li> <li><code>utils</code>: A collection of utility functions and tools to simplify common tasks and enhance the overall user experience.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Install the most recent stable version on PyPI:</p> Bash<pre><code>pip install multimolecule\n</code></pre> <p>Install the latest version from the source:</p> Bash<pre><code>pip install git+https://github.com/DLS5-Omics/MultiMolecule\n</code></pre>"},{"location":"#license","title":"License","text":"<p>We believe openness is the Foundation of Research.</p> <p>MultiMolecule is licensed under the GNU Affero General Public License.</p> <p>Please join us in building an open research community.</p> <p><code>SPDX-License-Identifier: AGPL-3.0-or-later</code></p>"},{"location":"models/","title":"models","text":"<ul> <li>Zhiyuan Chen date: 2024-05-04</li> </ul>"},{"location":"models/#models","title":"Models","text":"<p><code>models</code> provide a collection of pre-trained models.</p>"},{"location":"models/#usage","title":"Usage","text":""},{"location":"models/#direct-access","title":"Direct Access","text":"<p>All models can be directly loaded with the <code>from_pretrained</code> method.</p> Python<pre><code>from multimolecule.models import RnaFmModel, RnaTokenizer\n\nmodel = RnaFmModel.from_pretrained(\"multimolecule/rnafm\")\ntokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rnafm\")\n\nsequence = \"UAGCGUAUCAGACUGAUGUUG\"\noutput = model(**tokenizer(sequence, return_tensors=\"pt\"))\n</code></pre>"},{"location":"models/#build-with-transformersautomodels","title":"Build with <code>transformers.AutoModel</code>s","text":"<p>All models have been registered with <code>transformers.AutoModel</code>s, and can be directly loaded using corresponding <code>transformers.AutoModel</code>s classes.</p> Python<pre><code>from transformers import AutoModel, AutoTokenizer\n\nimport multimolecule  # noqa: F401\n\nmodel = AutoModel.from_pretrained(\"multimolecule/mrnafm\")\ntokenizer = AutoTokenizer.from_pretrained(\"multimolecule/mrnafm\")\n\nsequence = \"UAGCGUAUCAGACUGAUGUUG\"\noutput = model(**tokenizer(sequence, return_tensors=\"pt\"))\n</code></pre> <p><code>import multimolecule</code> before use</p> <p>Note that you must <code>import multimolecule</code> before building the model using <code>transformers.AutoModel</code>. The registration of models is done in the <code>multimolecule</code> package, and the models are not available in the <code>transformers</code> package.</p> <p>The following error will be raised if you do not <code>import multimolecule</code> before using <code>transformers.AutoModel</code>:</p> Python<pre><code>ValueError: The checkpoint you are trying to load has model type `rnafm` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n</code></pre>"},{"location":"models/#build-with-multimoleculeautomodels","title":"Build with <code>multimolecule.AutoModel</code>s","text":"<p>Similar to <code>transformers.AutoModel</code>s, MultiMolecule provides a set of <code>multimolecule.AutoModel</code>s for direct access to models.</p> <p>This includes:</p>"},{"location":"models/#multimoleculeautomodelfornucleotideclassification","title":"<code>multimolecule.AutoModelForNucleotideClassification</code>","text":"<p>Similar to <code>transformers.AutoModelForTokenClassification</code>, but removes the <code>&lt;bos&gt;</code> token and the <code>&lt;eos&gt;</code> token if they are defined in the model config.</p> <p><code>&lt;bos&gt;</code> and <code>&lt;eos&gt;</code> tokens</p> <p>In tokenizers provided by MultiMolecule, <code>&lt;bos&gt;</code> token is pointed to <code>&lt;cls&gt;</code> token, and <code>&lt;sep&gt;</code> token is pointed to <code>&lt;eos&gt;</code> token.</p> Python<pre><code>from transformers import AutoTokenizer\n\nfrom multimolecule import AutoModelForNucleotideClassification\n\nmodel = AutoModelForNucleotideClassification.from_pretrained(\"multimolecule/rnafm\")\ntokenizer = AutoTokenizer.from_pretrained(\"multimolecule/rnafm\")\n\nsequence = \"UAGCGUAUCAGACUGAUGUUG\"\noutput = model(**tokenizer(sequence, return_tensors=\"pt\"))\n</code></pre>"},{"location":"models/#initialize-a-vanilla-model","title":"Initialize a vanilla model","text":"<p>You can also initialize a vanilla model using the model class.</p> Python<pre><code>from multimolecule.models import RnaFmConfig, RnaFmModel, RnaTokenizer\n\nconfig = RnaFmConfig()\nmodel = RnaFmModel(config)\ntokenizer = RnaTokenizer()\n\nsequence = \"UAGCGUAUCAGACUGAUGUUG\"\noutput = model(**tokenizer(sequence, return_tensors=\"pt\"))\n</code></pre>"},{"location":"models/#available-models","title":"Available Models","text":""},{"location":"models/#rna","title":"RNA","text":"<ul> <li>RNABERT</li> <li>RNA-FM</li> <li>RNA-MSM</li> <li>SpliceBERT</li> <li>3UTRBERT</li> <li>UTR-LM</li> </ul>"},{"location":"models/#dna","title":"DNA","text":"<ul> <li>CaLM</li> </ul>"},{"location":"tokenisers/","title":"tokenisers","text":"<ul> <li>Zhiyuan Chen date: 2024-05-04</li> </ul>"},{"location":"tokenisers/#tokenisers","title":"Tokenisers","text":"<p><code>tokenisers</code> provide a collection of pre-defined tokenizers.</p> <p>A tokenizer is a class that converts a sequence of nucleotides or amino acids into a sequence of indices. It is used to pre-process the input sequence before feeding it into a model.</p> <p>Please refer to Tokenizer for more details.</p>"},{"location":"tokenisers/#available-tokenizers","title":"Available Tokenizers","text":"<ul> <li>RnaTokenizer</li> <li>DnaTokenizer</li> <li>ProteinTokenizer</li> </ul>"},{"location":"models/calm/","title":"CaLM","text":"<p>Pre-trained model on protein-coding DNA (cDNA) using a masked language modeling (MLM) objective.</p>"},{"location":"models/calm/#statement","title":"Statement","text":"<p>Codon language embeddings provide strong signals for use in protein engineering is published in Nature Machine Intelligence, which is a Closed Access / Author-Fee Journal.</p> <p>Machine learning has been at the forefront of the movement for free and open access to research.</p> <p>We see no role for closed access or author-fee publication in the future of machine learning research and believe the adoption of these journals as an outlet of record for the machine learning community would be a retrograde step.</p> <p>The MoltiMolecule team is committed to the principles of open access and open science.</p> <p>We do NOT endorse the publication of manuscripts on Closed Access / Author-Fee Journals and encourage the community to support Open Access Journals.</p> <p>Please consider signing the Statement on Nature Machine Intelligence.</p>"},{"location":"models/calm/#disclaimer","title":"Disclaimer","text":"<p>This is an UNOFFICIAL implementation of the Codon language embeddings provide strong signals for use in protein engineering by Carlos Outeiral and Charlotte M. Deane.</p> <p>The OFFICIAL repository of CaLM is at oxpig/CaLM.</p> <p>The team releasing CaLM did not write this model card for this model so this model card has been written by the MultiMolecule team.</p>"},{"location":"models/calm/#model-details","title":"Model Details","text":"<p>CaLM is a bert-style model pre-trained on a large corpus of protein-coding DNA sequences in a self-supervised fashion. This means that the model was trained on the raw nucleotides of DNA sequences only, with an automatic process to generate inputs and labels from those texts. Please refer to the Training Details section for more information on the training process.</p>"},{"location":"models/calm/#model-specification","title":"Model Specification","text":"Num Layers Hidden Size Num Heads Intermediate Size Num Parameters (M) FLOPs (G) MACs (G) Max Num Tokens 12 768 12 3072 85.75 22.36 11.17 1024"},{"location":"models/calm/#links","title":"Links","text":"<ul> <li>Code: multimolecule.calm</li> <li>Weights: multimolecule/calm</li> <li>Data: European Nucleotide Archive</li> <li>Paper: Codon language embeddings provide strong signals for use in protein engineering</li> <li>Developed by: Carlos Outeiral, Charlotte M. Deane</li> <li>Model type: BERT - ESM</li> <li>Original Repository: https://github.com/oxpig/CaLM</li> </ul>"},{"location":"models/calm/#usage","title":"Usage","text":"<p>The model file depends on the <code>multimolecule</code> library. You can install it using pip:</p> Bash<pre><code>pip install multimolecule\n</code></pre>"},{"location":"models/calm/#direct-use","title":"Direct Use","text":"<p>You can use this model directly with a pipeline for masked language modeling:</p> Python<pre><code>&gt;&gt;&gt; import multimolecule  # you must import multimolecule to register models\n&gt;&gt;&gt; from transformers import pipeline\n&gt;&gt;&gt; unmasker = pipeline('fill-mask', model='multimolecule/calm')\n&gt;&gt;&gt; unmasker(\"gcc&lt;mask&gt;cgctgacagccgcgg\")\n\n[{'score': 0.049356844276189804,\n  'token': 42,\n  'token_str': 'CGC',\n  'sequence': 'GCC CGC CGC UGA CAG CCG CGG'},\n {'score': 0.038425520062446594,\n  'token': 67,\n  'token_str': 'GGC',\n  'sequence': 'GCC GGC CGC UGA CAG CCG CGG'},\n {'score': 0.031837038695812225,\n  'token': 62,\n  'token_str': 'GCC',\n  'sequence': 'GCC GCC CGC UGA CAG CCG CGG'},\n {'score': 0.02501658722758293,\n  'token': 32,\n  'token_str': 'CAC',\n  'sequence': 'GCC CAC CGC UGA CAG CCG CGG'},\n {'score': 0.024257611483335495,\n  'token': 37,\n  'token_str': 'CCC',\n  'sequence': 'GCC CCC CGC UGA CAG CCG CGG'}]\n</code></pre>"},{"location":"models/calm/#downstream-use","title":"Downstream Use","text":""},{"location":"models/calm/#extract-features","title":"Extract Features","text":"<p>Here is how to use this model to get the features of a given sequence in PyTorch:</p> Python<pre><code>from multimolecule import RnaTokenizer, CaLmModel\n\n\ntokenizer = RnaTokenizer.from_pretrained('multimolecule/calm')\nmodel = CaLmModel.from_pretrained('multimolecule/calm')\n\ntext = \"GCCAGTCGCTGACAGCCGCGG\"\ninput = tokenizer(text, return_tensors='pt')\n\noutput = model(**input)\n</code></pre>"},{"location":"models/calm/#sequence-classification-regression","title":"Sequence Classification / Regression","text":"<p>Note: This model is not fine-tuned for any specific task. You will need to fine-tune the model on a downstream task to use it for sequence classification or regression.</p> <p>Here is how to use this model as backbone to fine-tune for a sequence-level task in PyTorch:</p> Python<pre><code>import torch\nfrom multimolecule import RnaTokenizer, CaLmForSequenceClassification\n\n\ntokenizer = RnaTokenizer.from_pretrained('multimolecule/calm')\nmodel = CaLmForSequenceClassification.from_pretrained('multimolecule/calm')\n\ntext = \"GCCAGTCGCTGACAGCCGCGG\"\ninput = tokenizer(text, return_tensors='pt')\nlabel = torch.tensor([1])\n\noutput = model(**input, labels=label)\n</code></pre>"},{"location":"models/calm/#nucleotide-classification-regression","title":"Nucleotide Classification / Regression","text":"<p>Note: This model is not fine-tuned for any specific task. You will need to fine-tune the model on a downstream task to use it for nucleotide classification or regression.</p> <p>Here is how to use this model as backbone to fine-tune for a nucleotide-level task in PyTorch:</p> Python<pre><code>import torch\nfrom multimolecule import RnaTokenizer, CaLmForNucleotideClassification\n\n\ntokenizer = RnaTokenizer.from_pretrained('multimolecule/calm')\nmodel = CaLmForNucleotideClassification.from_pretrained('multimolecule/calm')\n\ntext = \"GCCAGTCGCTGACAGCCGCGG\"\ninput = tokenizer(text, return_tensors='pt')\nlabel = torch.randint(2, (len(text), ))\n\noutput = model(**input, labels=label)\n</code></pre>"},{"location":"models/calm/#training-details","title":"Training Details","text":"<p>CaLM used Masked Language Modeling (MLM) as the pre-training objective: taking a sequence, the model randomly masks 25% of the tokens in the input then run the entire masked sentence through the model and has to predict the masked tokens. This is comparable to the Cloze task in language modeling.</p>"},{"location":"models/calm/#training-data","title":"Training Data","text":"<p>The CaLM model was pre-trained coding sequences of all organisms available on the European Nucleotide Archive (ENA). European Nucleotide Archive provides a comprehensive record of the world\u2019s nucleotide sequencing information, covering raw sequencing data, sequence assembly information and functional annotation.</p> <p>CaLM collected coding sequences of all organisms from ENA on April 2022, including 114,214,475 sequences. Only high level assembly information (dataclass CON) were used. Sequences matching the following criteria were filtered out:</p> <ul> <li>with unknown nucleotides (<code>N</code>, <code>Y</code>, <code>R</code>)</li> <li>start codon is not <code>ATG</code></li> <li>contains interstitial stop codons</li> <li>number of nucleotides is not a multiple of three</li> </ul> <p>To reduce redundancy, CaLM grouped the entries by organism, and apply CD-HIT (CD-HIT-EST) with a cut-off at 40% sequence identity to the translated protein sequences.</p> <p>The final dataset contains 9,858,385 cDNA sequences.</p> <p>Note that the alphabet in the original implementation is RNA instead of DNA, therefore, we use <code>RnaTokenizer</code> to tokenize the sequences. <code>RnaTokenizer</code> of <code>multimolecule</code> will convert \u201cU\u201ds to \u201cT\u201ds for you, you may disable this behaviour by passing <code>replace_T_with_U=False</code>.</p>"},{"location":"models/calm/#training-procedure","title":"Training Procedure","text":""},{"location":"models/calm/#preprocessing","title":"Preprocessing","text":"<p>CaLM used masked language modeling (MLM) as the pre-training objective. The masking procedure is similar to the one used in BERT:</p> <ul> <li>25% of the tokens are masked.</li> <li>In 80% of the cases, the masked tokens are replaced by <code>&lt;mask&gt;</code>.</li> <li>In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.</li> <li>In the 10% remaining cases, the masked tokens are left as is.</li> </ul>"},{"location":"models/calm/#pretraining","title":"Pretraining","text":"<p>The model was trained on 8 NVIDIA A100 GPUs with 80GiB memories.</p> <ul> <li>Learning rate: 1e-4</li> <li>Weight decay: 0.01</li> <li>Learning rate scheduler: inverse square root</li> <li>Learning rate warm-up: 10,000 steps</li> </ul>"},{"location":"models/calm/#citation","title":"Citation","text":"<p>BibTeX:</p> BibTeX<pre><code>@article {outeiral2022coodn,\n    author = {Outeiral, Carlos and Deane, Charlotte M.},\n    title = {Codon language embeddings provide strong signals for protein engineering},\n    elocation-id = {2022.12.15.519894},\n    year = {2022},\n    doi = {10.1101/2022.12.15.519894},\n    publisher = {Cold Spring Harbor Laboratory},\n    abstract = {Protein representations from deep language models have yielded state-of-the-art performance across many tasks in computational protein engineering. In recent years, progress has primarily focused on parameter count, with recent models{\\textquoteright} capacities surpassing the size of the very datasets they were trained on. Here, we propose an alternative direction. We show that large language models trained on codons, instead of amino acid sequences, provide high-quality representations that outperform comparable state-of-the-art models across a variety of tasks. In some tasks, like species recognition, prediction of protein and transcript abundance, or melting point estimation, we show that a language model trained on codons outperforms every other published protein language model, including some that contain over 50 times more parameters. These results suggest that, in addition to commonly studied scale and model complexity, the information content of biological data provides an orthogonal direction to improve the power of machine learning in biology.Competing Interest StatementThe authors have declared no competing interest.},\n    URL = {https://www.biorxiv.org/content/early/2022/12/19/2022.12.15.519894},\n    eprint = {https://www.biorxiv.org/content/early/2022/12/19/2022.12.15.519894.full.pdf},\n    journal = {bioRxiv}\n}\n</code></pre>"},{"location":"models/calm/#contact","title":"Contact","text":"<p>Please use GitHub issues of MultiMolecule for any questions or comments on the model card.</p> <p>Please contact the authors of the CaLM paper for questions or comments on the paper/model.</p>"},{"location":"models/calm/#license","title":"License","text":"<p>This model is licensed under the AGPL-3.0 License.</p> Text Only<pre><code>SPDX-License-Identifier: AGPL-3.0-or-later\n</code></pre>"},{"location":"models/calm/#multimolecule.models.calm","title":"<code>multimolecule.models.calm</code>","text":""},{"location":"models/calm/#multimolecule.models.calm.RnaTokenizer","title":"<code>RnaTokenizer</code>","text":"<p>               Bases: <code>Tokenizer</code></p> <p>Constructs an RNA tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>alphabet</code> <code>List[str] | None</code> <p>List of tokens to use. Defaults to IUPAC nucleotide code.</p> <code>None</code> <code>nmers</code> <code>int</code> <p>Size of nmers to tokenize. Defaults to 1.</p> <code>1</code> <code>codon</code> <code>bool</code> <p>Whether to tokenize into codons. Defaults to False.</p> <code>False</code> <code>replace_T_with_U</code> <code>bool</code> <p>Whether to replace T with U. Defaults to True.</p> <code>True</code> <code>do_upper_case</code> <code>bool</code> <p>Whether to convert input to uppercase. Defaults to True.</p> <code>True</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import RnaTokenizer\n&gt;&gt;&gt; tokenizer = RnaTokenizer()\n&gt;&gt;&gt; tokenizer('&lt;pad&gt;&lt;cls&gt;&lt;eos&gt;&lt;unk&gt;&lt;mask&gt;&lt;null&gt;ACGUNXVHDBMRWSYK.*-')[\"input_ids\"]\n[1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 2]\n&gt;&gt;&gt; tokenizer('acgu')[\"input_ids\"]\n[1, 6, 7, 8, 9, 2]\n&gt;&gt;&gt; tokenizer('acgt')[\"input_ids\"]\n[1, 6, 7, 8, 9, 2]\n&gt;&gt;&gt; tokenizer = RnaTokenizer(replace_T_with_U=False)\n&gt;&gt;&gt; tokenizer('acgt')[\"input_ids\"]\n[1, 6, 7, 8, 3, 2]\n&gt;&gt;&gt; tokenizer = RnaTokenizer(nmers=3)\n&gt;&gt;&gt; tokenizer('uagcuuauc')[\"input_ids\"]\n[1, 83, 17, 64, 49, 96, 84, 22, 2]\n&gt;&gt;&gt; tokenizer = RnaTokenizer(codon=True)\n&gt;&gt;&gt; tokenizer('uagcuuauc')[\"input_ids\"]\n[1, 83, 49, 22, 2]\n&gt;&gt;&gt; tokenizer('uagcuuauca')[\"input_ids\"]\nTraceback (most recent call last):\nValueError: length of input sequence  must be a multiple of 3 for codon tokenization, but got 10\n</code></pre> Source code in <code>multimolecule/tokenisers/rna/tokenization_rna.py</code> Python<pre><code>class RnaTokenizer(Tokenizer):\n    \"\"\"\n    Constructs an RNA tokenizer.\n\n    Args:\n        alphabet (List[str] | None, optional): List of tokens to use.\n            Defaults to [IUPAC nucleotide code](https://www.bioinformatics.org/sms2/iupac.html).\n        nmers (int, optional): Size of nmers to tokenize.\n            Defaults to 1.\n        codon (bool, optional): Whether to tokenize into codons.\n            Defaults to False.\n        replace_T_with_U (bool, optional): Whether to replace T with U.\n            Defaults to True.\n        do_upper_case (bool, optional): Whether to convert input to uppercase.\n            Defaults to True.\n\n    Examples:\n        &gt;&gt;&gt; from multimolecule import RnaTokenizer\n        &gt;&gt;&gt; tokenizer = RnaTokenizer()\n        &gt;&gt;&gt; tokenizer('&lt;pad&gt;&lt;cls&gt;&lt;eos&gt;&lt;unk&gt;&lt;mask&gt;&lt;null&gt;ACGUNXVHDBMRWSYK.*-')[\"input_ids\"]\n        [1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 2]\n        &gt;&gt;&gt; tokenizer('acgu')[\"input_ids\"]\n        [1, 6, 7, 8, 9, 2]\n        &gt;&gt;&gt; tokenizer('acgt')[\"input_ids\"]\n        [1, 6, 7, 8, 9, 2]\n        &gt;&gt;&gt; tokenizer = RnaTokenizer(replace_T_with_U=False)\n        &gt;&gt;&gt; tokenizer('acgt')[\"input_ids\"]\n        [1, 6, 7, 8, 3, 2]\n        &gt;&gt;&gt; tokenizer = RnaTokenizer(nmers=3)\n        &gt;&gt;&gt; tokenizer('uagcuuauc')[\"input_ids\"]\n        [1, 83, 17, 64, 49, 96, 84, 22, 2]\n        &gt;&gt;&gt; tokenizer = RnaTokenizer(codon=True)\n        &gt;&gt;&gt; tokenizer('uagcuuauc')[\"input_ids\"]\n        [1, 83, 49, 22, 2]\n        &gt;&gt;&gt; tokenizer('uagcuuauca')[\"input_ids\"]\n        Traceback (most recent call last):\n        ValueError: length of input sequence  must be a multiple of 3 for codon tokenization, but got 10\n    \"\"\"\n\n    model_input_names = [\"input_ids\", \"attention_mask\"]\n\n    def __init__(\n        self,\n        alphabet: List[str] | None = None,\n        nmers: int = 1,\n        codon: bool = False,\n        replace_T_with_U: bool = True,\n        bos_token: str = \"&lt;cls&gt;\",\n        cls_token: str = \"&lt;cls&gt;\",\n        pad_token: str = \"&lt;pad&gt;\",\n        eos_token: str = \"&lt;eos&gt;\",\n        sep_token: str = \"&lt;eos&gt;\",\n        unk_token: str = \"&lt;unk&gt;\",\n        mask_token: str = \"&lt;mask&gt;\",\n        additional_special_tokens: List | Tuple | None = None,\n        do_upper_case: bool = True,\n        **kwargs,\n    ):\n        if codon and nmers &gt; 1:\n            raise ValueError(\"Codon and nmers cannot be used together.\")\n        if codon:\n            nmers = 3  # set to 3 to get correct vocab\n        super().__init__(\n            alphabet=get_vocab_list(alphabet, nmers),\n            bos_token=bos_token,\n            cls_token=cls_token,\n            pad_token=pad_token,\n            eos_token=eos_token,\n            sep_token=sep_token,\n            unk_token=unk_token,\n            mask_token=mask_token,\n            additional_special_tokens=additional_special_tokens,\n            do_upper_case=do_upper_case,\n            **kwargs,\n        )\n        self.replace_T_with_U = replace_T_with_U\n        self.nmers = nmers\n        self.condon = codon\n\n    def _tokenize(self, text: str, **kwargs):\n        if self.do_upper_case:\n            text = text.upper()\n        if self.replace_T_with_U:\n            text = text.replace(\"T\", \"U\")\n        if self.condon:\n            if len(text) % 3 != 0:\n                raise ValueError(\n                    f\"length of input sequence  must be a multiple of 3 for codon tokenization, but got {len(text)}\"\n                )\n            return [text[i : i + 3] for i in range(0, len(text), 3)]\n        if self.nmers &gt; 1:\n            return [text[i : i + self.nmers] for i in range(len(text) - self.nmers + 1)]  # noqa: E203\n        return list(text)\n</code></pre>"},{"location":"models/calm/#multimolecule.models.calm.CaLmConfig","title":"<code>CaLmConfig</code>","text":"<p>               Bases: <code>PreTrainedConfig</code></p> <p>This is the configuration class to store the configuration of a [<code>CaLmModel</code>]. It is used to instantiate a CaLM model according to the specified arguments, defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of the CaLM oxpig/CaLM architecture.</p> <p>Configuration objects inherit from [<code>PreTrainedConfig</code>] and can be used to control the model outputs. Read the documentation from [<code>PreTrainedConfig</code>] for more information.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_size</code> <code>`int`, *optional*, defaults to 131</code> <p>Vocabulary size of the CaLM model. Defines the number of different tokens that can be represented by the <code>inputs_ids</code> passed when calling [<code>CaLmModel</code>].</p> <code>131</code> <code>hidden_size</code> <code>`int`, *optional*, defaults to 768</code> <p>Dimensionality of the encoder layers and the pooler layer.</p> <code>768</code> <code>num_hidden_layers</code> <code>`int`, *optional*, defaults to 12</code> <p>Number of hidden layers in the Transformer encoder.</p> <code>12</code> <code>num_attention_heads</code> <code>`int`, *optional*, defaults to 12</code> <p>Number of attention heads for each attention layer in the Transformer encoder.</p> <code>12</code> <code>intermediate_size</code> <code>`int`, *optional*, defaults to 3072</code> <p>Dimensionality of the \u201cintermediate\u201d (often named feed-forward) layer in the Transformer encoder.</p> <code>3072</code> <code>hidden_dropout</code> <code>`float`, *optional*, defaults to 0.1</code> <p>The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.</p> <code>0.1</code> <code>attention_dropout</code> <code>`float`, *optional*, defaults to 0.1</code> <p>The dropout ratio for the attention probabilities.</p> <code>0.1</code> <code>max_position_embeddings</code> <code>`int`, *optional*, defaults to 1026</code> <p>The maximum sequence length that this model might ever be used with. Typically set this to something large just in case (e.g., 512 or 1024 or 2048).</p> <code>1026</code> <code>initializer_range</code> <code>`float`, *optional*, defaults to 0.02</code> <p>The standard deviation of the truncated_normal_initializer for initializing all weight matrices.</p> <code>0.02</code> <code>layer_norm_eps</code> <code>`float`, *optional*, defaults to 1e-12</code> <p>The epsilon used by the layer normalization layers.</p> <code>1e-12</code> <code>position_embedding_type</code> <code>`str`, *optional*, defaults to `\"absolute\"`</code> <p>Type of position embedding. Choose one of <code>\"absolute\"</code>, <code>\"relative_key\"</code>, <code>\"relative_key_query\", \"rotary\"</code>. For positional embeddings use <code>\"absolute\"</code>. For more information on <code>\"relative_key\"</code>, please refer to Self-Attention with Relative Position Representations (Shaw et al.). For more information on <code>\"relative_key_query\"</code>, please refer to Method 4 in Improve Transformer Models with Better Relative Position Embeddings (Huang et al.).</p> <code>'rotary'</code> <code>is_decoder</code> <code>`bool`, *optional*, defaults to `False`</code> <p>Whether the model is used as a decoder or not. If <code>False</code>, the model is used as an encoder.</p> required <code>use_cache</code> <code>`bool`, *optional*, defaults to `True`</code> <p>Whether or not the model should return the last key/values attentions (not used by all models). Only relevant if <code>config.is_decoder=True</code>.</p> <code>True</code> <code>emb_layer_norm_before</code> <code>`bool`, *optional*</code> <p>Whether to apply layer normalization after embeddings but before the main stem of the network.</p> <code>False</code> <code>token_dropout</code> <code>`bool`, defaults to `False`</code> <p>When this is enabled, masked tokens are treated as if they had been dropped out by input dropout.</p> <code>True</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import CaLmModel, CaLmConfig\n</code></pre> Python Console Session<pre><code>&gt;&gt;&gt; # Initializing a CaLM multimolecule/calm style configuration\n&gt;&gt;&gt; configuration = CaLmConfig()\n</code></pre> Python Console Session<pre><code>&gt;&gt;&gt; # Initializing a model (with random weights) from the multimolecule/calm style configuration\n&gt;&gt;&gt; model = CaLmModel(configuration)\n</code></pre> Python Console Session<pre><code>&gt;&gt;&gt; # Accessing the model configuration\n&gt;&gt;&gt; configuration = model.config\n</code></pre> Source code in <code>multimolecule/models/calm/configuration_calm.py</code> Python<pre><code>class CaLmConfig(PreTrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`CaLmModel`]. It is used to instantiate a CaLM\n    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n    defaults will yield a similar configuration to that of the CaLM\n    [oxpig/CaLM](https://github.com/oxpig/CaLM) architecture.\n\n    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n    documentation from [`PreTrainedConfig`] for more information.\n\n\n    Args:\n        vocab_size (`int`, *optional*, defaults to 131):\n            Vocabulary size of the CaLM model. Defines the number of different tokens that can be represented by the\n            `inputs_ids` passed when calling [`CaLmModel`].\n        hidden_size (`int`, *optional*, defaults to 768):\n            Dimensionality of the encoder layers and the pooler layer.\n        num_hidden_layers (`int`, *optional*, defaults to 12):\n            Number of hidden layers in the Transformer encoder.\n        num_attention_heads (`int`, *optional*, defaults to 12):\n            Number of attention heads for each attention layer in the Transformer encoder.\n        intermediate_size (`int`, *optional*, defaults to 3072):\n            Dimensionality of the \"intermediate\" (often named feed-forward) layer in the Transformer encoder.\n        hidden_dropout (`float`, *optional*, defaults to 0.1):\n            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n        attention_dropout (`float`, *optional*, defaults to 0.1):\n            The dropout ratio for the attention probabilities.\n        max_position_embeddings (`int`, *optional*, defaults to 1026):\n            The maximum sequence length that this model might ever be used with. Typically set this to something large\n            just in case (e.g., 512 or 1024 or 2048).\n        initializer_range (`float`, *optional*, defaults to 0.02):\n            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n        layer_norm_eps (`float`, *optional*, defaults to 1e-12):\n            The epsilon used by the layer normalization layers.\n        position_embedding_type (`str`, *optional*, defaults to `\"absolute\"`):\n            Type of position embedding. Choose one of `\"absolute\"`, `\"relative_key\"`, `\"relative_key_query\", \"rotary\"`.\n            For positional embeddings use `\"absolute\"`. For more information on `\"relative_key\"`, please refer to\n            [Self-Attention with Relative Position Representations (Shaw et al.)](https://arxiv.org/abs/1803.02155).\n            For more information on `\"relative_key_query\"`, please refer to *Method 4* in [Improve Transformer Models\n            with Better Relative Position Embeddings (Huang et al.)](https://arxiv.org/abs/2009.13658).\n        is_decoder (`bool`, *optional*, defaults to `False`):\n            Whether the model is used as a decoder or not. If `False`, the model is used as an encoder.\n        use_cache (`bool`, *optional*, defaults to `True`):\n            Whether or not the model should return the last key/values attentions (not used by all models). Only\n            relevant if `config.is_decoder=True`.\n        emb_layer_norm_before (`bool`, *optional*):\n            Whether to apply layer normalization after embeddings but before the main stem of the network.\n        token_dropout (`bool`, defaults to `False`):\n            When this is enabled, masked tokens are treated as if they had been dropped out by input dropout.\n\n    Examples:\n        &gt;&gt;&gt; from multimolecule import CaLmModel, CaLmConfig\n\n        &gt;&gt;&gt; # Initializing a CaLM multimolecule/calm style configuration\n        &gt;&gt;&gt; configuration = CaLmConfig()\n\n        &gt;&gt;&gt; # Initializing a model (with random weights) from the multimolecule/calm style configuration\n        &gt;&gt;&gt; model = CaLmModel(configuration)\n\n        &gt;&gt;&gt; # Accessing the model configuration\n        &gt;&gt;&gt; configuration = model.config\n    \"\"\"\n\n    model_type = \"calm\"\n\n    def __init__(\n        self,\n        vocab_size=131,\n        codon=True,\n        hidden_size=768,\n        num_hidden_layers=12,\n        num_attention_heads=12,\n        intermediate_size=3072,\n        hidden_act=\"gelu\",\n        hidden_dropout=0.1,\n        attention_dropout=0.1,\n        max_position_embeddings=1026,\n        initializer_range=0.02,\n        layer_norm_eps=1e-12,\n        position_embedding_type=\"rotary\",\n        use_cache=True,\n        emb_layer_norm_before=False,\n        token_dropout=True,\n        head=None,\n        lm_head=None,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n\n        self.vocab_size = vocab_size\n        self.codon = codon\n        self.hidden_size = hidden_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.intermediate_size = intermediate_size\n        self.hidden_act = hidden_act\n        self.hidden_dropout = hidden_dropout\n        self.attention_dropout = attention_dropout\n        self.max_position_embeddings = max_position_embeddings\n        self.initializer_range = initializer_range\n        self.layer_norm_eps = layer_norm_eps\n        self.position_embedding_type = position_embedding_type\n        self.use_cache = use_cache\n        self.emb_layer_norm_before = emb_layer_norm_before\n        self.token_dropout = token_dropout\n        self.head = HeadConfig(**head if head is not None else {})\n        self.lm_head = MaskedLMHeadConfig(**lm_head if lm_head is not None else {})\n</code></pre>"},{"location":"models/calm/#multimolecule.models.calm.CaLmForMaskedLM","title":"<code>CaLmForMaskedLM</code>","text":"<p>               Bases: <code>CaLmPreTrainedModel</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import CaLmConfig, CaLmForMaskedLM, RnaTokenizer\n&gt;&gt;&gt; config = CaLmConfig()\n&gt;&gt;&gt; model = CaLmForMaskedLM(config)\n&gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n&gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n&gt;&gt;&gt; output = model(**input)\n</code></pre> Source code in <code>multimolecule/models/calm/modeling_calm.py</code> Python<pre><code>class CaLmForMaskedLM(CaLmPreTrainedModel):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; from multimolecule import CaLmConfig, CaLmForMaskedLM, RnaTokenizer\n        &gt;&gt;&gt; config = CaLmConfig()\n        &gt;&gt;&gt; model = CaLmForMaskedLM(config)\n        &gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n        &gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n        &gt;&gt;&gt; output = model(**input)\n    \"\"\"\n\n    _tied_weights_keys = [\"lm_head.decoder.weight\"]\n\n    def __init__(self, config: CaLmConfig):\n        super().__init__(config)\n        if config.is_decoder:\n            logger.warning(\n                \"If you want to use `CaLmForMaskedLM` make sure `config.is_decoder=False` for \"\n                \"bi-directional self-attention.\"\n            )\n        self.calm = CaLmModel(config, add_pooling_layer=False)\n        self.lm_head = MaskedLMHead(config, self.calm.embeddings.word_embeddings.weight)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(\n        self,\n        input_ids: Tensor | NestedTensor,\n        attention_mask: Tensor | None = None,\n        position_ids: Tensor | None = None,\n        head_mask: Tensor | None = None,\n        inputs_embeds: Tensor | NestedTensor | None = None,\n        encoder_hidden_states: Tensor | None = None,\n        encoder_attention_mask: Tensor | None = None,\n        labels: Tensor | None = None,\n        output_attentions: bool | None = None,\n        output_hidden_states: bool | None = None,\n        return_dict: bool | None = None,\n    ) -&gt; Tuple[Tensor, ...] | MaskedLMOutput:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n            config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n            loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n        \"\"\"\n\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.calm(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        output = self.lm_head(outputs, labels)\n        logits, loss = output.logits, output.loss\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return MaskedLMOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"models/calm/#multimolecule.models.calm.CaLmForMaskedLM.forward","title":"<code>forward(input_ids, attention_mask=None, position_ids=None, head_mask=None, inputs_embeds=None, encoder_hidden_states=None, encoder_attention_mask=None, labels=None, output_attentions=None, output_hidden_states=None, return_dict=None)</code>","text":"<p>labels (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, optional):     Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ...,     config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the     loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code></p> Source code in <code>multimolecule/models/calm/modeling_calm.py</code> Python<pre><code>def forward(\n    self,\n    input_ids: Tensor | NestedTensor,\n    attention_mask: Tensor | None = None,\n    position_ids: Tensor | None = None,\n    head_mask: Tensor | None = None,\n    inputs_embeds: Tensor | NestedTensor | None = None,\n    encoder_hidden_states: Tensor | None = None,\n    encoder_attention_mask: Tensor | None = None,\n    labels: Tensor | None = None,\n    output_attentions: bool | None = None,\n    output_hidden_states: bool | None = None,\n    return_dict: bool | None = None,\n) -&gt; Tuple[Tensor, ...] | MaskedLMOutput:\n    r\"\"\"\n    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n        Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n        config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n        loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n    \"\"\"\n\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n    outputs = self.calm(\n        input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        head_mask=head_mask,\n        inputs_embeds=inputs_embeds,\n        encoder_hidden_states=encoder_hidden_states,\n        encoder_attention_mask=encoder_attention_mask,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n    output = self.lm_head(outputs, labels)\n    logits, loss = output.logits, output.loss\n\n    if not return_dict:\n        output = (logits,) + outputs[2:]\n        return ((loss,) + output) if loss is not None else output\n\n    return MaskedLMOutput(\n        loss=loss,\n        logits=logits,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"models/calm/#multimolecule.models.calm.CaLmForNucleotideClassification","title":"<code>CaLmForNucleotideClassification</code>","text":"<p>               Bases: <code>CaLmPreTrainedModel</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import CaLmConfig, CaLmForNucleotideClassification, RnaTokenizer\n&gt;&gt;&gt; config = CaLmConfig()\n&gt;&gt;&gt; model = CaLmForNucleotideClassification(config)\n&gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n&gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n&gt;&gt;&gt; output = model(**input)\n</code></pre> Source code in <code>multimolecule/models/calm/modeling_calm.py</code> Python<pre><code>class CaLmForNucleotideClassification(CaLmPreTrainedModel):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; from multimolecule import CaLmConfig, CaLmForNucleotideClassification, RnaTokenizer\n        &gt;&gt;&gt; config = CaLmConfig()\n        &gt;&gt;&gt; model = CaLmForNucleotideClassification(config)\n        &gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n        &gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n        &gt;&gt;&gt; output = model(**input)\n    \"\"\"\n\n    def __init__(self, config: CaLmConfig):\n        super().__init__(config)\n        self.num_labels = config.head.num_labels\n        self.calm = CaLmModel(config, add_pooling_layer=False)\n        self.nucleotide_head = NucleotideClassificationHead(config)\n        self.head_config = self.nucleotide_head.config\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(\n        self,\n        input_ids: Tensor | NestedTensor,\n        attention_mask: Tensor | None = None,\n        position_ids: Tensor | None = None,\n        head_mask: Tensor | None = None,\n        inputs_embeds: Tensor | NestedTensor | None = None,\n        labels: Tensor | None = None,\n        output_attentions: bool | None = None,\n        output_hidden_states: bool | None = None,\n        return_dict: bool | None = None,\n        **kwargs,\n    ) -&gt; Tuple[Tensor, ...] | TokenClassifierOutput:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n            `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        if kwargs:\n            warn(\n                f\"Additional keyword arguments `{', '.join(kwargs)}` are detected in \"\n                f\"`{self.__class__.__name__}.forward`, they will be ignored.\\n\"\n                \"This is provided for backward compatibility and may lead to unexpected behavior.\"\n            )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.calm(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        output = self.nucleotide_head(outputs, attention_mask, input_ids, labels)\n        logits, loss = output.logits, output.loss\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return TokenClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"models/calm/#multimolecule.models.calm.CaLmForNucleotideClassification.forward","title":"<code>forward(input_ids, attention_mask=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None, output_attentions=None, output_hidden_states=None, return_dict=None, **kwargs)</code>","text":"<p>labels (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, optional):     Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ...,     config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If     <code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).</p> Source code in <code>multimolecule/models/calm/modeling_calm.py</code> Python<pre><code>def forward(\n    self,\n    input_ids: Tensor | NestedTensor,\n    attention_mask: Tensor | None = None,\n    position_ids: Tensor | None = None,\n    head_mask: Tensor | None = None,\n    inputs_embeds: Tensor | NestedTensor | None = None,\n    labels: Tensor | None = None,\n    output_attentions: bool | None = None,\n    output_hidden_states: bool | None = None,\n    return_dict: bool | None = None,\n    **kwargs,\n) -&gt; Tuple[Tensor, ...] | TokenClassifierOutput:\n    r\"\"\"\n    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n        `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).\n    \"\"\"\n    if kwargs:\n        warn(\n            f\"Additional keyword arguments `{', '.join(kwargs)}` are detected in \"\n            f\"`{self.__class__.__name__}.forward`, they will be ignored.\\n\"\n            \"This is provided for backward compatibility and may lead to unexpected behavior.\"\n        )\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n    outputs = self.calm(\n        input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        head_mask=head_mask,\n        inputs_embeds=inputs_embeds,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n    output = self.nucleotide_head(outputs, attention_mask, input_ids, labels)\n    logits, loss = output.logits, output.loss\n\n    if not return_dict:\n        output = (logits,) + outputs[2:]\n        return ((loss,) + output) if loss is not None else output\n\n    return TokenClassifierOutput(\n        loss=loss,\n        logits=logits,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"models/calm/#multimolecule.models.calm.CaLmForSequenceClassification","title":"<code>CaLmForSequenceClassification</code>","text":"<p>               Bases: <code>CaLmPreTrainedModel</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import CaLmConfig, CaLmForSequenceClassification, RnaTokenizer\n&gt;&gt;&gt; config = CaLmConfig()\n&gt;&gt;&gt; model = CaLmForSequenceClassification(config)\n&gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n&gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n&gt;&gt;&gt; output = model(**input)\n</code></pre> Source code in <code>multimolecule/models/calm/modeling_calm.py</code> Python<pre><code>class CaLmForSequenceClassification(CaLmPreTrainedModel):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; from multimolecule import CaLmConfig, CaLmForSequenceClassification, RnaTokenizer\n        &gt;&gt;&gt; config = CaLmConfig()\n        &gt;&gt;&gt; model = CaLmForSequenceClassification(config)\n        &gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n        &gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n        &gt;&gt;&gt; output = model(**input)\n    \"\"\"\n\n    def __init__(self, config: CaLmConfig):\n        super().__init__(config)\n        self.num_labels = config.head.num_labels\n        self.calm = CaLmModel(config, add_pooling_layer=True)\n        self.sequence_head = SequenceClassificationHead(config)\n        self.head_config = self.sequence_head.config\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(\n        self,\n        input_ids: Tensor | NestedTensor,\n        attention_mask: Tensor | None = None,\n        position_ids: Tensor | None = None,\n        head_mask: Tensor | None = None,\n        inputs_embeds: Tensor | NestedTensor | None = None,\n        labels: Tensor | None = None,\n        output_attentions: bool | None = None,\n        output_hidden_states: bool | None = None,\n        return_dict: bool | None = None,\n    ) -&gt; Tuple[Tensor, ...] | SequenceClassifierOutput:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n            `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.calm(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        output = self.sequence_head(outputs, labels)\n        logits, loss = output.logits, output.loss\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"models/calm/#multimolecule.models.calm.CaLmForSequenceClassification.forward","title":"<code>forward(input_ids, attention_mask=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None, output_attentions=None, output_hidden_states=None, return_dict=None)</code>","text":"<p>labels (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, optional):     Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ...,     config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If     <code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).</p> Source code in <code>multimolecule/models/calm/modeling_calm.py</code> Python<pre><code>def forward(\n    self,\n    input_ids: Tensor | NestedTensor,\n    attention_mask: Tensor | None = None,\n    position_ids: Tensor | None = None,\n    head_mask: Tensor | None = None,\n    inputs_embeds: Tensor | NestedTensor | None = None,\n    labels: Tensor | None = None,\n    output_attentions: bool | None = None,\n    output_hidden_states: bool | None = None,\n    return_dict: bool | None = None,\n) -&gt; Tuple[Tensor, ...] | SequenceClassifierOutput:\n    r\"\"\"\n    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n        `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).\n    \"\"\"\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n    outputs = self.calm(\n        input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        head_mask=head_mask,\n        inputs_embeds=inputs_embeds,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n    output = self.sequence_head(outputs, labels)\n    logits, loss = output.logits, output.loss\n\n    if not return_dict:\n        output = (logits,) + outputs[2:]\n        return ((loss,) + output) if loss is not None else output\n\n    return SequenceClassifierOutput(\n        loss=loss,\n        logits=logits,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"models/calm/#multimolecule.models.calm.CaLmForTokenClassification","title":"<code>CaLmForTokenClassification</code>","text":"<p>               Bases: <code>CaLmPreTrainedModel</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import CaLmConfig, CaLmForTokenClassification, RnaTokenizer\n&gt;&gt;&gt; config = CaLmConfig()\n&gt;&gt;&gt; model = CaLmForTokenClassification(config)\n&gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n&gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n&gt;&gt;&gt; output = model(**input)\n</code></pre> Source code in <code>multimolecule/models/calm/modeling_calm.py</code> Python<pre><code>class CaLmForTokenClassification(CaLmPreTrainedModel):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; from multimolecule import CaLmConfig, CaLmForTokenClassification, RnaTokenizer\n        &gt;&gt;&gt; config = CaLmConfig()\n        &gt;&gt;&gt; model = CaLmForTokenClassification(config)\n        &gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n        &gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n        &gt;&gt;&gt; output = model(**input)\n    \"\"\"\n\n    def __init__(self, config: CaLmConfig):\n        super().__init__(config)\n        self.num_labels = config.head.num_labels\n        self.calm = CaLmModel(config, add_pooling_layer=False)\n        self.token_head = TokenClassificationHead(config)\n        self.head_config = self.token_head.config\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(\n        self,\n        input_ids: Tensor | NestedTensor,\n        attention_mask: Tensor | None = None,\n        position_ids: Tensor | None = None,\n        head_mask: Tensor | None = None,\n        inputs_embeds: Tensor | NestedTensor | None = None,\n        labels: Tensor | None = None,\n        output_attentions: bool | None = None,\n        output_hidden_states: bool | None = None,\n        return_dict: bool | None = None,\n    ) -&gt; Tuple[Tensor, ...] | TokenClassifierOutput:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.calm(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        output = self.token_head(outputs, attention_mask, input_ids, labels)\n        logits, loss = output.logits, output.loss\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return TokenClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"models/calm/#multimolecule.models.calm.CaLmForTokenClassification.forward","title":"<code>forward(input_ids, attention_mask=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None, output_attentions=None, output_hidden_states=None, return_dict=None)</code>","text":"<p>labels (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, optional):     Labels for computing the token classification loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>.</p> Source code in <code>multimolecule/models/calm/modeling_calm.py</code> Python<pre><code>def forward(\n    self,\n    input_ids: Tensor | NestedTensor,\n    attention_mask: Tensor | None = None,\n    position_ids: Tensor | None = None,\n    head_mask: Tensor | None = None,\n    inputs_embeds: Tensor | NestedTensor | None = None,\n    labels: Tensor | None = None,\n    output_attentions: bool | None = None,\n    output_hidden_states: bool | None = None,\n    return_dict: bool | None = None,\n) -&gt; Tuple[Tensor, ...] | TokenClassifierOutput:\n    r\"\"\"\n    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n        Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n    \"\"\"\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n    outputs = self.calm(\n        input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        head_mask=head_mask,\n        inputs_embeds=inputs_embeds,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n    output = self.token_head(outputs, attention_mask, input_ids, labels)\n    logits, loss = output.logits, output.loss\n\n    if not return_dict:\n        output = (logits,) + outputs[2:]\n        return ((loss,) + output) if loss is not None else output\n\n    return TokenClassifierOutput(\n        loss=loss,\n        logits=logits,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"models/calm/#multimolecule.models.calm.CaLmModel","title":"<code>CaLmModel</code>","text":"<p>               Bases: <code>CaLmPreTrainedModel</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import CaLmConfig, CaLmModel, RnaTokenizer\n&gt;&gt;&gt; config = CaLmConfig()\n&gt;&gt;&gt; model = CaLmModel(config)\n&gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n&gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n&gt;&gt;&gt; output = model(**input)\n</code></pre> Source code in <code>multimolecule/models/calm/modeling_calm.py</code> Python<pre><code>class CaLmModel(CaLmPreTrainedModel):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; from multimolecule import CaLmConfig, CaLmModel, RnaTokenizer\n        &gt;&gt;&gt; config = CaLmConfig()\n        &gt;&gt;&gt; model = CaLmModel(config)\n        &gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n        &gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n        &gt;&gt;&gt; output = model(**input)\n    \"\"\"\n\n    def __init__(self, config: CaLmConfig, add_pooling_layer: bool = True):\n        super().__init__(config)\n        self.pad_token_id = config.pad_token_id\n        self.embeddings = CaLmEmbeddings(config)\n        self.encoder = CaLmEncoder(config)\n        self.pooler = CaLmPooler(config) if add_pooling_layer else None\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.embeddings.word_embeddings\n\n    def set_input_embeddings(self, value):\n        self.embeddings.word_embeddings = value\n\n    def _prune_heads(self, heads_to_prune):\n        \"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"\n        for layer, heads in heads_to_prune.items():\n            self.encoder.layer[layer].attention.prune_heads(heads)\n\n    def forward(\n        self,\n        input_ids: Tensor | NestedTensor,\n        attention_mask: Tensor | None = None,\n        position_ids: Tensor | None = None,\n        head_mask: Tensor | None = None,\n        inputs_embeds: Tensor | NestedTensor | None = None,\n        encoder_hidden_states: Tensor | None = None,\n        encoder_attention_mask: Tensor | None = None,\n        past_key_values: Tuple[Tuple[torch.FloatTensor, torch.FloatTensor], ...] | None = None,\n        use_cache: bool | None = None,\n        output_attentions: bool | None = None,\n        output_hidden_states: bool | None = None,\n        return_dict: bool | None = None,\n    ) -&gt; Tuple[Tensor, ...] | BaseModelOutputWithPoolingAndCrossAttentions:\n        r\"\"\"\n        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n            the model is configured as a decoder.\n        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors\n            of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n\n            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n            `past_key_values`).\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if self.config.is_decoder:\n            use_cache = use_cache if use_cache is not None else self.config.use_cache\n        else:\n            use_cache = False\n\n        if isinstance(input_ids, NestedTensor):\n            input_ids, attention_mask = input_ids.tensor, input_ids.mask\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n        if input_ids is not None:\n            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n            input_shape = input_ids.size()\n        elif inputs_embeds is not None:\n            input_shape = inputs_embeds.size()[:-1]\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n        batch_size, seq_length = input_shape\n        device = input_ids.device if input_ids is not None else inputs_embeds.device  # type: ignore[union-attr]\n\n        # past_key_values_length\n        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n\n        if attention_mask is None:\n            attention_mask = (\n                input_ids.ne(self.pad_token_id)\n                if self.pad_token_id is not None\n                else torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n            )\n\n        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n        # ourselves in which case we just need to make it broadcastable to all heads.\n        extended_attention_mask: Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n\n        # If a 2D or 3D attention mask is provided for the cross-attention\n        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n        if self.config.is_decoder and encoder_hidden_states is not None:\n            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n            if encoder_attention_mask is None:\n                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n        else:\n            encoder_extended_attention_mask = None\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n\n        embedding_output = self.embeddings(\n            input_ids=input_ids,\n            position_ids=position_ids,\n            attention_mask=attention_mask,\n            inputs_embeds=inputs_embeds,\n            past_key_values_length=past_key_values_length,\n        )\n        encoder_outputs = self.encoder(\n            embedding_output,\n            attention_mask=extended_attention_mask,\n            head_mask=head_mask,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_extended_attention_mask,\n            past_key_values=past_key_values,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        sequence_output = encoder_outputs[0]\n        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n\n        if not return_dict:\n            return (sequence_output, pooled_output) + encoder_outputs[1:]\n\n        return BaseModelOutputWithPoolingAndCrossAttentions(\n            last_hidden_state=sequence_output,\n            pooler_output=pooled_output,\n            past_key_values=encoder_outputs.past_key_values,\n            hidden_states=encoder_outputs.hidden_states,\n            attentions=encoder_outputs.attentions,\n            cross_attentions=encoder_outputs.cross_attentions,\n        )\n</code></pre>"},{"location":"models/calm/#multimolecule.models.calm.CaLmModel.forward","title":"<code>forward(input_ids, attention_mask=None, position_ids=None, head_mask=None, inputs_embeds=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_values=None, use_cache=None, output_attentions=None, output_hidden_states=None, return_dict=None)</code>","text":"<p>encoder_hidden_states  (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, optional):     Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if     the model is configured as a decoder. encoder_attention_mask (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, optional):     Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in     the cross-attention if the model is configured as a decoder. Mask values selected in <code>[0, 1]</code>:</p> Text Only<pre><code>- 1 for tokens that are **not masked**,\n- 0 for tokens that are **masked**.\n</code></pre> <p>past_key_values (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors     of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>):     Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.</p> Text Only<pre><code>If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\ndon't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n`decoder_input_ids` of shape `(batch_size, sequence_length)`.\n</code></pre> <p>use_cache (<code>bool</code>, optional):     If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see     <code>past_key_values</code>).</p> Source code in <code>multimolecule/models/calm/modeling_calm.py</code> Python<pre><code>def forward(\n    self,\n    input_ids: Tensor | NestedTensor,\n    attention_mask: Tensor | None = None,\n    position_ids: Tensor | None = None,\n    head_mask: Tensor | None = None,\n    inputs_embeds: Tensor | NestedTensor | None = None,\n    encoder_hidden_states: Tensor | None = None,\n    encoder_attention_mask: Tensor | None = None,\n    past_key_values: Tuple[Tuple[torch.FloatTensor, torch.FloatTensor], ...] | None = None,\n    use_cache: bool | None = None,\n    output_attentions: bool | None = None,\n    output_hidden_states: bool | None = None,\n    return_dict: bool | None = None,\n) -&gt; Tuple[Tensor, ...] | BaseModelOutputWithPoolingAndCrossAttentions:\n    r\"\"\"\n    encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n        Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n        the model is configured as a decoder.\n    encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n        Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n        the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n\n        - 1 for tokens that are **not masked**,\n        - 0 for tokens that are **masked**.\n    past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors\n        of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n        Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n\n        If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n        don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n        `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n    use_cache (`bool`, *optional*):\n        If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n        `past_key_values`).\n    \"\"\"\n    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n    output_hidden_states = (\n        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n    )\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n    if self.config.is_decoder:\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n    else:\n        use_cache = False\n\n    if isinstance(input_ids, NestedTensor):\n        input_ids, attention_mask = input_ids.tensor, input_ids.mask\n    if input_ids is not None and inputs_embeds is not None:\n        raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n    if input_ids is not None:\n        self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n        input_shape = input_ids.size()\n    elif inputs_embeds is not None:\n        input_shape = inputs_embeds.size()[:-1]\n    else:\n        raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n    batch_size, seq_length = input_shape\n    device = input_ids.device if input_ids is not None else inputs_embeds.device  # type: ignore[union-attr]\n\n    # past_key_values_length\n    past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n\n    if attention_mask is None:\n        attention_mask = (\n            input_ids.ne(self.pad_token_id)\n            if self.pad_token_id is not None\n            else torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n        )\n\n    # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n    # ourselves in which case we just need to make it broadcastable to all heads.\n    extended_attention_mask: Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n\n    # If a 2D or 3D attention mask is provided for the cross-attention\n    # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n    if self.config.is_decoder and encoder_hidden_states is not None:\n        encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n        encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n        if encoder_attention_mask is None:\n            encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n        encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n    else:\n        encoder_extended_attention_mask = None\n\n    # Prepare head mask if needed\n    # 1.0 in head_mask indicate we keep the head\n    # attention_probs has shape bsz x n_heads x N x N\n    # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n    # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n    head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n\n    embedding_output = self.embeddings(\n        input_ids=input_ids,\n        position_ids=position_ids,\n        attention_mask=attention_mask,\n        inputs_embeds=inputs_embeds,\n        past_key_values_length=past_key_values_length,\n    )\n    encoder_outputs = self.encoder(\n        embedding_output,\n        attention_mask=extended_attention_mask,\n        head_mask=head_mask,\n        encoder_hidden_states=encoder_hidden_states,\n        encoder_attention_mask=encoder_extended_attention_mask,\n        past_key_values=past_key_values,\n        use_cache=use_cache,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n    sequence_output = encoder_outputs[0]\n    pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n\n    if not return_dict:\n        return (sequence_output, pooled_output) + encoder_outputs[1:]\n\n    return BaseModelOutputWithPoolingAndCrossAttentions(\n        last_hidden_state=sequence_output,\n        pooler_output=pooled_output,\n        past_key_values=encoder_outputs.past_key_values,\n        hidden_states=encoder_outputs.hidden_states,\n        attentions=encoder_outputs.attentions,\n        cross_attentions=encoder_outputs.cross_attentions,\n    )\n</code></pre>"},{"location":"models/calm/#multimolecule.models.calm.CaLmPreTrainedModel","title":"<code>CaLmPreTrainedModel</code>","text":"<p>               Bases: <code>PreTrainedModel</code></p> <p>An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.</p> Source code in <code>multimolecule/models/calm/modeling_calm.py</code> Python<pre><code>class CaLmPreTrainedModel(PreTrainedModel):\n    \"\"\"\n    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n    models.\n    \"\"\"\n\n    config_class = CaLmConfig\n    base_model_prefix = \"calm\"\n    supports_gradient_checkpointing = True\n    _no_split_modules = [\"CaLmLayer\", \"CaLmEmbeddings\"]\n\n    # Copied from transformers.models.bert.modeling_bert.BertPreTrainedModel._init_weights\n    def _init_weights(self, module: nn.Module):\n        \"\"\"Initialize the weights\"\"\"\n        if isinstance(module, nn.Linear):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n</code></pre>"},{"location":"models/rnabert/","title":"RNABERT","text":"<p>Pre-trained model on non-coding RNA (ncRNA) using masked language modeling (MLM) and structural alignment learning (SAL) objectives.</p>"},{"location":"models/rnabert/#disclaimer","title":"Disclaimer","text":"<p>This is an UNOFFICIAL implementation of the Informative RNA-base embedding for functional RNA clustering and structural alignment by Manato Akiyama and Yasubumi Sakakibara.</p> <p>The OFFICIAL repository of RNABERT is at mana438/RNABERT.</p> <p>The team releasing RNABERT did not write this model card for this model so this model card has been written by the MultiMolecule team.</p>"},{"location":"models/rnabert/#model-details","title":"Model Details","text":"<p>RNABERT is a bert-style model pre-trained on a large corpus of non-coding RNA sequences in a self-supervised fashion. This means that the model was trained on the raw nucleotides of RNA sequences only, with an automatic process to generate inputs and labels from those texts. Please refer to the Training Details section for more information on the training process.</p>"},{"location":"models/rnabert/#model-specification","title":"Model Specification","text":"Num Layers Hidden Size Num Heads Intermediate Size Num Parameters (M) FLOPs (G) MACs (G) Max Num Tokens 6 120 12 40 0.48 0.15 0.08 440"},{"location":"models/rnabert/#links","title":"Links","text":"<ul> <li>Code: multimolecule.rnabert</li> <li>Weights: multimolecule/rnabert</li> <li>Data: RNAcentral</li> <li>Paper: Informative RNA-base embedding for functional RNA clustering and structural alignment</li> <li>Developed by: JManato Akiyama and Yasubumi Sakakibara</li> <li>Model type: BERT</li> <li>Original Repository: https://github.com/mana438/RNABERT</li> </ul>"},{"location":"models/rnabert/#usage","title":"Usage","text":"<p>The model file depends on the <code>multimolecule</code> library. You can install it using pip:</p> Bash<pre><code>pip install multimolecule\n</code></pre>"},{"location":"models/rnabert/#direct-use","title":"Direct Use","text":"<p>You can use this model directly with a pipeline for masked language modeling:</p> Python<pre><code>&gt;&gt;&gt; import multimolecule  # you must import multimolecule to register models\n&gt;&gt;&gt; from transformers import pipeline\n&gt;&gt;&gt; unmasker = pipeline('fill-mask', model='multimolecule/rnabert')\n&gt;&gt;&gt; unmasker(\"uagc&lt;mask&gt;uaucagacugauguuga\")\n\n[{'score': 0.04004698991775513,\n  'token': 2,\n  'token_str': '&lt;eos&gt;',\n  'sequence': 'U A G C U A U C A G A C U G A U G U U G A'},\n {'score': 0.04004558548331261,\n  'token': 22,\n  'token_str': '.',\n  'sequence': 'U A G C. U A U C A G A C U G A U G U U G A'},\n {'score': 0.040029074996709824,\n  'token': 10,\n  'token_str': 'N',\n  'sequence': 'U A G C N U A U C A G A C U G A U G U U G A'},\n {'score': 0.0400208942592144,\n  'token': 5,\n  'token_str': '&lt;null&gt;',\n  'sequence': 'U A G C U A U C A G A C U G A U G U U G A'},\n {'score': 0.04000627622008324,\n  'token': 3,\n  'token_str': '&lt;unk&gt;',\n  'sequence': 'U A G C U A U C A G A C U G A U G U U G A'}]\n</code></pre>"},{"location":"models/rnabert/#downstream-use","title":"Downstream Use","text":""},{"location":"models/rnabert/#extract-features","title":"Extract Features","text":"<p>Here is how to use this model to get the features of a given sequence in PyTorch:</p> Python<pre><code>from multimolecule import RnaTokenizer, RnaBertModel\n\n\ntokenizer = RnaTokenizer.from_pretrained('multimolecule/rnabert')\nmodel = RnaBertModel.from_pretrained('multimolecule/rnabert')\n\ntext = \"UAGCUUAUCAGACUGAUGUUGA\"\ninput = tokenizer(text, return_tensors='pt')\n\noutput = model(**input)\n</code></pre>"},{"location":"models/rnabert/#sequence-classification-regression","title":"Sequence Classification / Regression","text":"<p>Note: This model is not fine-tuned for any specific task. You will need to fine-tune the model on a downstream task to use it for sequence classification or regression.</p> <p>Here is how to use this model as backbone to fine-tune for a sequence-level task in PyTorch:</p> Python<pre><code>import torch\nfrom multimolecule import RnaTokenizer, RnaBertForSequenceClassification\n\n\ntokenizer = RnaTokenizer.from_pretrained('multimolecule/rnabert')\nmodel = RnaBertForSequenceClassification.from_pretrained('multimolecule/rnabert')\n\ntext = \"UAGCUUAUCAGACUGAUGUUGA\"\ninput = tokenizer(text, return_tensors='pt')\nlabel = torch.tensor([1])\n\noutput = model(**input, labels=label)\n</code></pre>"},{"location":"models/rnabert/#nucleotide-classification-regression","title":"Nucleotide Classification / Regression","text":"<p>Note: This model is not fine-tuned for any specific task. You will need to fine-tune the model on a downstream task to use it for nucleotide classification or regression.</p> <p>Here is how to use this model as backbone to fine-tune for a nucleotide-level task in PyTorch:</p> Python<pre><code>import torch\nfrom multimolecule import RnaTokenizer, RnaBertForNucleotideClassification\n\n\ntokenizer = RnaTokenizer.from_pretrained('multimolecule/rnabert')\nmodel = RnaBertForNucleotideClassification.from_pretrained('multimolecule/rnabert')\n\ntext = \"UAGCUUAUCAGACUGAUGUUGA\"\ninput = tokenizer(text, return_tensors='pt')\nlabel = torch.randint(2, (len(text), ))\n\noutput = model(**input, labels=label)\n</code></pre>"},{"location":"models/rnabert/#training-details","title":"Training Details","text":"<p>RNABERT has two pre-training objectives: masked language modeling (MLM) and structural alignment learning (SAL).</p> <ul> <li>Masked Language Modeling (MLM): taking a sequence, the model randomly masks 15% of the tokens in the input then run the entire masked sentence through the model and has to predict the masked tokens. This is comparable to the Cloze task in language modeling.</li> <li>Structural Alignment Learning (SAL): the model learns to predict the structural alignment of two RNA sequences. The model is trained to predict the alignment score of two RNA sequences using the Needleman-Wunsch algorithm.</li> </ul>"},{"location":"models/rnabert/#training-data","title":"Training Data","text":"<p>The RNABERT model was pre-trained on RNAcentral. RNAcentral is a comprehensive database of non-coding RNA sequences from a wide range of species. It combines 47 different databases, adding up to around 27 million RNA sequences in total. RNABERT used a subset of 76, 237 human ncRNA sequences from RNAcentral for pre-training.</p> <p>RNABERT preprocessed all tokens by replacing \u201cU\u201ds with \u201cT\u201ds.</p> <p>Note that during model conversions, \u201cT\u201d is replaced with \u201cU\u201d. <code>RnaTokenizer</code> will convert \u201cT\u201ds to \u201cU\u201ds for you, you may disable this behaviour by passing <code>replace_T_with_U=False</code>.</p>"},{"location":"models/rnabert/#training-procedure","title":"Training Procedure","text":""},{"location":"models/rnabert/#preprocessing","title":"Preprocessing","text":"<p>RNABERT preprocess the dataset by applying 10 different mask patterns to the 72, 237 human ncRNA sequences. The final dataset contains 722, 370 sequences. The masking procedure is similar to the one used in BERT:</p> <ul> <li>15% of the tokens are masked.</li> <li>In 80% of the cases, the masked tokens are replaced by <code>&lt;mask&gt;</code>.</li> <li>In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.</li> <li>In the 10% remaining cases, the masked tokens are left as is.</li> </ul>"},{"location":"models/rnabert/#pretraining","title":"Pretraining","text":"<p>The model was trained on 1 NVIDIA V100 GPU.</p>"},{"location":"models/rnabert/#citation","title":"Citation","text":"<p>BibTeX:</p> BibTeX<pre><code>@article{akiyama2022informative,\n    author = {Akiyama, Manato and Sakakibara, Yasubumi},\n    title = \"{Informative RNA base embedding for RNA structural alignment and clustering by deep representation learning}\",\n    journal = {NAR Genomics and Bioinformatics},\n    volume = {4},\n    number = {1},\n    pages = {lqac012},\n    year = {2022},\n    month = {02},\n    abstract = \"{Effective embedding is actively conducted by applying deep learning to biomolecular information. Obtaining better embeddings enhances the quality of downstream analyses, such as DNA sequence motif detection and protein function prediction. In this study, we adopt a pre-training algorithm for the effective embedding of RNA bases to acquire semantically rich representations and apply this algorithm to two fundamental RNA sequence problems: structural alignment and clustering. By using the pre-training algorithm to embed the four bases of RNA in a position-dependent manner using a large number of RNA sequences from various RNA families, a context-sensitive embedding representation is obtained. As a result, not only base information but also secondary structure and context information of RNA sequences are embedded for each base. We call this \u2018informative base embedding\u2019 and use it to achieve accuracies superior to those of existing state-of-the-art methods on RNA structural alignment and RNA family clustering tasks. Furthermore, upon performing RNA sequence alignment by combining this informative base embedding with a simple Needleman\u2013Wunsch alignment algorithm, we succeed in calculating structural alignments with a time complexity of O(n2) instead of the O(n6) time complexity of the naive implementation of Sankoff-style algorithm for input RNA sequence of length n.}\",\n    issn = {2631-9268},\n    doi = {10.1093/nargab/lqac012},\n    url = {https://doi.org/10.1093/nargab/lqac012},\n    eprint = {https://academic.oup.com/nargab/article-pdf/4/1/lqac012/42577168/lqac012.pdf},\n}\n</code></pre>"},{"location":"models/rnabert/#contact","title":"Contact","text":"<p>Please use GitHub issues of MultiMolecule for any questions or comments on the model card.</p> <p>Please contact the authors of the RNABERT paper for questions or comments on the paper/model.</p>"},{"location":"models/rnabert/#license","title":"License","text":"<p>This model is licensed under the AGPL-3.0 License.</p> Text Only<pre><code>SPDX-License-Identifier: AGPL-3.0-or-later\n</code></pre>"},{"location":"models/rnabert/#multimolecule.models.rnabert","title":"<code>multimolecule.models.rnabert</code>","text":""},{"location":"models/rnabert/#multimolecule.models.rnabert.RnaTokenizer","title":"<code>RnaTokenizer</code>","text":"<p>               Bases: <code>Tokenizer</code></p> <p>Constructs an RNA tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>alphabet</code> <code>List[str] | None</code> <p>List of tokens to use. Defaults to IUPAC nucleotide code.</p> <code>None</code> <code>nmers</code> <code>int</code> <p>Size of nmers to tokenize. Defaults to 1.</p> <code>1</code> <code>codon</code> <code>bool</code> <p>Whether to tokenize into codons. Defaults to False.</p> <code>False</code> <code>replace_T_with_U</code> <code>bool</code> <p>Whether to replace T with U. Defaults to True.</p> <code>True</code> <code>do_upper_case</code> <code>bool</code> <p>Whether to convert input to uppercase. Defaults to True.</p> <code>True</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import RnaTokenizer\n&gt;&gt;&gt; tokenizer = RnaTokenizer()\n&gt;&gt;&gt; tokenizer('&lt;pad&gt;&lt;cls&gt;&lt;eos&gt;&lt;unk&gt;&lt;mask&gt;&lt;null&gt;ACGUNXVHDBMRWSYK.*-')[\"input_ids\"]\n[1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 2]\n&gt;&gt;&gt; tokenizer('acgu')[\"input_ids\"]\n[1, 6, 7, 8, 9, 2]\n&gt;&gt;&gt; tokenizer('acgt')[\"input_ids\"]\n[1, 6, 7, 8, 9, 2]\n&gt;&gt;&gt; tokenizer = RnaTokenizer(replace_T_with_U=False)\n&gt;&gt;&gt; tokenizer('acgt')[\"input_ids\"]\n[1, 6, 7, 8, 3, 2]\n&gt;&gt;&gt; tokenizer = RnaTokenizer(nmers=3)\n&gt;&gt;&gt; tokenizer('uagcuuauc')[\"input_ids\"]\n[1, 83, 17, 64, 49, 96, 84, 22, 2]\n&gt;&gt;&gt; tokenizer = RnaTokenizer(codon=True)\n&gt;&gt;&gt; tokenizer('uagcuuauc')[\"input_ids\"]\n[1, 83, 49, 22, 2]\n&gt;&gt;&gt; tokenizer('uagcuuauca')[\"input_ids\"]\nTraceback (most recent call last):\nValueError: length of input sequence  must be a multiple of 3 for codon tokenization, but got 10\n</code></pre> Source code in <code>multimolecule/tokenisers/rna/tokenization_rna.py</code> Python<pre><code>class RnaTokenizer(Tokenizer):\n    \"\"\"\n    Constructs an RNA tokenizer.\n\n    Args:\n        alphabet (List[str] | None, optional): List of tokens to use.\n            Defaults to [IUPAC nucleotide code](https://www.bioinformatics.org/sms2/iupac.html).\n        nmers (int, optional): Size of nmers to tokenize.\n            Defaults to 1.\n        codon (bool, optional): Whether to tokenize into codons.\n            Defaults to False.\n        replace_T_with_U (bool, optional): Whether to replace T with U.\n            Defaults to True.\n        do_upper_case (bool, optional): Whether to convert input to uppercase.\n            Defaults to True.\n\n    Examples:\n        &gt;&gt;&gt; from multimolecule import RnaTokenizer\n        &gt;&gt;&gt; tokenizer = RnaTokenizer()\n        &gt;&gt;&gt; tokenizer('&lt;pad&gt;&lt;cls&gt;&lt;eos&gt;&lt;unk&gt;&lt;mask&gt;&lt;null&gt;ACGUNXVHDBMRWSYK.*-')[\"input_ids\"]\n        [1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 2]\n        &gt;&gt;&gt; tokenizer('acgu')[\"input_ids\"]\n        [1, 6, 7, 8, 9, 2]\n        &gt;&gt;&gt; tokenizer('acgt')[\"input_ids\"]\n        [1, 6, 7, 8, 9, 2]\n        &gt;&gt;&gt; tokenizer = RnaTokenizer(replace_T_with_U=False)\n        &gt;&gt;&gt; tokenizer('acgt')[\"input_ids\"]\n        [1, 6, 7, 8, 3, 2]\n        &gt;&gt;&gt; tokenizer = RnaTokenizer(nmers=3)\n        &gt;&gt;&gt; tokenizer('uagcuuauc')[\"input_ids\"]\n        [1, 83, 17, 64, 49, 96, 84, 22, 2]\n        &gt;&gt;&gt; tokenizer = RnaTokenizer(codon=True)\n        &gt;&gt;&gt; tokenizer('uagcuuauc')[\"input_ids\"]\n        [1, 83, 49, 22, 2]\n        &gt;&gt;&gt; tokenizer('uagcuuauca')[\"input_ids\"]\n        Traceback (most recent call last):\n        ValueError: length of input sequence  must be a multiple of 3 for codon tokenization, but got 10\n    \"\"\"\n\n    model_input_names = [\"input_ids\", \"attention_mask\"]\n\n    def __init__(\n        self,\n        alphabet: List[str] | None = None,\n        nmers: int = 1,\n        codon: bool = False,\n        replace_T_with_U: bool = True,\n        bos_token: str = \"&lt;cls&gt;\",\n        cls_token: str = \"&lt;cls&gt;\",\n        pad_token: str = \"&lt;pad&gt;\",\n        eos_token: str = \"&lt;eos&gt;\",\n        sep_token: str = \"&lt;eos&gt;\",\n        unk_token: str = \"&lt;unk&gt;\",\n        mask_token: str = \"&lt;mask&gt;\",\n        additional_special_tokens: List | Tuple | None = None,\n        do_upper_case: bool = True,\n        **kwargs,\n    ):\n        if codon and nmers &gt; 1:\n            raise ValueError(\"Codon and nmers cannot be used together.\")\n        if codon:\n            nmers = 3  # set to 3 to get correct vocab\n        super().__init__(\n            alphabet=get_vocab_list(alphabet, nmers),\n            bos_token=bos_token,\n            cls_token=cls_token,\n            pad_token=pad_token,\n            eos_token=eos_token,\n            sep_token=sep_token,\n            unk_token=unk_token,\n            mask_token=mask_token,\n            additional_special_tokens=additional_special_tokens,\n            do_upper_case=do_upper_case,\n            **kwargs,\n        )\n        self.replace_T_with_U = replace_T_with_U\n        self.nmers = nmers\n        self.condon = codon\n\n    def _tokenize(self, text: str, **kwargs):\n        if self.do_upper_case:\n            text = text.upper()\n        if self.replace_T_with_U:\n            text = text.replace(\"T\", \"U\")\n        if self.condon:\n            if len(text) % 3 != 0:\n                raise ValueError(\n                    f\"length of input sequence  must be a multiple of 3 for codon tokenization, but got {len(text)}\"\n                )\n            return [text[i : i + 3] for i in range(0, len(text), 3)]\n        if self.nmers &gt; 1:\n            return [text[i : i + self.nmers] for i in range(len(text) - self.nmers + 1)]  # noqa: E203\n        return list(text)\n</code></pre>"},{"location":"models/rnabert/#multimolecule.models.rnabert.RnaBertConfig","title":"<code>RnaBertConfig</code>","text":"<p>               Bases: <code>PreTrainedConfig</code></p> <p>This is the configuration class to store the configuration of a [<code>RnaBertModel</code>]. It is used to instantiate a RnaBert model according to the specified arguments, defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of the RnaBert mana438/RNABERT architecture.</p> <p>Configuration objects inherit from [<code>PreTrainedConfig</code>] and can be used to control the model outputs. Read the documentation from [<code>PreTrainedConfig</code>] for more information.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_size</code> <code>`int`, *optional*, defaults to 25</code> <p>Vocabulary size of the RnaBert model. Defines the number of different tokens that can be represented by the <code>inputs_ids</code> passed when calling [<code>RnaBertModel</code>].</p> <code>25</code> <code>hidden_size</code> <code>`int`, *optional*, defaults to 120</code> <p>Dimensionality of the encoder layers and the pooler layer.</p> <code>None</code> <code>num_hidden_layers</code> <code>`int`, *optional*, defaults to 12</code> <p>Number of hidden layers in the Transformer encoder.</p> <code>6</code> <code>num_attention_heads</code> <code>`int`, *optional*, defaults to 6</code> <p>Number of attention heads for each attention layer in the Transformer encoder.</p> <code>12</code> <code>intermediate_size</code> <code>`int`, *optional*, defaults to 40</code> <p>Dimensionality of the \u201cintermediate\u201d (often named feed-forward) layer in the Transformer encoder.</p> <code>40</code> <code>hidden_dropout</code> <code>`float`, *optional*, defaults to 0.0</code> <p>The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.</p> <code>0.0</code> <code>attention_dropout</code> <code>`float`, *optional*, defaults to 0.0</code> <p>The dropout ratio for the attention probabilities.</p> <code>0.0</code> <code>max_position_embeddings</code> <code>`int`, *optional*, defaults to 440</code> <p>The maximum sequence length that this model might ever be used with. Typically set this to something large just in case (e.g., 512 or 1024 or 2048).</p> <code>440</code> <code>initializer_range</code> <code>`float`, *optional*, defaults to 0.02</code> <p>The standard deviation of the truncated_normal_initializer for initializing all weight matrices.</p> <code>0.02</code> <code>layer_norm_eps</code> <code>`float`, *optional*, defaults to 1e-12</code> <p>The epsilon used by the layer normalization layers.</p> <code>1e-12</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import RnaBertModel, RnaBertConfig\n</code></pre> Python Console Session<pre><code>&gt;&gt;&gt; # Initializing a RNABERT multimolecule/rnabert style configuration\n&gt;&gt;&gt; configuration = RnaBertConfig()\n</code></pre> Python Console Session<pre><code>&gt;&gt;&gt; # Initializing a model (with random weights) from the multimolecule/rnabert style configuration\n&gt;&gt;&gt; model = RnaBertModel(configuration)\n</code></pre> Python Console Session<pre><code>&gt;&gt;&gt; # Accessing the model configuration\n&gt;&gt;&gt; configuration = model.config\n</code></pre> Source code in <code>multimolecule/models/rnabert/configuration_rnabert.py</code> Python<pre><code>class RnaBertConfig(PreTrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`RnaBertModel`]. It is used to instantiate a\n    RnaBert model according to the specified arguments, defining the model architecture. Instantiating a configuration\n    with the defaults will yield a similar configuration to that of the RnaBert\n    [mana438/RNABERT](https://github.com/mana438/RNABERT) architecture.\n\n    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n    documentation from [`PreTrainedConfig`] for more information.\n\n\n    Args:\n        vocab_size (`int`, *optional*, defaults to 25):\n            Vocabulary size of the RnaBert model. Defines the number of different tokens that can be represented by the\n            `inputs_ids` passed when calling [`RnaBertModel`].\n        hidden_size (`int`, *optional*, defaults to 120):\n            Dimensionality of the encoder layers and the pooler layer.\n        num_hidden_layers (`int`, *optional*, defaults to 12):\n            Number of hidden layers in the Transformer encoder.\n        num_attention_heads (`int`, *optional*, defaults to 6):\n            Number of attention heads for each attention layer in the Transformer encoder.\n        intermediate_size (`int`, *optional*, defaults to 40):\n            Dimensionality of the \"intermediate\" (often named feed-forward) layer in the Transformer encoder.\n        hidden_dropout (`float`, *optional*, defaults to 0.0):\n            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n        attention_dropout (`float`, *optional*, defaults to 0.0):\n            The dropout ratio for the attention probabilities.\n        max_position_embeddings (`int`, *optional*, defaults to 440):\n            The maximum sequence length that this model might ever be used with. Typically set this to something large\n            just in case (e.g., 512 or 1024 or 2048).\n        initializer_range (`float`, *optional*, defaults to 0.02):\n            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n        layer_norm_eps (`float`, *optional*, defaults to 1e-12):\n            The epsilon used by the layer normalization layers.\n\n    Examples:\n        &gt;&gt;&gt; from multimolecule import RnaBertModel, RnaBertConfig\n\n        &gt;&gt;&gt; # Initializing a RNABERT multimolecule/rnabert style configuration\n        &gt;&gt;&gt; configuration = RnaBertConfig()\n\n        &gt;&gt;&gt; # Initializing a model (with random weights) from the multimolecule/rnabert style configuration\n        &gt;&gt;&gt; model = RnaBertModel(configuration)\n\n        &gt;&gt;&gt; # Accessing the model configuration\n        &gt;&gt;&gt; configuration = model.config\n    \"\"\"\n\n    model_type = \"rnabert\"\n\n    def __init__(\n        self,\n        vocab_size=25,\n        ss_vocab_size=8,\n        hidden_size=None,\n        multiple=None,\n        num_hidden_layers=6,\n        num_attention_heads=12,\n        intermediate_size=40,\n        hidden_act=\"gelu\",\n        hidden_dropout=0.0,\n        attention_dropout=0.0,\n        max_position_embeddings=440,\n        initializer_range=0.02,\n        layer_norm_eps=1e-12,\n        position_embedding_type=\"absolute\",\n        use_cache=True,\n        head=None,\n        lm_head=None,\n        **kwargs,\n    ):\n        if hidden_size is None:\n            hidden_size = num_attention_heads * multiple if multiple is not None else 120\n        super().__init__(**kwargs)\n\n        self.vocab_size = vocab_size\n        self.ss_vocab_size = ss_vocab_size\n        self.type_vocab_size = 2\n        self.hidden_size = hidden_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.intermediate_size = intermediate_size\n        self.hidden_act = hidden_act\n        self.hidden_dropout = hidden_dropout\n        self.attention_dropout = attention_dropout\n        self.max_position_embeddings = max_position_embeddings\n        self.initializer_range = initializer_range\n        self.layer_norm_eps = layer_norm_eps\n        self.position_embedding_type = position_embedding_type\n        self.use_cache = use_cache\n        self.head = HeadConfig(**head if head is not None else {})\n        self.lm_head = MaskedLMHeadConfig(**lm_head if lm_head is not None else {})\n</code></pre>"},{"location":"models/rnabert/#multimolecule.models.rnabert.RnaBertForMaskedLM","title":"<code>RnaBertForMaskedLM</code>","text":"<p>               Bases: <code>RnaBertPreTrainedModel</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import RnaBertConfig, RnaBertForMaskedLM, RnaTokenizer\n&gt;&gt;&gt; config = RnaBertConfig()\n&gt;&gt;&gt; model = RnaBertForMaskedLM(config)\n&gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n&gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n&gt;&gt;&gt; output = model(**input)\n</code></pre> Source code in <code>multimolecule/models/rnabert/modeling_rnabert.py</code> Python<pre><code>class RnaBertForMaskedLM(RnaBertPreTrainedModel):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; from multimolecule import RnaBertConfig, RnaBertForMaskedLM, RnaTokenizer\n        &gt;&gt;&gt; config = RnaBertConfig()\n        &gt;&gt;&gt; model = RnaBertForMaskedLM(config)\n        &gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n        &gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n        &gt;&gt;&gt; output = model(**input)\n    \"\"\"\n\n    def __init__(self, config: RnaBertConfig):\n        super().__init__(config)\n        self.rnabert = RnaBertModel(config, add_pooling_layer=False)\n        self.lm_head = MaskedLMHead(config)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(\n        self,\n        input_ids: Tensor | NestedTensor,\n        attention_mask: Tensor | None = None,\n        labels: Tensor | None = None,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n    ) -&gt; Tuple[Tensor, ...] | MaskedLMOutput:\n        outputs = self.rnabert(\n            input_ids,\n            attention_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        output = self.lm_head(outputs, labels)\n        logits, loss = output.logits, output.loss\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return MaskedLMOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"models/rnabert/#multimolecule.models.rnabert.RnaBertForNucleotideClassification","title":"<code>RnaBertForNucleotideClassification</code>","text":"<p>               Bases: <code>RnaBertPreTrainedModel</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import RnaBertConfig, RnaBertForNucleotideClassification, RnaTokenizer\n&gt;&gt;&gt; config = RnaBertConfig()\n&gt;&gt;&gt; model = RnaBertForNucleotideClassification(config)\n&gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n&gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n&gt;&gt;&gt; output = model(**input)\n</code></pre> Source code in <code>multimolecule/models/rnabert/modeling_rnabert.py</code> Python<pre><code>class RnaBertForNucleotideClassification(RnaBertPreTrainedModel):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; from multimolecule import RnaBertConfig, RnaBertForNucleotideClassification, RnaTokenizer\n        &gt;&gt;&gt; config = RnaBertConfig()\n        &gt;&gt;&gt; model = RnaBertForNucleotideClassification(config)\n        &gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n        &gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n        &gt;&gt;&gt; output = model(**input)\n    \"\"\"\n\n    def __init__(self, config: RnaBertConfig):\n        super().__init__(config)\n        self.num_labels = config.head.num_labels\n        self.rnabert = RnaBertModel(config, add_pooling_layer=False)\n        self.nucleotide_head = NucleotideClassificationHead(config)\n        self.head_config = self.nucleotide_head.config\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(\n        self,\n        input_ids: Tensor | NestedTensor,\n        attention_mask: Tensor | None = None,\n        position_ids: Tensor | None = None,\n        head_mask: Tensor | None = None,\n        inputs_embeds: Tensor | NestedTensor | None = None,\n        labels: Tensor | None = None,\n        output_attentions: bool | None = None,\n        output_hidden_states: bool | None = None,\n        return_dict: bool | None = None,\n        **kwargs,\n    ) -&gt; Tuple[Tensor, ...] | TokenClassifierOutput:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n            `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        if kwargs:\n            warn(\n                f\"Additional keyword arguments `{', '.join(kwargs)}` are detected in \"\n                f\"`{self.__class__.__name__}.forward`, they will be ignored.\\n\"\n                \"This is provided for backward compatibility and may lead to unexpected behavior.\"\n            )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.rnabert(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        output = self.nucleotide_head(outputs, attention_mask, input_ids, labels)\n        logits, loss = output.logits, output.loss\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return TokenClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"models/rnabert/#multimolecule.models.rnabert.RnaBertForNucleotideClassification.forward","title":"<code>forward(input_ids, attention_mask=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None, output_attentions=None, output_hidden_states=None, return_dict=None, **kwargs)</code>","text":"<p>labels (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, optional):     Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ...,     config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If     <code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).</p> Source code in <code>multimolecule/models/rnabert/modeling_rnabert.py</code> Python<pre><code>def forward(\n    self,\n    input_ids: Tensor | NestedTensor,\n    attention_mask: Tensor | None = None,\n    position_ids: Tensor | None = None,\n    head_mask: Tensor | None = None,\n    inputs_embeds: Tensor | NestedTensor | None = None,\n    labels: Tensor | None = None,\n    output_attentions: bool | None = None,\n    output_hidden_states: bool | None = None,\n    return_dict: bool | None = None,\n    **kwargs,\n) -&gt; Tuple[Tensor, ...] | TokenClassifierOutput:\n    r\"\"\"\n    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n        `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).\n    \"\"\"\n    if kwargs:\n        warn(\n            f\"Additional keyword arguments `{', '.join(kwargs)}` are detected in \"\n            f\"`{self.__class__.__name__}.forward`, they will be ignored.\\n\"\n            \"This is provided for backward compatibility and may lead to unexpected behavior.\"\n        )\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n    outputs = self.rnabert(\n        input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        head_mask=head_mask,\n        inputs_embeds=inputs_embeds,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n    output = self.nucleotide_head(outputs, attention_mask, input_ids, labels)\n    logits, loss = output.logits, output.loss\n\n    if not return_dict:\n        output = (logits,) + outputs[2:]\n        return ((loss,) + output) if loss is not None else output\n\n    return TokenClassifierOutput(\n        loss=loss,\n        logits=logits,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"models/rnabert/#multimolecule.models.rnabert.RnaBertForPretraining","title":"<code>RnaBertForPretraining</code>","text":"<p>               Bases: <code>RnaBertPreTrainedModel</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import RnaBertConfig, RnaBertForPretraining, RnaTokenizer\n&gt;&gt;&gt; config = RnaBertConfig()\n&gt;&gt;&gt; model = RnaBertForPretraining(config)\n&gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n&gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n&gt;&gt;&gt; output = model(**input)\n</code></pre> Source code in <code>multimolecule/models/rnabert/modeling_rnabert.py</code> Python<pre><code>class RnaBertForPretraining(RnaBertPreTrainedModel):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; from multimolecule import RnaBertConfig, RnaBertForPretraining, RnaTokenizer\n        &gt;&gt;&gt; config = RnaBertConfig()\n        &gt;&gt;&gt; model = RnaBertForPretraining(config)\n        &gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n        &gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n        &gt;&gt;&gt; output = model(**input)\n    \"\"\"\n\n    def __init__(self, config: RnaBertConfig):\n        super().__init__(config)\n        self.rnabert = RnaBertModel(config, add_pooling_layer=True)\n        self.pretrain_head = RnaBertPreTrainingHeads(config)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(\n        self,\n        input_ids: Tensor | NestedTensor,\n        attention_mask: Tensor | None = None,\n        labels: Tensor | None = None,\n        labels_ss: Tensor | None = None,\n        next_sentence_label: Tensor | None = None,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n    ) -&gt; Tuple[Tensor, ...] | RnaBertForPretrainingOutput:\n        outputs = self.rnabert(\n            input_ids,\n            attention_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        logits, logits_ss, seq_relationship_score = self.pretrain_head(outputs)\n\n        loss = None\n        if any(x is not None for x in (labels, labels_ss, next_sentence_label)):\n            loss_mlm = loss_ss = loss_nsp = 0\n            if labels is not None:\n                loss_mlm = F.cross_entropy(logits.view(-1, self.config.vocab_size), labels.view(-1))\n            if labels_ss is not None:\n                loss_ss = F.cross_entropy(logits_ss.view(-1, self.config.ss_vocab_size), labels_ss.view(-1))\n            if next_sentence_label is not None:\n                loss_nsp = F.cross_entropy(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n            loss = loss_mlm + loss_ss + loss_nsp\n\n        if not return_dict:\n            output = (logits, logits_ss) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return RnaBertForPretrainingOutput(\n            loss=loss,\n            logits=logits,\n            logits_ss=logits_ss,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"models/rnabert/#multimolecule.models.rnabert.RnaBertForSequenceClassification","title":"<code>RnaBertForSequenceClassification</code>","text":"<p>               Bases: <code>RnaBertPreTrainedModel</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import RnaBertConfig, RnaBertForSequenceClassification, RnaTokenizer\n&gt;&gt;&gt; config = RnaBertConfig()\n&gt;&gt;&gt; model = RnaBertForSequenceClassification(config)\n&gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n&gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n&gt;&gt;&gt; output = model(**input)\n</code></pre> Source code in <code>multimolecule/models/rnabert/modeling_rnabert.py</code> Python<pre><code>class RnaBertForSequenceClassification(RnaBertPreTrainedModel):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; from multimolecule import RnaBertConfig, RnaBertForSequenceClassification, RnaTokenizer\n        &gt;&gt;&gt; config = RnaBertConfig()\n        &gt;&gt;&gt; model = RnaBertForSequenceClassification(config)\n        &gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n        &gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n        &gt;&gt;&gt; output = model(**input)\n    \"\"\"\n\n    def __init__(self, config: RnaBertConfig):\n        super().__init__(config)\n        self.num_labels = config.head.num_labels\n        self.rnabert = RnaBertModel(config, add_pooling_layer=True)\n        self.sequence_head = SequenceClassificationHead(config)\n        self.head_config = self.sequence_head.config\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(\n        self,\n        input_ids: Tensor | NestedTensor,\n        attention_mask: Tensor | None = None,\n        position_ids: Tensor | None = None,\n        head_mask: Tensor | None = None,\n        inputs_embeds: Tensor | NestedTensor | None = None,\n        labels: Tensor | None = None,\n        output_attentions: bool | None = None,\n        output_hidden_states: bool | None = None,\n        return_dict: bool | None = None,\n    ) -&gt; Tuple[Tensor, ...] | SequenceClassifierOutput:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n            `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.rnabert(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        output = self.sequence_head(outputs, labels)\n        logits, loss = output.logits, output.loss\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"models/rnabert/#multimolecule.models.rnabert.RnaBertForSequenceClassification.forward","title":"<code>forward(input_ids, attention_mask=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None, output_attentions=None, output_hidden_states=None, return_dict=None)</code>","text":"<p>labels (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, optional):     Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ...,     config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If     <code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).</p> Source code in <code>multimolecule/models/rnabert/modeling_rnabert.py</code> Python<pre><code>def forward(\n    self,\n    input_ids: Tensor | NestedTensor,\n    attention_mask: Tensor | None = None,\n    position_ids: Tensor | None = None,\n    head_mask: Tensor | None = None,\n    inputs_embeds: Tensor | NestedTensor | None = None,\n    labels: Tensor | None = None,\n    output_attentions: bool | None = None,\n    output_hidden_states: bool | None = None,\n    return_dict: bool | None = None,\n) -&gt; Tuple[Tensor, ...] | SequenceClassifierOutput:\n    r\"\"\"\n    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n        `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).\n    \"\"\"\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n    outputs = self.rnabert(\n        input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        head_mask=head_mask,\n        inputs_embeds=inputs_embeds,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n    output = self.sequence_head(outputs, labels)\n    logits, loss = output.logits, output.loss\n\n    if not return_dict:\n        output = (logits,) + outputs[2:]\n        return ((loss,) + output) if loss is not None else output\n\n    return SequenceClassifierOutput(\n        loss=loss,\n        logits=logits,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"models/rnabert/#multimolecule.models.rnabert.RnaBertForTokenClassification","title":"<code>RnaBertForTokenClassification</code>","text":"<p>               Bases: <code>RnaBertPreTrainedModel</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import RnaBertConfig, RnaBertForTokenClassification, RnaTokenizer\n&gt;&gt;&gt; config = RnaBertConfig()\n&gt;&gt;&gt; model = RnaBertForTokenClassification(config)\n&gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n&gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n&gt;&gt;&gt; output = model(**input)\n</code></pre> Source code in <code>multimolecule/models/rnabert/modeling_rnabert.py</code> Python<pre><code>class RnaBertForTokenClassification(RnaBertPreTrainedModel):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; from multimolecule import RnaBertConfig, RnaBertForTokenClassification, RnaTokenizer\n        &gt;&gt;&gt; config = RnaBertConfig()\n        &gt;&gt;&gt; model = RnaBertForTokenClassification(config)\n        &gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n        &gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n        &gt;&gt;&gt; output = model(**input)\n    \"\"\"\n\n    def __init__(self, config: RnaBertConfig):\n        super().__init__(config)\n        self.num_labels = config.head.num_labels\n        self.rnabert = RnaBertModel(config, add_pooling_layer=False)\n        self.token_head = TokenClassificationHead(config)\n        self.head_config = self.token_head.config\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(\n        self,\n        input_ids: Tensor | NestedTensor,\n        attention_mask: Tensor | None = None,\n        position_ids: Tensor | None = None,\n        head_mask: Tensor | None = None,\n        inputs_embeds: Tensor | NestedTensor | None = None,\n        labels: Tensor | None = None,\n        output_attentions: bool | None = None,\n        output_hidden_states: bool | None = None,\n        return_dict: bool | None = None,\n    ) -&gt; Tuple[Tensor, ...] | TokenClassifierOutput:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n            `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.rnabert(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        output = self.token_head(outputs, attention_mask, input_ids, labels)\n        logits, loss = output.logits, output.loss\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return TokenClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"models/rnabert/#multimolecule.models.rnabert.RnaBertForTokenClassification.forward","title":"<code>forward(input_ids, attention_mask=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None, output_attentions=None, output_hidden_states=None, return_dict=None)</code>","text":"<p>labels (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, optional):     Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ...,     config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If     <code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).</p> Source code in <code>multimolecule/models/rnabert/modeling_rnabert.py</code> Python<pre><code>def forward(\n    self,\n    input_ids: Tensor | NestedTensor,\n    attention_mask: Tensor | None = None,\n    position_ids: Tensor | None = None,\n    head_mask: Tensor | None = None,\n    inputs_embeds: Tensor | NestedTensor | None = None,\n    labels: Tensor | None = None,\n    output_attentions: bool | None = None,\n    output_hidden_states: bool | None = None,\n    return_dict: bool | None = None,\n) -&gt; Tuple[Tensor, ...] | TokenClassifierOutput:\n    r\"\"\"\n    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n        `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).\n    \"\"\"\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n    outputs = self.rnabert(\n        input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        head_mask=head_mask,\n        inputs_embeds=inputs_embeds,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n    output = self.token_head(outputs, attention_mask, input_ids, labels)\n    logits, loss = output.logits, output.loss\n\n    if not return_dict:\n        output = (logits,) + outputs[2:]\n        return ((loss,) + output) if loss is not None else output\n\n    return TokenClassifierOutput(\n        loss=loss,\n        logits=logits,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"models/rnabert/#multimolecule.models.rnabert.RnaBertModel","title":"<code>RnaBertModel</code>","text":"<p>               Bases: <code>RnaBertPreTrainedModel</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import RnaBertConfig, RnaBertModel, RnaTokenizer\n&gt;&gt;&gt; config = RnaBertConfig()\n&gt;&gt;&gt; model = RnaBertModel(config)\n&gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n&gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n&gt;&gt;&gt; output = model(**input)\n</code></pre> Source code in <code>multimolecule/models/rnabert/modeling_rnabert.py</code> Python<pre><code>class RnaBertModel(RnaBertPreTrainedModel):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; from multimolecule import RnaBertConfig, RnaBertModel, RnaTokenizer\n        &gt;&gt;&gt; config = RnaBertConfig()\n        &gt;&gt;&gt; model = RnaBertModel(config)\n        &gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n        &gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n        &gt;&gt;&gt; output = model(**input)\n    \"\"\"\n\n    def __init__(self, config: RnaBertConfig, add_pooling_layer: bool = True):\n        super().__init__(config)\n        self.pad_token_id = config.pad_token_id\n        self.embeddings = RnaBertEmbeddings(config)\n        self.encoder = RnaBertEncoder(config)\n        self.pooler = RnaBertPooler(config) if add_pooling_layer else None\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.embeddings.word_embeddings\n\n    def set_input_embeddings(self, value):\n        self.embeddings.word_embeddings = value\n\n    def _prune_heads(self, heads_to_prune):\n        \"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"\n        for layer, heads in heads_to_prune.items():\n            self.encoder.layer[layer].attention.prune_heads(heads)\n\n    def forward(\n        self,\n        input_ids: Tensor | NestedTensor,\n        attention_mask: Tensor | None = None,\n        position_ids: Tensor | None = None,\n        head_mask: Tensor | None = None,\n        inputs_embeds: Tensor | NestedTensor | None = None,\n        encoder_hidden_states: Tensor | None = None,\n        encoder_attention_mask: Tensor | None = None,\n        past_key_values: Tuple[Tuple[torch.FloatTensor, torch.FloatTensor], ...] | None = None,\n        use_cache: bool | None = None,\n        output_attentions: bool | None = None,\n        output_hidden_states: bool | None = None,\n        return_dict: bool | None = None,\n    ) -&gt; Tuple[Tensor, ...] | BaseModelOutputWithPoolingAndCrossAttentions:\n        r\"\"\"\n        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n            the model is configured as a decoder.\n        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors\n            of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n\n            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n            `past_key_values`).\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if self.config.is_decoder:\n            use_cache = use_cache if use_cache is not None else self.config.use_cache\n        else:\n            use_cache = False\n\n        if isinstance(input_ids, NestedTensor):\n            input_ids, attention_mask = input_ids.tensor, input_ids.mask\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n        if input_ids is not None:\n            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n            input_shape = input_ids.size()\n        elif inputs_embeds is not None:\n            input_shape = inputs_embeds.size()[:-1]\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n        batch_size, seq_length = input_shape\n        device = input_ids.device if input_ids is not None else inputs_embeds.device  # type: ignore[union-attr]\n\n        # past_key_values_length\n        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n\n        if attention_mask is None:\n            attention_mask = (\n                input_ids.ne(self.pad_token_id)\n                if self.pad_token_id is not None\n                else torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n            )\n\n        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n        # ourselves in which case we just need to make it broadcastable to all heads.\n        extended_attention_mask: Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n\n        # If a 2D or 3D attention mask is provided for the cross-attention\n        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n        if self.config.is_decoder and encoder_hidden_states is not None:\n            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n            if encoder_attention_mask is None:\n                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n        else:\n            encoder_extended_attention_mask = None\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n\n        embedding_output = self.embeddings(\n            input_ids=input_ids,\n            position_ids=position_ids,\n            inputs_embeds=inputs_embeds,\n            past_key_values_length=past_key_values_length,\n        )\n        encoder_outputs = self.encoder(\n            embedding_output,\n            attention_mask=extended_attention_mask,\n            head_mask=head_mask,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_extended_attention_mask,\n            past_key_values=past_key_values,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        sequence_output = encoder_outputs[0]\n        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n\n        if not return_dict:\n            return (sequence_output, pooled_output) + encoder_outputs[1:]\n\n        return BaseModelOutputWithPoolingAndCrossAttentions(\n            last_hidden_state=sequence_output,\n            pooler_output=pooled_output,\n            past_key_values=encoder_outputs.past_key_values,\n            hidden_states=encoder_outputs.hidden_states,\n            attentions=encoder_outputs.attentions,\n            cross_attentions=encoder_outputs.cross_attentions,\n        )\n</code></pre>"},{"location":"models/rnabert/#multimolecule.models.rnabert.RnaBertModel.forward","title":"<code>forward(input_ids, attention_mask=None, position_ids=None, head_mask=None, inputs_embeds=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_values=None, use_cache=None, output_attentions=None, output_hidden_states=None, return_dict=None)</code>","text":"<p>encoder_hidden_states  (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, optional):     Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if     the model is configured as a decoder. encoder_attention_mask (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, optional):     Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in     the cross-attention if the model is configured as a decoder. Mask values selected in <code>[0, 1]</code>:</p> Text Only<pre><code>- 1 for tokens that are **not masked**,\n- 0 for tokens that are **masked**.\n</code></pre> <p>past_key_values (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors     of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>):     Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.</p> Text Only<pre><code>If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\ndon't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n`decoder_input_ids` of shape `(batch_size, sequence_length)`.\n</code></pre> <p>use_cache (<code>bool</code>, optional):     If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see     <code>past_key_values</code>).</p> Source code in <code>multimolecule/models/rnabert/modeling_rnabert.py</code> Python<pre><code>def forward(\n    self,\n    input_ids: Tensor | NestedTensor,\n    attention_mask: Tensor | None = None,\n    position_ids: Tensor | None = None,\n    head_mask: Tensor | None = None,\n    inputs_embeds: Tensor | NestedTensor | None = None,\n    encoder_hidden_states: Tensor | None = None,\n    encoder_attention_mask: Tensor | None = None,\n    past_key_values: Tuple[Tuple[torch.FloatTensor, torch.FloatTensor], ...] | None = None,\n    use_cache: bool | None = None,\n    output_attentions: bool | None = None,\n    output_hidden_states: bool | None = None,\n    return_dict: bool | None = None,\n) -&gt; Tuple[Tensor, ...] | BaseModelOutputWithPoolingAndCrossAttentions:\n    r\"\"\"\n    encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n        Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n        the model is configured as a decoder.\n    encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n        Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n        the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n\n        - 1 for tokens that are **not masked**,\n        - 0 for tokens that are **masked**.\n    past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors\n        of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n        Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n\n        If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n        don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n        `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n    use_cache (`bool`, *optional*):\n        If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n        `past_key_values`).\n    \"\"\"\n    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n    output_hidden_states = (\n        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n    )\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n    if self.config.is_decoder:\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n    else:\n        use_cache = False\n\n    if isinstance(input_ids, NestedTensor):\n        input_ids, attention_mask = input_ids.tensor, input_ids.mask\n    if input_ids is not None and inputs_embeds is not None:\n        raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n    if input_ids is not None:\n        self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n        input_shape = input_ids.size()\n    elif inputs_embeds is not None:\n        input_shape = inputs_embeds.size()[:-1]\n    else:\n        raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n    batch_size, seq_length = input_shape\n    device = input_ids.device if input_ids is not None else inputs_embeds.device  # type: ignore[union-attr]\n\n    # past_key_values_length\n    past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n\n    if attention_mask is None:\n        attention_mask = (\n            input_ids.ne(self.pad_token_id)\n            if self.pad_token_id is not None\n            else torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n        )\n\n    # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n    # ourselves in which case we just need to make it broadcastable to all heads.\n    extended_attention_mask: Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n\n    # If a 2D or 3D attention mask is provided for the cross-attention\n    # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n    if self.config.is_decoder and encoder_hidden_states is not None:\n        encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n        encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n        if encoder_attention_mask is None:\n            encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n        encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n    else:\n        encoder_extended_attention_mask = None\n\n    # Prepare head mask if needed\n    # 1.0 in head_mask indicate we keep the head\n    # attention_probs has shape bsz x n_heads x N x N\n    # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n    # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n    head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n\n    embedding_output = self.embeddings(\n        input_ids=input_ids,\n        position_ids=position_ids,\n        inputs_embeds=inputs_embeds,\n        past_key_values_length=past_key_values_length,\n    )\n    encoder_outputs = self.encoder(\n        embedding_output,\n        attention_mask=extended_attention_mask,\n        head_mask=head_mask,\n        encoder_hidden_states=encoder_hidden_states,\n        encoder_attention_mask=encoder_extended_attention_mask,\n        past_key_values=past_key_values,\n        use_cache=use_cache,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n    sequence_output = encoder_outputs[0]\n    pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n\n    if not return_dict:\n        return (sequence_output, pooled_output) + encoder_outputs[1:]\n\n    return BaseModelOutputWithPoolingAndCrossAttentions(\n        last_hidden_state=sequence_output,\n        pooler_output=pooled_output,\n        past_key_values=encoder_outputs.past_key_values,\n        hidden_states=encoder_outputs.hidden_states,\n        attentions=encoder_outputs.attentions,\n        cross_attentions=encoder_outputs.cross_attentions,\n    )\n</code></pre>"},{"location":"models/rnabert/#multimolecule.models.rnabert.RnaBertPreTrainedModel","title":"<code>RnaBertPreTrainedModel</code>","text":"<p>               Bases: <code>PreTrainedModel</code></p> <p>An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.</p> Source code in <code>multimolecule/models/rnabert/modeling_rnabert.py</code> Python<pre><code>class RnaBertPreTrainedModel(PreTrainedModel):\n    \"\"\"\n    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n    models.\n    \"\"\"\n\n    config_class = RnaBertConfig\n    base_model_prefix = \"rnabert\"\n    supports_gradient_checkpointing = True\n    _no_split_modules = [\"RnaBertLayer\", \"RnaBertEmbeddings\"]\n\n    # Copied from transformers.models.bert.modeling_bert.BertPreTrainedModel._init_weights\n    def _init_weights(self, module: nn.Module):\n        \"\"\"Initialize the weights\"\"\"\n        if isinstance(module, nn.Linear):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n</code></pre>"},{"location":"models/rnafm/","title":"RNA-FM","text":"<p>Pre-trained model on non-coding RNA (ncRNA) and mRNA CoDing Sequence (CDS) using a masked language modeling (MLM) objective.</p>"},{"location":"models/rnafm/#disclaimer","title":"Disclaimer","text":"<p>This is an UNOFFICIAL implementation of the Interpretable RNA Foundation Model from Unannotated Data for Highly Accurate RNA Structure and Function Predictions by Jiayang Chen, Zhihang Hue, Siqi Sun, et al.</p> <p>The OFFICIAL repository of RNA-FM is at ml4bio/RNA-FM.</p> <p>The team releasing RNA-FM did not write this model card for this model so this model card has been written by the MultiMolecule team.</p>"},{"location":"models/rnafm/#model-details","title":"Model Details","text":"<p>RNA-FM is a bert-style model pre-trained on a large corpus of non-coding RNA sequences in a self-supervised fashion. This means that the model was trained on the raw nucleotides of RNA sequences only, with an automatic process to generate inputs and labels from those texts. Please refer to the Training Details section for more information on the training process.</p>"},{"location":"models/rnafm/#variations","title":"Variations","text":"<ul> <li><code>multimolecule/rnafm</code>: The RNA-FM model pre-trained on non-coding RNA sequences.</li> <li><code>multimolecule/mrnafm</code>: The RNA-FM model pre-trained on mRNA coding sequences.</li> </ul>"},{"location":"models/rnafm/#model-specification","title":"Model Specification","text":"Variants Num Layers Hidden Size Num Heads Intermediate Size Num Parameters (M) FLOPs (G) MACs (G) Max Num Tokens RNA-FM 12 640 20 5120 99.52 25.68 12.83 1024 mRNA-FM 1280 239.25 61.43 30.7"},{"location":"models/rnafm/#links","title":"Links","text":"<ul> <li>Code: multimolecule.rnafm</li> <li>Data: RNAcentral</li> <li>Paper: Interpretable RNA Foundation Model from Unannotated Data for Highly Accurate RNA Structure and Function Predictions</li> <li>Developed by: Jiayang Chen, Zhihang Hu, Siqi Sun, Qingxiong Tan, Yixuan Wang, Qinze Yu, Licheng Zong, Liang Hong, Jin Xiao, Tao Shen, Irwin King, Yu Li</li> <li>Model type: BERT - ESM</li> <li>Original Repository: https://github.com/ml4bio/RNA-FM</li> </ul>"},{"location":"models/rnafm/#usage","title":"Usage","text":"<p>The model file depends on the <code>multimolecule</code> library. You can install it using pip:</p> Bash<pre><code>pip install multimolecule\n</code></pre>"},{"location":"models/rnafm/#direct-use","title":"Direct Use","text":"<p>You can use this model directly with a pipeline for masked language modeling:</p> Python<pre><code>&gt;&gt;&gt; import multimolecule  # you must import multimolecule to register models\n&gt;&gt;&gt; from transformers import pipeline\n&gt;&gt;&gt; unmasker = pipeline('fill-mask', model='multimolecule/rnafm')\n&gt;&gt;&gt; unmasker(\"uagc&lt;mask&gt;uaucagacugauguuga\")\n\n[{'score': 0.3830885887145996,\n  'token': 23,\n  'token_str': '*',\n  'sequence': 'U A G C * U A U C A G A C U G A U G U U G A'},\n {'score': 0.16808930039405823,\n  'token': 22,\n  'token_str': '.',\n  'sequence': 'U A G C. U A U C A G A C U G A U G U U G A'},\n {'score': 0.14214453101158142,\n  'token': 6,\n  'token_str': 'A',\n  'sequence': 'U A G C A U A U C A G A C U G A U G U U G A'},\n {'score': 0.11032014340162277,\n  'token': 9,\n  'token_str': 'U',\n  'sequence': 'U A G C U U A U C A G A C U G A U G U U G A'},\n {'score': 0.09523089975118637,\n  'token': 7,\n  'token_str': 'C',\n  'sequence': 'U A G C C U A U C A G A C U G A U G U U G A'}]\n</code></pre>"},{"location":"models/rnafm/#downstream-use","title":"Downstream Use","text":""},{"location":"models/rnafm/#extract-features","title":"Extract Features","text":"<p>Here is how to use this model to get the features of a given sequence in PyTorch:</p> Python<pre><code>from multimolecule import RnaTokenizer, RnaFmModel\n\n\ntokenizer = RnaTokenizer.from_pretrained('multimolecule/rnafm')\nmodel = RnaFmModel.from_pretrained('multimolecule/rnafm')\n\ntext = \"UAGCUUAUCAGACUGAUGUUGA\"\ninput = tokenizer(text, return_tensors='pt')\n\noutput = model(**input)\n</code></pre>"},{"location":"models/rnafm/#sequence-classification-regression","title":"Sequence Classification / Regression","text":"<p>Note: This model is not fine-tuned for any specific task. You will need to fine-tune the model on a downstream task to use it for sequence classification or regression.</p> <p>Here is how to use this model as backbone to fine-tune for a sequence-level task in PyTorch:</p> Python<pre><code>import torch\nfrom multimolecule import RnaTokenizer, RnaFmForSequenceClassification\n\n\ntokenizer = RnaTokenizer.from_pretrained('multimolecule/rnafm')\nmodel = RnaFmForSequenceClassification.from_pretrained('multimolecule/rnafm')\n\ntext = \"UAGCUUAUCAGACUGAUGUUGA\"\ninput = tokenizer(text, return_tensors='pt')\nlabel = torch.tensor([1])\n\noutput = model(**input, labels=label)\n</code></pre>"},{"location":"models/rnafm/#nucleotide-classification-regression","title":"Nucleotide Classification / Regression","text":"<p>Note: This model is not fine-tuned for any specific task. You will need to fine-tune the model on a downstream task to use it for nucleotide classification or regression.</p> <p>Here is how to use this model as backbone to fine-tune for a nucleotide-level task in PyTorch:</p> Python<pre><code>import torch\nfrom multimolecule import RnaTokenizer, RnaFmForNucleotideClassification\n\n\ntokenizer = RnaTokenizer.from_pretrained('multimolecule/rnafm')\nmodel = RnaFmForNucleotideClassification.from_pretrained('multimolecule/rnafm')\n\ntext = \"UAGCUUAUCAGACUGAUGUUGA\"\ninput = tokenizer(text, return_tensors='pt')\nlabel = torch.randint(2, (len(text), ))\n\noutput = model(**input, labels=label)\n</code></pre>"},{"location":"models/rnafm/#training-details","title":"Training Details","text":"<p>RNA-FM used Masked Language Modeling (MLM) as the pre-training objective: taking a sequence, the model randomly masks 15% of the tokens in the input then run the entire masked sentence through the model and has to predict the masked tokens. This is comparable to the Cloze task in language modeling.</p>"},{"location":"models/rnafm/#training-data","title":"Training Data","text":"<p>The RNA-FM model was pre-trained on RNAcentral. RNAcentral is a comprehensive database of non-coding RNA sequences from a wide range of species. It combines 47 different databases, adding up to around 27 million RNA sequences in total.</p> <p>RNA-FM applied CD-HIT (CD-HIT-EST) with a cut-off at 100% sequence identity to remove redundancy from the RNAcentral. The final dataset contains 23.7 million non-redundant RNA sequences.</p> <p>RNA-FM preprocessed all tokens by replacing \u201cU\u201ds with \u201cT\u201ds.</p> <p>Note that during model conversions, \u201cT\u201d is replaced with \u201cU\u201d. <code>RnaTokenizer</code> will convert \u201cT\u201ds to \u201cU\u201ds for you, you may disable this behaviour by passing <code>replace_T_with_U=False</code>.</p>"},{"location":"models/rnafm/#training-procedure","title":"Training Procedure","text":""},{"location":"models/rnafm/#preprocessing","title":"Preprocessing","text":"<p>RNA-FM used masked language modeling (MLM) as the pre-training objective. The masking procedure is similar to the one used in BERT:</p> <ul> <li>15% of the tokens are masked.</li> <li>In 80% of the cases, the masked tokens are replaced by <code>&lt;mask&gt;</code>.</li> <li>In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.</li> <li>In the 10% remaining cases, the masked tokens are left as is.</li> </ul>"},{"location":"models/rnafm/#pretraining","title":"Pretraining","text":"<p>The model was trained on 8 NVIDIA A100 GPUs with 80GiB memories.</p> <ul> <li>Learning rate: 1e-4</li> <li>Weight decay: 0.01</li> <li>Learning rate scheduler: inverse square root</li> <li>Learning rate warm-up: 10,000 steps</li> </ul>"},{"location":"models/rnafm/#citation","title":"Citation","text":"<p>BibTeX:</p> BibTeX<pre><code>@article{chen2022interpretable,\n  title={Interpretable rna foundation model from unannotated data for highly accurate rna structure and function predictions},\n  author={Chen, Jiayang and Hu, Zhihang and Sun, Siqi and Tan, Qingxiong and Wang, Yixuan and Yu, Qinze and Zong, Licheng and Hong, Liang and Xiao, Jin and King, Irwin and others},\n  journal={arXiv preprint arXiv:2204.00300},\n  year={2022}\n}\n</code></pre>"},{"location":"models/rnafm/#contact","title":"Contact","text":"<p>Please use GitHub issues of MultiMolecule for any questions or comments on the model card.</p> <p>Please contact the authors of the RNA-FM paper for questions or comments on the paper/model.</p>"},{"location":"models/rnafm/#license","title":"License","text":"<p>This model is licensed under the AGPL-3.0 License.</p> Text Only<pre><code>SPDX-License-Identifier: AGPL-3.0-or-later\n</code></pre>"},{"location":"models/rnafm/#multimolecule.models.rnafm","title":"<code>multimolecule.models.rnafm</code>","text":""},{"location":"models/rnafm/#multimolecule.models.rnafm.RnaTokenizer","title":"<code>RnaTokenizer</code>","text":"<p>               Bases: <code>Tokenizer</code></p> <p>Constructs an RNA tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>alphabet</code> <code>List[str] | None</code> <p>List of tokens to use. Defaults to IUPAC nucleotide code.</p> <code>None</code> <code>nmers</code> <code>int</code> <p>Size of nmers to tokenize. Defaults to 1.</p> <code>1</code> <code>codon</code> <code>bool</code> <p>Whether to tokenize into codons. Defaults to False.</p> <code>False</code> <code>replace_T_with_U</code> <code>bool</code> <p>Whether to replace T with U. Defaults to True.</p> <code>True</code> <code>do_upper_case</code> <code>bool</code> <p>Whether to convert input to uppercase. Defaults to True.</p> <code>True</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import RnaTokenizer\n&gt;&gt;&gt; tokenizer = RnaTokenizer()\n&gt;&gt;&gt; tokenizer('&lt;pad&gt;&lt;cls&gt;&lt;eos&gt;&lt;unk&gt;&lt;mask&gt;&lt;null&gt;ACGUNXVHDBMRWSYK.*-')[\"input_ids\"]\n[1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 2]\n&gt;&gt;&gt; tokenizer('acgu')[\"input_ids\"]\n[1, 6, 7, 8, 9, 2]\n&gt;&gt;&gt; tokenizer('acgt')[\"input_ids\"]\n[1, 6, 7, 8, 9, 2]\n&gt;&gt;&gt; tokenizer = RnaTokenizer(replace_T_with_U=False)\n&gt;&gt;&gt; tokenizer('acgt')[\"input_ids\"]\n[1, 6, 7, 8, 3, 2]\n&gt;&gt;&gt; tokenizer = RnaTokenizer(nmers=3)\n&gt;&gt;&gt; tokenizer('uagcuuauc')[\"input_ids\"]\n[1, 83, 17, 64, 49, 96, 84, 22, 2]\n&gt;&gt;&gt; tokenizer = RnaTokenizer(codon=True)\n&gt;&gt;&gt; tokenizer('uagcuuauc')[\"input_ids\"]\n[1, 83, 49, 22, 2]\n&gt;&gt;&gt; tokenizer('uagcuuauca')[\"input_ids\"]\nTraceback (most recent call last):\nValueError: length of input sequence  must be a multiple of 3 for codon tokenization, but got 10\n</code></pre> Source code in <code>multimolecule/tokenisers/rna/tokenization_rna.py</code> Python<pre><code>class RnaTokenizer(Tokenizer):\n    \"\"\"\n    Constructs an RNA tokenizer.\n\n    Args:\n        alphabet (List[str] | None, optional): List of tokens to use.\n            Defaults to [IUPAC nucleotide code](https://www.bioinformatics.org/sms2/iupac.html).\n        nmers (int, optional): Size of nmers to tokenize.\n            Defaults to 1.\n        codon (bool, optional): Whether to tokenize into codons.\n            Defaults to False.\n        replace_T_with_U (bool, optional): Whether to replace T with U.\n            Defaults to True.\n        do_upper_case (bool, optional): Whether to convert input to uppercase.\n            Defaults to True.\n\n    Examples:\n        &gt;&gt;&gt; from multimolecule import RnaTokenizer\n        &gt;&gt;&gt; tokenizer = RnaTokenizer()\n        &gt;&gt;&gt; tokenizer('&lt;pad&gt;&lt;cls&gt;&lt;eos&gt;&lt;unk&gt;&lt;mask&gt;&lt;null&gt;ACGUNXVHDBMRWSYK.*-')[\"input_ids\"]\n        [1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 2]\n        &gt;&gt;&gt; tokenizer('acgu')[\"input_ids\"]\n        [1, 6, 7, 8, 9, 2]\n        &gt;&gt;&gt; tokenizer('acgt')[\"input_ids\"]\n        [1, 6, 7, 8, 9, 2]\n        &gt;&gt;&gt; tokenizer = RnaTokenizer(replace_T_with_U=False)\n        &gt;&gt;&gt; tokenizer('acgt')[\"input_ids\"]\n        [1, 6, 7, 8, 3, 2]\n        &gt;&gt;&gt; tokenizer = RnaTokenizer(nmers=3)\n        &gt;&gt;&gt; tokenizer('uagcuuauc')[\"input_ids\"]\n        [1, 83, 17, 64, 49, 96, 84, 22, 2]\n        &gt;&gt;&gt; tokenizer = RnaTokenizer(codon=True)\n        &gt;&gt;&gt; tokenizer('uagcuuauc')[\"input_ids\"]\n        [1, 83, 49, 22, 2]\n        &gt;&gt;&gt; tokenizer('uagcuuauca')[\"input_ids\"]\n        Traceback (most recent call last):\n        ValueError: length of input sequence  must be a multiple of 3 for codon tokenization, but got 10\n    \"\"\"\n\n    model_input_names = [\"input_ids\", \"attention_mask\"]\n\n    def __init__(\n        self,\n        alphabet: List[str] | None = None,\n        nmers: int = 1,\n        codon: bool = False,\n        replace_T_with_U: bool = True,\n        bos_token: str = \"&lt;cls&gt;\",\n        cls_token: str = \"&lt;cls&gt;\",\n        pad_token: str = \"&lt;pad&gt;\",\n        eos_token: str = \"&lt;eos&gt;\",\n        sep_token: str = \"&lt;eos&gt;\",\n        unk_token: str = \"&lt;unk&gt;\",\n        mask_token: str = \"&lt;mask&gt;\",\n        additional_special_tokens: List | Tuple | None = None,\n        do_upper_case: bool = True,\n        **kwargs,\n    ):\n        if codon and nmers &gt; 1:\n            raise ValueError(\"Codon and nmers cannot be used together.\")\n        if codon:\n            nmers = 3  # set to 3 to get correct vocab\n        super().__init__(\n            alphabet=get_vocab_list(alphabet, nmers),\n            bos_token=bos_token,\n            cls_token=cls_token,\n            pad_token=pad_token,\n            eos_token=eos_token,\n            sep_token=sep_token,\n            unk_token=unk_token,\n            mask_token=mask_token,\n            additional_special_tokens=additional_special_tokens,\n            do_upper_case=do_upper_case,\n            **kwargs,\n        )\n        self.replace_T_with_U = replace_T_with_U\n        self.nmers = nmers\n        self.condon = codon\n\n    def _tokenize(self, text: str, **kwargs):\n        if self.do_upper_case:\n            text = text.upper()\n        if self.replace_T_with_U:\n            text = text.replace(\"T\", \"U\")\n        if self.condon:\n            if len(text) % 3 != 0:\n                raise ValueError(\n                    f\"length of input sequence  must be a multiple of 3 for codon tokenization, but got {len(text)}\"\n                )\n            return [text[i : i + 3] for i in range(0, len(text), 3)]\n        if self.nmers &gt; 1:\n            return [text[i : i + self.nmers] for i in range(len(text) - self.nmers + 1)]  # noqa: E203\n        return list(text)\n</code></pre>"},{"location":"models/rnafm/#multimolecule.models.rnafm.RnaFmConfig","title":"<code>RnaFmConfig</code>","text":"<p>               Bases: <code>PreTrainedConfig</code></p> <p>This is the configuration class to store the configuration of a [<code>RnaFmModel</code>]. It is used to instantiate a RNA-FM model according to the specified arguments, defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of the RNA-FM ml4bio/RNA-FM architecture.</p> <p>Configuration objects inherit from [<code>PreTrainedConfig</code>] and can be used to control the model outputs. Read the documentation from [<code>PreTrainedConfig</code>] for more information.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_size</code> <code>`int`, *optional*, defaults to 25 if `codon=False` else 131</code> <p>Vocabulary size of the RNA-FM model. Defines the number of different tokens that can be represented by the <code>inputs_ids</code> passed when calling [<code>RnaFmModel</code>].</p> <code>None</code> <code>codon</code> <code>`bool`, *optional*, defaults to `False`</code> <p>Whether to use codon tokenization.</p> <code>False</code> <code>hidden_size</code> <code>`int`, *optional*, defaults to 640</code> <p>Dimensionality of the encoder layers and the pooler layer.</p> <code>640</code> <code>num_hidden_layers</code> <code>`int`, *optional*, defaults to 12</code> <p>Number of hidden layers in the Transformer encoder.</p> <code>12</code> <code>num_attention_heads</code> <code>`int`, *optional*, defaults to 20</code> <p>Number of attention heads for each attention layer in the Transformer encoder.</p> <code>20</code> <code>intermediate_size</code> <code>`int`, *optional*, defaults to 5120</code> <p>Dimensionality of the \u201cintermediate\u201d (often named feed-forward) layer in the Transformer encoder.</p> <code>5120</code> <code>hidden_dropout</code> <code>`float`, *optional*, defaults to 0.1</code> <p>The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.</p> <code>0.1</code> <code>attention_dropout</code> <code>`float`, *optional*, defaults to 0.1</code> <p>The dropout ratio for the attention probabilities.</p> <code>0.1</code> <code>max_position_embeddings</code> <code>`int`, *optional*, defaults to 1026</code> <p>The maximum sequence length that this model might ever be used with. Typically set this to something large just in case (e.g., 512 or 1024 or 2048).</p> <code>1026</code> <code>initializer_range</code> <code>`float`, *optional*, defaults to 0.02</code> <p>The standard deviation of the truncated_normal_initializer for initializing all weight matrices.</p> <code>0.02</code> <code>layer_norm_eps</code> <code>`float`, *optional*, defaults to 1e-12</code> <p>The epsilon used by the layer normalization layers.</p> <code>1e-12</code> <code>position_embedding_type</code> <code>`str`, *optional*, defaults to `\"absolute\"`</code> <p>Type of position embedding. Choose one of <code>\"absolute\"</code>, <code>\"relative_key\"</code>, <code>\"relative_key_query\", \"rotary\"</code>. For positional embeddings use <code>\"absolute\"</code>. For more information on <code>\"relative_key\"</code>, please refer to Self-Attention with Relative Position Representations (Shaw et al.). For more information on <code>\"relative_key_query\"</code>, please refer to Method 4 in Improve Transformer Models with Better Relative Position Embeddings (Huang et al.).</p> <code>'absolute'</code> <code>is_decoder</code> <code>`bool`, *optional*, defaults to `False`</code> <p>Whether the model is used as a decoder or not. If <code>False</code>, the model is used as an encoder.</p> required <code>use_cache</code> <code>`bool`, *optional*, defaults to `True`</code> <p>Whether or not the model should return the last key/values attentions (not used by all models). Only relevant if <code>config.is_decoder=True</code>.</p> <code>True</code> <code>emb_layer_norm_before</code> <code>`bool`, *optional*</code> <p>Whether to apply layer normalization after embeddings but before the main stem of the network.</p> <code>True</code> <code>token_dropout</code> <code>`bool`, defaults to `False`</code> <p>When this is enabled, masked tokens are treated as if they had been dropped out by input dropout.</p> <code>True</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import RnaFmModel, RnaFmConfig\n</code></pre> Python Console Session<pre><code>&gt;&gt;&gt; # Initializing a RNA-FM multimolecule/rnafm style configuration\n&gt;&gt;&gt; configuration = RnaFmConfig()\n</code></pre> Python Console Session<pre><code>&gt;&gt;&gt; # Initializing a model (with random weights) from the multimolecule/rnafm style configuration\n&gt;&gt;&gt; model = RnaFmModel(configuration)\n</code></pre> Python Console Session<pre><code>&gt;&gt;&gt; # Accessing the model configuration\n&gt;&gt;&gt; configuration = model.config\n</code></pre> Source code in <code>multimolecule/models/rnafm/configuration_rnafm.py</code> Python<pre><code>class RnaFmConfig(PreTrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`RnaFmModel`]. It is used to instantiate a RNA-FM\n    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n    defaults will yield a similar configuration to that of the RNA-FM\n    [ml4bio/RNA-FM](https://github.com/ml4bio/RNA-FM) architecture.\n\n    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n    documentation from [`PreTrainedConfig`] for more information.\n\n\n    Args:\n        vocab_size (`int`, *optional*, defaults to 25 if `codon=False` else 131):\n            Vocabulary size of the RNA-FM model. Defines the number of different tokens that can be represented by the\n            `inputs_ids` passed when calling [`RnaFmModel`].\n        codon (`bool`, *optional*, defaults to `False`):\n            Whether to use codon tokenization.\n        hidden_size (`int`, *optional*, defaults to 640):\n            Dimensionality of the encoder layers and the pooler layer.\n        num_hidden_layers (`int`, *optional*, defaults to 12):\n            Number of hidden layers in the Transformer encoder.\n        num_attention_heads (`int`, *optional*, defaults to 20):\n            Number of attention heads for each attention layer in the Transformer encoder.\n        intermediate_size (`int`, *optional*, defaults to 5120):\n            Dimensionality of the \"intermediate\" (often named feed-forward) layer in the Transformer encoder.\n        hidden_dropout (`float`, *optional*, defaults to 0.1):\n            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n        attention_dropout (`float`, *optional*, defaults to 0.1):\n            The dropout ratio for the attention probabilities.\n        max_position_embeddings (`int`, *optional*, defaults to 1026):\n            The maximum sequence length that this model might ever be used with. Typically set this to something large\n            just in case (e.g., 512 or 1024 or 2048).\n        initializer_range (`float`, *optional*, defaults to 0.02):\n            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n        layer_norm_eps (`float`, *optional*, defaults to 1e-12):\n            The epsilon used by the layer normalization layers.\n        position_embedding_type (`str`, *optional*, defaults to `\"absolute\"`):\n            Type of position embedding. Choose one of `\"absolute\"`, `\"relative_key\"`, `\"relative_key_query\", \"rotary\"`.\n            For positional embeddings use `\"absolute\"`. For more information on `\"relative_key\"`, please refer to\n            [Self-Attention with Relative Position Representations (Shaw et al.)](https://arxiv.org/abs/1803.02155).\n            For more information on `\"relative_key_query\"`, please refer to *Method 4* in [Improve Transformer Models\n            with Better Relative Position Embeddings (Huang et al.)](https://arxiv.org/abs/2009.13658).\n        is_decoder (`bool`, *optional*, defaults to `False`):\n            Whether the model is used as a decoder or not. If `False`, the model is used as an encoder.\n        use_cache (`bool`, *optional*, defaults to `True`):\n            Whether or not the model should return the last key/values attentions (not used by all models). Only\n            relevant if `config.is_decoder=True`.\n        emb_layer_norm_before (`bool`, *optional*):\n            Whether to apply layer normalization after embeddings but before the main stem of the network.\n        token_dropout (`bool`, defaults to `False`):\n            When this is enabled, masked tokens are treated as if they had been dropped out by input dropout.\n\n    Examples:\n        &gt;&gt;&gt; from multimolecule import RnaFmModel, RnaFmConfig\n\n        &gt;&gt;&gt; # Initializing a RNA-FM multimolecule/rnafm style configuration\n        &gt;&gt;&gt; configuration = RnaFmConfig()\n\n        &gt;&gt;&gt; # Initializing a model (with random weights) from the multimolecule/rnafm style configuration\n        &gt;&gt;&gt; model = RnaFmModel(configuration)\n\n        &gt;&gt;&gt; # Accessing the model configuration\n        &gt;&gt;&gt; configuration = model.config\n    \"\"\"\n\n    model_type = \"rnafm\"\n\n    def __init__(\n        self,\n        vocab_size=None,\n        codon=False,\n        hidden_size=640,\n        num_hidden_layers=12,\n        num_attention_heads=20,\n        intermediate_size=5120,\n        hidden_act=\"gelu\",\n        hidden_dropout=0.1,\n        attention_dropout=0.1,\n        max_position_embeddings=1026,\n        initializer_range=0.02,\n        layer_norm_eps=1e-12,\n        position_embedding_type=\"absolute\",\n        use_cache=True,\n        emb_layer_norm_before=True,\n        token_dropout=True,\n        head=None,\n        lm_head=None,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        if vocab_size is None:\n            vocab_size = 131 if codon else 25\n        self.vocab_size = vocab_size\n        self.codon = codon\n        self.hidden_size = hidden_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.intermediate_size = intermediate_size\n        self.hidden_act = hidden_act\n        self.hidden_dropout = hidden_dropout\n        self.attention_dropout = attention_dropout\n        self.max_position_embeddings = max_position_embeddings\n        self.initializer_range = initializer_range\n        self.layer_norm_eps = layer_norm_eps\n        self.position_embedding_type = position_embedding_type\n        self.use_cache = use_cache\n        self.emb_layer_norm_before = emb_layer_norm_before\n        self.token_dropout = token_dropout\n        self.head = HeadConfig(**head if head is not None else {})\n        self.lm_head = MaskedLMHeadConfig(**lm_head if lm_head is not None else {})\n</code></pre>"},{"location":"models/rnafm/#multimolecule.models.rnafm.RnaFmForMaskedLM","title":"<code>RnaFmForMaskedLM</code>","text":"<p>               Bases: <code>RnaFmPreTrainedModel</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import RnaFmConfig, RnaFmForMaskedLM, RnaTokenizer\n&gt;&gt;&gt; config = RnaFmConfig()\n&gt;&gt;&gt; model = RnaFmForMaskedLM(config)\n&gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n&gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n&gt;&gt;&gt; output = model(**input)\n</code></pre> Source code in <code>multimolecule/models/rnafm/modeling_rnafm.py</code> Python<pre><code>class RnaFmForMaskedLM(RnaFmPreTrainedModel):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; from multimolecule import RnaFmConfig, RnaFmForMaskedLM, RnaTokenizer\n        &gt;&gt;&gt; config = RnaFmConfig()\n        &gt;&gt;&gt; model = RnaFmForMaskedLM(config)\n        &gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n        &gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n        &gt;&gt;&gt; output = model(**input)\n    \"\"\"\n\n    _tied_weights_keys = [\"lm_head.decoder.weight\"]\n\n    def __init__(self, config: RnaFmConfig):\n        super().__init__(config)\n        if config.is_decoder:\n            logger.warning(\n                \"If you want to use `RnaFmForMaskedLM` make sure `config.is_decoder=False` for \"\n                \"bi-directional self-attention.\"\n            )\n        self.rnafm = RnaFmModel(config, add_pooling_layer=False)\n        self.lm_head = MaskedLMHead(config)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(\n        self,\n        input_ids: Tensor | NestedTensor,\n        attention_mask: Tensor | None = None,\n        position_ids: Tensor | None = None,\n        head_mask: Tensor | None = None,\n        inputs_embeds: Tensor | NestedTensor | None = None,\n        encoder_hidden_states: Tensor | None = None,\n        encoder_attention_mask: Tensor | None = None,\n        labels: Tensor | None = None,\n        output_attentions: bool | None = None,\n        output_hidden_states: bool | None = None,\n        return_dict: bool | None = None,\n    ) -&gt; Tuple[Tensor, ...] | MaskedLMOutput:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n            config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n            loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n        \"\"\"\n\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.rnafm(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        output = self.lm_head(outputs, labels)\n        logits, loss = output.logits, output.loss\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return MaskedLMOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"models/rnafm/#multimolecule.models.rnafm.RnaFmForMaskedLM.forward","title":"<code>forward(input_ids, attention_mask=None, position_ids=None, head_mask=None, inputs_embeds=None, encoder_hidden_states=None, encoder_attention_mask=None, labels=None, output_attentions=None, output_hidden_states=None, return_dict=None)</code>","text":"<p>labels (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, optional):     Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ...,     config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the     loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code></p> Source code in <code>multimolecule/models/rnafm/modeling_rnafm.py</code> Python<pre><code>def forward(\n    self,\n    input_ids: Tensor | NestedTensor,\n    attention_mask: Tensor | None = None,\n    position_ids: Tensor | None = None,\n    head_mask: Tensor | None = None,\n    inputs_embeds: Tensor | NestedTensor | None = None,\n    encoder_hidden_states: Tensor | None = None,\n    encoder_attention_mask: Tensor | None = None,\n    labels: Tensor | None = None,\n    output_attentions: bool | None = None,\n    output_hidden_states: bool | None = None,\n    return_dict: bool | None = None,\n) -&gt; Tuple[Tensor, ...] | MaskedLMOutput:\n    r\"\"\"\n    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n        Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n        config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n        loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n    \"\"\"\n\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n    outputs = self.rnafm(\n        input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        head_mask=head_mask,\n        inputs_embeds=inputs_embeds,\n        encoder_hidden_states=encoder_hidden_states,\n        encoder_attention_mask=encoder_attention_mask,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n    output = self.lm_head(outputs, labels)\n    logits, loss = output.logits, output.loss\n\n    if not return_dict:\n        output = (logits,) + outputs[2:]\n        return ((loss,) + output) if loss is not None else output\n\n    return MaskedLMOutput(\n        loss=loss,\n        logits=logits,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"models/rnafm/#multimolecule.models.rnafm.RnaFmForNucleotideClassification","title":"<code>RnaFmForNucleotideClassification</code>","text":"<p>               Bases: <code>RnaFmPreTrainedModel</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import RnaFmConfig, RnaFmForNucleotideClassification, RnaTokenizer\n&gt;&gt;&gt; config = RnaFmConfig()\n&gt;&gt;&gt; model = RnaFmForNucleotideClassification(config)\n&gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n&gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n&gt;&gt;&gt; output = model(**input)\n</code></pre> Source code in <code>multimolecule/models/rnafm/modeling_rnafm.py</code> Python<pre><code>class RnaFmForNucleotideClassification(RnaFmPreTrainedModel):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; from multimolecule import RnaFmConfig, RnaFmForNucleotideClassification, RnaTokenizer\n        &gt;&gt;&gt; config = RnaFmConfig()\n        &gt;&gt;&gt; model = RnaFmForNucleotideClassification(config)\n        &gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n        &gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n        &gt;&gt;&gt; output = model(**input)\n    \"\"\"\n\n    def __init__(self, config: RnaFmConfig):\n        super().__init__(config)\n        self.num_labels = config.head.num_labels\n        self.rnafm = RnaFmModel(config, add_pooling_layer=False)\n        self.nucleotide_head = NucleotideClassificationHead(config)\n        self.head_config = self.nucleotide_head.config\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(\n        self,\n        input_ids: Tensor | NestedTensor,\n        attention_mask: Tensor | None = None,\n        position_ids: Tensor | None = None,\n        head_mask: Tensor | None = None,\n        inputs_embeds: Tensor | NestedTensor | None = None,\n        labels: Tensor | None = None,\n        output_attentions: bool | None = None,\n        output_hidden_states: bool | None = None,\n        return_dict: bool | None = None,\n        **kwargs,\n    ) -&gt; Tuple[Tensor, ...] | TokenClassifierOutput:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n            `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        if kwargs:\n            warn(\n                f\"Additional keyword arguments `{', '.join(kwargs)}` are detected in \"\n                f\"`{self.__class__.__name__}.forward`, they will be ignored.\\n\"\n                \"This is provided for backward compatibility and may lead to unexpected behavior.\"\n            )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.rnafm(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        output = self.nucleotide_head(outputs, attention_mask, input_ids, labels)\n        logits, loss = output.logits, output.loss\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return TokenClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"models/rnafm/#multimolecule.models.rnafm.RnaFmForNucleotideClassification.forward","title":"<code>forward(input_ids, attention_mask=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None, output_attentions=None, output_hidden_states=None, return_dict=None, **kwargs)</code>","text":"<p>labels (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, optional):     Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ...,     config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If     <code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).</p> Source code in <code>multimolecule/models/rnafm/modeling_rnafm.py</code> Python<pre><code>def forward(\n    self,\n    input_ids: Tensor | NestedTensor,\n    attention_mask: Tensor | None = None,\n    position_ids: Tensor | None = None,\n    head_mask: Tensor | None = None,\n    inputs_embeds: Tensor | NestedTensor | None = None,\n    labels: Tensor | None = None,\n    output_attentions: bool | None = None,\n    output_hidden_states: bool | None = None,\n    return_dict: bool | None = None,\n    **kwargs,\n) -&gt; Tuple[Tensor, ...] | TokenClassifierOutput:\n    r\"\"\"\n    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n        `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).\n    \"\"\"\n    if kwargs:\n        warn(\n            f\"Additional keyword arguments `{', '.join(kwargs)}` are detected in \"\n            f\"`{self.__class__.__name__}.forward`, they will be ignored.\\n\"\n            \"This is provided for backward compatibility and may lead to unexpected behavior.\"\n        )\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n    outputs = self.rnafm(\n        input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        head_mask=head_mask,\n        inputs_embeds=inputs_embeds,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n    output = self.nucleotide_head(outputs, attention_mask, input_ids, labels)\n    logits, loss = output.logits, output.loss\n\n    if not return_dict:\n        output = (logits,) + outputs[2:]\n        return ((loss,) + output) if loss is not None else output\n\n    return TokenClassifierOutput(\n        loss=loss,\n        logits=logits,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"models/rnafm/#multimolecule.models.rnafm.RnaFmForPretraining","title":"<code>RnaFmForPretraining</code>","text":"<p>               Bases: <code>RnaFmPreTrainedModel</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import RnaFmConfig, RnaFmForPretraining, RnaTokenizer\n&gt;&gt;&gt; config = RnaFmConfig()\n&gt;&gt;&gt; model = RnaFmForPretraining(config)\n&gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n&gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n&gt;&gt;&gt; output = model(**input)\n</code></pre> Source code in <code>multimolecule/models/rnafm/modeling_rnafm.py</code> Python<pre><code>class RnaFmForPretraining(RnaFmPreTrainedModel):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; from multimolecule import RnaFmConfig, RnaFmForPretraining, RnaTokenizer\n        &gt;&gt;&gt; config = RnaFmConfig()\n        &gt;&gt;&gt; model = RnaFmForPretraining(config)\n        &gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n        &gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n        &gt;&gt;&gt; output = model(**input)\n    \"\"\"\n\n    _tied_weights_keys = [\"head.predictions.decoder.weight\"]\n\n    def __init__(self, config: RnaFmConfig):\n        super().__init__(config)\n        if config.is_decoder:\n            logger.warning(\n                \"If you want to use `RnaFmForPretraining` make sure `config.is_decoder=False` for \"\n                \"bi-directional self-attention.\"\n            )\n        self.rnafm = RnaFmModel(config, add_pooling_layer=False)\n        self.pretrain_head = RnaFmPreTrainingHeads(config)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_output_embeddings(self):\n        return self.pretrain_head.predictions.decoder\n\n    def set_output_embeddings(self, embeddings):\n        self.pretrain_head.predictions.decoder = embeddings\n\n    def forward(\n        self,\n        input_ids: Tensor | NestedTensor,\n        attention_mask: Tensor | None = None,\n        position_ids: Tensor | None = None,\n        head_mask: Tensor | None = None,\n        inputs_embeds: Tensor | NestedTensor | None = None,\n        encoder_hidden_states: Tensor | None = None,\n        encoder_attention_mask: Tensor | None = None,\n        labels: Tensor | None = None,\n        labels_contact: Tensor | None = None,\n        output_attentions: bool | None = None,\n        output_hidden_states: bool | None = None,\n        return_dict: bool | None = None,\n    ) -&gt; Tuple[Tensor, ...] | RnaFmForPretrainingOutput:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n            config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n            loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n        \"\"\"\n\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.rnafm(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n            output_attentions=True,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        logits, contact_map = self.pretrain_head(outputs, attention_mask, input_ids)\n\n        loss = None\n        if any(x is not None for x in [labels, labels_contact]):\n            loss_mlm = loss_contact = 0\n            if labels is not None:\n                loss_mlm = F.cross_entropy(logits.view(-1, self.config.vocab_size), labels.view(-1))\n            if labels_contact is not None:\n                loss_contact = F.mse_loss(contact_map.view(-1), labels_contact.view(-1))\n            loss = loss_mlm + loss_contact\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return RnaFmForPretrainingOutput(\n            loss=loss,\n            logits=logits,\n            contact_map=contact_map,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"models/rnafm/#multimolecule.models.rnafm.RnaFmForPretraining.forward","title":"<code>forward(input_ids, attention_mask=None, position_ids=None, head_mask=None, inputs_embeds=None, encoder_hidden_states=None, encoder_attention_mask=None, labels=None, labels_contact=None, output_attentions=None, output_hidden_states=None, return_dict=None)</code>","text":"<p>labels (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, optional):     Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ...,     config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the     loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code></p> Source code in <code>multimolecule/models/rnafm/modeling_rnafm.py</code> Python<pre><code>def forward(\n    self,\n    input_ids: Tensor | NestedTensor,\n    attention_mask: Tensor | None = None,\n    position_ids: Tensor | None = None,\n    head_mask: Tensor | None = None,\n    inputs_embeds: Tensor | NestedTensor | None = None,\n    encoder_hidden_states: Tensor | None = None,\n    encoder_attention_mask: Tensor | None = None,\n    labels: Tensor | None = None,\n    labels_contact: Tensor | None = None,\n    output_attentions: bool | None = None,\n    output_hidden_states: bool | None = None,\n    return_dict: bool | None = None,\n) -&gt; Tuple[Tensor, ...] | RnaFmForPretrainingOutput:\n    r\"\"\"\n    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n        Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n        config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n        loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n    \"\"\"\n\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n    outputs = self.rnafm(\n        input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        head_mask=head_mask,\n        inputs_embeds=inputs_embeds,\n        encoder_hidden_states=encoder_hidden_states,\n        encoder_attention_mask=encoder_attention_mask,\n        output_attentions=True,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n    logits, contact_map = self.pretrain_head(outputs, attention_mask, input_ids)\n\n    loss = None\n    if any(x is not None for x in [labels, labels_contact]):\n        loss_mlm = loss_contact = 0\n        if labels is not None:\n            loss_mlm = F.cross_entropy(logits.view(-1, self.config.vocab_size), labels.view(-1))\n        if labels_contact is not None:\n            loss_contact = F.mse_loss(contact_map.view(-1), labels_contact.view(-1))\n        loss = loss_mlm + loss_contact\n\n    if not return_dict:\n        output = (logits,) + outputs[2:]\n        return ((loss,) + output) if loss is not None else output\n\n    return RnaFmForPretrainingOutput(\n        loss=loss,\n        logits=logits,\n        contact_map=contact_map,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"models/rnafm/#multimolecule.models.rnafm.RnaFmForSequenceClassification","title":"<code>RnaFmForSequenceClassification</code>","text":"<p>               Bases: <code>RnaFmPreTrainedModel</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import RnaFmConfig, RnaFmForSequenceClassification, RnaTokenizer\n&gt;&gt;&gt; config = RnaFmConfig()\n&gt;&gt;&gt; model = RnaFmForSequenceClassification(config)\n&gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n&gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n&gt;&gt;&gt; output = model(**input)\n</code></pre> Source code in <code>multimolecule/models/rnafm/modeling_rnafm.py</code> Python<pre><code>class RnaFmForSequenceClassification(RnaFmPreTrainedModel):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; from multimolecule import RnaFmConfig, RnaFmForSequenceClassification, RnaTokenizer\n        &gt;&gt;&gt; config = RnaFmConfig()\n        &gt;&gt;&gt; model = RnaFmForSequenceClassification(config)\n        &gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n        &gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n        &gt;&gt;&gt; output = model(**input)\n    \"\"\"\n\n    def __init__(self, config: RnaFmConfig):\n        super().__init__(config)\n        self.num_labels = config.head.num_labels\n        self.rnafm = RnaFmModel(config, add_pooling_layer=True)\n        self.sequence_head = SequenceClassificationHead(config)\n        self.head_config = self.sequence_head.config\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(\n        self,\n        input_ids: Tensor | NestedTensor,\n        attention_mask: Tensor | None = None,\n        position_ids: Tensor | None = None,\n        head_mask: Tensor | None = None,\n        inputs_embeds: Tensor | NestedTensor | None = None,\n        labels: Tensor | None = None,\n        output_attentions: bool | None = None,\n        output_hidden_states: bool | None = None,\n        return_dict: bool | None = None,\n    ) -&gt; Tuple[Tensor, ...] | SequenceClassifierOutput:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n            `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.rnafm(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        output = self.sequence_head(outputs, labels)\n        logits, loss = output.logits, output.loss\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"models/rnafm/#multimolecule.models.rnafm.RnaFmForSequenceClassification.forward","title":"<code>forward(input_ids, attention_mask=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None, output_attentions=None, output_hidden_states=None, return_dict=None)</code>","text":"<p>labels (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, optional):     Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ...,     config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If     <code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).</p> Source code in <code>multimolecule/models/rnafm/modeling_rnafm.py</code> Python<pre><code>def forward(\n    self,\n    input_ids: Tensor | NestedTensor,\n    attention_mask: Tensor | None = None,\n    position_ids: Tensor | None = None,\n    head_mask: Tensor | None = None,\n    inputs_embeds: Tensor | NestedTensor | None = None,\n    labels: Tensor | None = None,\n    output_attentions: bool | None = None,\n    output_hidden_states: bool | None = None,\n    return_dict: bool | None = None,\n) -&gt; Tuple[Tensor, ...] | SequenceClassifierOutput:\n    r\"\"\"\n    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n        `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).\n    \"\"\"\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n    outputs = self.rnafm(\n        input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        head_mask=head_mask,\n        inputs_embeds=inputs_embeds,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n    output = self.sequence_head(outputs, labels)\n    logits, loss = output.logits, output.loss\n\n    if not return_dict:\n        output = (logits,) + outputs[2:]\n        return ((loss,) + output) if loss is not None else output\n\n    return SequenceClassifierOutput(\n        loss=loss,\n        logits=logits,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"models/rnafm/#multimolecule.models.rnafm.RnaFmForTokenClassification","title":"<code>RnaFmForTokenClassification</code>","text":"<p>               Bases: <code>RnaFmPreTrainedModel</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import RnaFmConfig, RnaFmForTokenClassification, RnaTokenizer\n&gt;&gt;&gt; config = RnaFmConfig()\n&gt;&gt;&gt; model = RnaFmForTokenClassification(config)\n&gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n&gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n&gt;&gt;&gt; output = model(**input)\n</code></pre> Source code in <code>multimolecule/models/rnafm/modeling_rnafm.py</code> Python<pre><code>class RnaFmForTokenClassification(RnaFmPreTrainedModel):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; from multimolecule import RnaFmConfig, RnaFmForTokenClassification, RnaTokenizer\n        &gt;&gt;&gt; config = RnaFmConfig()\n        &gt;&gt;&gt; model = RnaFmForTokenClassification(config)\n        &gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n        &gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n        &gt;&gt;&gt; output = model(**input)\n    \"\"\"\n\n    def __init__(self, config: RnaFmConfig):\n        super().__init__(config)\n        self.num_labels = config.head.num_labels\n        self.rnafm = RnaFmModel(config, add_pooling_layer=False)\n        self.token_head = TokenClassificationHead(config)\n        self.head_config = self.token_head.config\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(\n        self,\n        input_ids: Tensor | NestedTensor,\n        attention_mask: Tensor | None = None,\n        position_ids: Tensor | None = None,\n        head_mask: Tensor | None = None,\n        inputs_embeds: Tensor | NestedTensor | None = None,\n        labels: Tensor | None = None,\n        output_attentions: bool | None = None,\n        output_hidden_states: bool | None = None,\n        return_dict: bool | None = None,\n    ) -&gt; Tuple[Tensor, ...] | TokenClassifierOutput:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.rnafm(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        output = self.token_head(outputs, attention_mask, input_ids, labels)\n        logits, loss = output.logits, output.loss\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return TokenClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"models/rnafm/#multimolecule.models.rnafm.RnaFmForTokenClassification.forward","title":"<code>forward(input_ids, attention_mask=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None, output_attentions=None, output_hidden_states=None, return_dict=None)</code>","text":"<p>labels (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, optional):     Labels for computing the token classification loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>.</p> Source code in <code>multimolecule/models/rnafm/modeling_rnafm.py</code> Python<pre><code>def forward(\n    self,\n    input_ids: Tensor | NestedTensor,\n    attention_mask: Tensor | None = None,\n    position_ids: Tensor | None = None,\n    head_mask: Tensor | None = None,\n    inputs_embeds: Tensor | NestedTensor | None = None,\n    labels: Tensor | None = None,\n    output_attentions: bool | None = None,\n    output_hidden_states: bool | None = None,\n    return_dict: bool | None = None,\n) -&gt; Tuple[Tensor, ...] | TokenClassifierOutput:\n    r\"\"\"\n    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n        Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n    \"\"\"\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n    outputs = self.rnafm(\n        input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        head_mask=head_mask,\n        inputs_embeds=inputs_embeds,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n    output = self.token_head(outputs, attention_mask, input_ids, labels)\n    logits, loss = output.logits, output.loss\n\n    if not return_dict:\n        output = (logits,) + outputs[2:]\n        return ((loss,) + output) if loss is not None else output\n\n    return TokenClassifierOutput(\n        loss=loss,\n        logits=logits,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"models/rnafm/#multimolecule.models.rnafm.RnaFmModel","title":"<code>RnaFmModel</code>","text":"<p>               Bases: <code>RnaFmPreTrainedModel</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import RnaFmConfig, RnaFmModel, RnaTokenizer\n&gt;&gt;&gt; config = RnaFmConfig()\n&gt;&gt;&gt; model = RnaFmModel(config)\n&gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n&gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n&gt;&gt;&gt; output = model(**input)\n</code></pre> Source code in <code>multimolecule/models/rnafm/modeling_rnafm.py</code> Python<pre><code>class RnaFmModel(RnaFmPreTrainedModel):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; from multimolecule import RnaFmConfig, RnaFmModel, RnaTokenizer\n        &gt;&gt;&gt; config = RnaFmConfig()\n        &gt;&gt;&gt; model = RnaFmModel(config)\n        &gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n        &gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n        &gt;&gt;&gt; output = model(**input)\n    \"\"\"\n\n    def __init__(self, config: RnaFmConfig, add_pooling_layer: bool = True):\n        super().__init__(config)\n        self.pad_token_id = config.pad_token_id\n        self.embeddings = RnaFmEmbeddings(config)\n        self.encoder = RnaFmEncoder(config)\n        self.pooler = RnaFmPooler(config) if add_pooling_layer else None\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.embeddings.word_embeddings\n\n    def set_input_embeddings(self, value):\n        self.embeddings.word_embeddings = value\n\n    def _prune_heads(self, heads_to_prune):\n        \"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"\n        for layer, heads in heads_to_prune.items():\n            self.encoder.layer[layer].attention.prune_heads(heads)\n\n    def forward(\n        self,\n        input_ids: Tensor | NestedTensor,\n        attention_mask: Tensor | None = None,\n        position_ids: Tensor | None = None,\n        head_mask: Tensor | None = None,\n        inputs_embeds: Tensor | NestedTensor | None = None,\n        encoder_hidden_states: Tensor | None = None,\n        encoder_attention_mask: Tensor | None = None,\n        past_key_values: Tuple[Tuple[torch.FloatTensor, torch.FloatTensor], ...] | None = None,\n        use_cache: bool | None = None,\n        output_attentions: bool | None = None,\n        output_hidden_states: bool | None = None,\n        return_dict: bool | None = None,\n    ) -&gt; Tuple[Tensor, ...] | BaseModelOutputWithPoolingAndCrossAttentions:\n        r\"\"\"\n        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n            the model is configured as a decoder.\n        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors\n            of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n\n            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n            `past_key_values`).\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if self.config.is_decoder:\n            use_cache = use_cache if use_cache is not None else self.config.use_cache\n        else:\n            use_cache = False\n\n        if isinstance(input_ids, NestedTensor):\n            input_ids, attention_mask = input_ids.tensor, input_ids.mask\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n        if input_ids is not None:\n            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n            input_shape = input_ids.size()\n        elif inputs_embeds is not None:\n            input_shape = inputs_embeds.size()[:-1]\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n        batch_size, seq_length = input_shape\n        device = input_ids.device if input_ids is not None else inputs_embeds.device  # type: ignore[union-attr]\n\n        # past_key_values_length\n        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n\n        if attention_mask is None:\n            attention_mask = (\n                input_ids.ne(self.pad_token_id)\n                if self.pad_token_id is not None\n                else torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n            )\n\n        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n        # ourselves in which case we just need to make it broadcastable to all heads.\n        extended_attention_mask: Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n\n        # If a 2D or 3D attention mask is provided for the cross-attention\n        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n        if self.config.is_decoder and encoder_hidden_states is not None:\n            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n            if encoder_attention_mask is None:\n                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n        else:\n            encoder_extended_attention_mask = None\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n\n        embedding_output = self.embeddings(\n            input_ids=input_ids,\n            position_ids=position_ids,\n            attention_mask=attention_mask,\n            inputs_embeds=inputs_embeds,\n            past_key_values_length=past_key_values_length,\n        )\n        encoder_outputs = self.encoder(\n            embedding_output,\n            attention_mask=extended_attention_mask,\n            head_mask=head_mask,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_extended_attention_mask,\n            past_key_values=past_key_values,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        sequence_output = encoder_outputs[0]\n        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n\n        if not return_dict:\n            return (sequence_output, pooled_output) + encoder_outputs[1:]\n\n        return BaseModelOutputWithPoolingAndCrossAttentions(\n            last_hidden_state=sequence_output,\n            pooler_output=pooled_output,\n            past_key_values=encoder_outputs.past_key_values,\n            hidden_states=encoder_outputs.hidden_states,\n            attentions=encoder_outputs.attentions,\n            cross_attentions=encoder_outputs.cross_attentions,\n        )\n</code></pre>"},{"location":"models/rnafm/#multimolecule.models.rnafm.RnaFmModel.forward","title":"<code>forward(input_ids, attention_mask=None, position_ids=None, head_mask=None, inputs_embeds=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_values=None, use_cache=None, output_attentions=None, output_hidden_states=None, return_dict=None)</code>","text":"<p>encoder_hidden_states  (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, optional):     Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if     the model is configured as a decoder. encoder_attention_mask (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, optional):     Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in     the cross-attention if the model is configured as a decoder. Mask values selected in <code>[0, 1]</code>:</p> Text Only<pre><code>- 1 for tokens that are **not masked**,\n- 0 for tokens that are **masked**.\n</code></pre> <p>past_key_values (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors     of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>):     Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.</p> Text Only<pre><code>If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\ndon't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n`decoder_input_ids` of shape `(batch_size, sequence_length)`.\n</code></pre> <p>use_cache (<code>bool</code>, optional):     If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see     <code>past_key_values</code>).</p> Source code in <code>multimolecule/models/rnafm/modeling_rnafm.py</code> Python<pre><code>def forward(\n    self,\n    input_ids: Tensor | NestedTensor,\n    attention_mask: Tensor | None = None,\n    position_ids: Tensor | None = None,\n    head_mask: Tensor | None = None,\n    inputs_embeds: Tensor | NestedTensor | None = None,\n    encoder_hidden_states: Tensor | None = None,\n    encoder_attention_mask: Tensor | None = None,\n    past_key_values: Tuple[Tuple[torch.FloatTensor, torch.FloatTensor], ...] | None = None,\n    use_cache: bool | None = None,\n    output_attentions: bool | None = None,\n    output_hidden_states: bool | None = None,\n    return_dict: bool | None = None,\n) -&gt; Tuple[Tensor, ...] | BaseModelOutputWithPoolingAndCrossAttentions:\n    r\"\"\"\n    encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n        Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n        the model is configured as a decoder.\n    encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n        Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n        the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n\n        - 1 for tokens that are **not masked**,\n        - 0 for tokens that are **masked**.\n    past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors\n        of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n        Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n\n        If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n        don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n        `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n    use_cache (`bool`, *optional*):\n        If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n        `past_key_values`).\n    \"\"\"\n    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n    output_hidden_states = (\n        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n    )\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n    if self.config.is_decoder:\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n    else:\n        use_cache = False\n\n    if isinstance(input_ids, NestedTensor):\n        input_ids, attention_mask = input_ids.tensor, input_ids.mask\n    if input_ids is not None and inputs_embeds is not None:\n        raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n    if input_ids is not None:\n        self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n        input_shape = input_ids.size()\n    elif inputs_embeds is not None:\n        input_shape = inputs_embeds.size()[:-1]\n    else:\n        raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n    batch_size, seq_length = input_shape\n    device = input_ids.device if input_ids is not None else inputs_embeds.device  # type: ignore[union-attr]\n\n    # past_key_values_length\n    past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n\n    if attention_mask is None:\n        attention_mask = (\n            input_ids.ne(self.pad_token_id)\n            if self.pad_token_id is not None\n            else torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n        )\n\n    # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n    # ourselves in which case we just need to make it broadcastable to all heads.\n    extended_attention_mask: Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n\n    # If a 2D or 3D attention mask is provided for the cross-attention\n    # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n    if self.config.is_decoder and encoder_hidden_states is not None:\n        encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n        encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n        if encoder_attention_mask is None:\n            encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n        encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n    else:\n        encoder_extended_attention_mask = None\n\n    # Prepare head mask if needed\n    # 1.0 in head_mask indicate we keep the head\n    # attention_probs has shape bsz x n_heads x N x N\n    # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n    # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n    head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n\n    embedding_output = self.embeddings(\n        input_ids=input_ids,\n        position_ids=position_ids,\n        attention_mask=attention_mask,\n        inputs_embeds=inputs_embeds,\n        past_key_values_length=past_key_values_length,\n    )\n    encoder_outputs = self.encoder(\n        embedding_output,\n        attention_mask=extended_attention_mask,\n        head_mask=head_mask,\n        encoder_hidden_states=encoder_hidden_states,\n        encoder_attention_mask=encoder_extended_attention_mask,\n        past_key_values=past_key_values,\n        use_cache=use_cache,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n    sequence_output = encoder_outputs[0]\n    pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n\n    if not return_dict:\n        return (sequence_output, pooled_output) + encoder_outputs[1:]\n\n    return BaseModelOutputWithPoolingAndCrossAttentions(\n        last_hidden_state=sequence_output,\n        pooler_output=pooled_output,\n        past_key_values=encoder_outputs.past_key_values,\n        hidden_states=encoder_outputs.hidden_states,\n        attentions=encoder_outputs.attentions,\n        cross_attentions=encoder_outputs.cross_attentions,\n    )\n</code></pre>"},{"location":"models/rnafm/#multimolecule.models.rnafm.RnaFmPreTrainedModel","title":"<code>RnaFmPreTrainedModel</code>","text":"<p>               Bases: <code>PreTrainedModel</code></p> <p>An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.</p> Source code in <code>multimolecule/models/rnafm/modeling_rnafm.py</code> Python<pre><code>class RnaFmPreTrainedModel(PreTrainedModel):\n    \"\"\"\n    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n    models.\n    \"\"\"\n\n    config_class = RnaFmConfig\n    base_model_prefix = \"rnafm\"\n    supports_gradient_checkpointing = True\n    _no_split_modules = [\"RnaFmLayer\", \"RnaFmEmbeddings\"]\n\n    # Copied from transformers.models.bert.modeling_bert.BertPreTrainedModel._init_weights\n    def _init_weights(self, module: nn.Module):\n        \"\"\"Initialize the weights\"\"\"\n        if isinstance(module, nn.Linear):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n</code></pre>"},{"location":"models/rnamsm/","title":"RNA-MSM","text":"<p>Pre-trained model on non-coding RNA (ncRNA) with multi (homologous) sequence alignment using a masked language modeling (MLM) objective.</p>"},{"location":"models/rnamsm/#disclaimer","title":"Disclaimer","text":"<p>This is an UNOFFICIAL implementation of the Multiple sequence alignment-based RNA language model and its application to structural inference by Yikun Zhang, Mei Lang, Jiuhong Jiang, Zhiqiang Gao, et al.</p> <p>The OFFICIAL repository of RNA-MSM is at yikunpku/RNA-MSM.</p> <p>The team releasing RNA-MSM did not write this model card for this model so this model card has been written by the MultiMolecule team.</p>"},{"location":"models/rnamsm/#model-details","title":"Model Details","text":"<p>RNA-MSM is a bert-style model pre-trained on a large corpus of non-coding RNA sequences in a self-supervised fashion. This means that the model was trained on the raw nucleotides of RNA sequences only, with an automatic process to generate inputs and labels from those texts. Please refer to the Training Details section for more information on the training process.</p>"},{"location":"models/rnamsm/#model-specification","title":"Model Specification","text":"Num Layers Hidden Size Num Heads Intermediate Size Num Parameters (M) FLOPs (G) MACs (G) Max Num Tokens 10 768 12 3072 95.92 21.66 10.57 1024"},{"location":"models/rnamsm/#links","title":"Links","text":"<ul> <li>Code: multimolecule.rnamsm</li> <li>Weights: multimolecule/rnamsm</li> <li>Data: Rfam</li> <li>Paper: Multiple sequence alignment-based RNA language model and its application to structural inference</li> <li>Developed by: Yikun Zhang, Mei Lang, Jiuhong Jiang, Zhiqiang Gao, Fan Xu, Thomas Litfin, Ke Chen, Jaswinder Singh, Xiansong Huang, Guoli Song, Yonghong Tian, Jian Zhan, Jie Chen, Yaoqi Zhou</li> <li>Model type: BERT - MSA</li> <li>Original Repository: https://github.com/yikunpku/RNA-MSM</li> </ul>"},{"location":"models/rnamsm/#usage","title":"Usage","text":"<p>The model file depends on the <code>multimolecule</code> library. You can install it using pip:</p> Bash<pre><code>pip install multimolecule\n</code></pre>"},{"location":"models/rnamsm/#direct-use","title":"Direct Use","text":"<p>You can use this model directly with a pipeline for masked language modeling:</p> Python<pre><code>&gt;&gt;&gt; import multimolecule  # you must import multimolecule to register models\n&gt;&gt;&gt; from transformers import pipeline\n&gt;&gt;&gt; unmasker = pipeline('fill-mask', model='multimolecule/rnamsm')\n&gt;&gt;&gt; unmasker(\"uagc&lt;mask&gt;uaucagacugauguuga\")\n\n[{'score': 0.28509262204170227,\n  'token': 9,\n  'token_str': 'U',\n  'sequence': 'U A G C U U A U C A G A C U G A U G U U G A'},\n {'score': 0.2530057430267334,\n  'token': 21,\n  'token_str': 'K',\n  'sequence': 'U A G C K U A U C A G A C U G A U G U U G A'},\n {'score': 0.22453057765960693,\n  'token': 8,\n  'token_str': 'G',\n  'sequence': 'U A G C G U A U C A G A C U G A U G U U G A'},\n {'score': 0.0683528482913971,\n  'token': 14,\n  'token_str': 'D',\n  'sequence': 'U A G C D U A U C A G A C U G A U G U U G A'},\n {'score': 0.037713583558797836,\n  'token': 18,\n  'token_str': 'W',\n  'sequence': 'U A G C W U A U C A G A C U G A U G U U G A'}]\n</code></pre>"},{"location":"models/rnamsm/#downstream-use","title":"Downstream Use","text":""},{"location":"models/rnamsm/#extract-features","title":"Extract Features","text":"<p>Here is how to use this model to get the features of a given sequence in PyTorch:</p> Python<pre><code>from multimolecule import RnaTokenizer, RnaMsmModel\n\n\ntokenizer = RnaTokenizer.from_pretrained('multimolecule/rnamsm')\nmodel = RnaMsmModel.from_pretrained('multimolecule/rnamsm')\n\ntext = \"UAGCUUAUCAGACUGAUGUUGA\"\ninput = tokenizer(text, return_tensors='pt')\n\noutput = model(**input)\n</code></pre>"},{"location":"models/rnamsm/#sequence-classification-regression","title":"Sequence Classification / Regression","text":"<p>Note: This model is not fine-tuned for any specific task. You will need to fine-tune the model on a downstream task to use it for sequence classification or regression.</p> <p>Here is how to use this model as backbone to fine-tune for a sequence-level task in PyTorch:</p> Python<pre><code>import torch\nfrom multimolecule import RnaTokenizer, RnaMsmForSequenceClassification\n\n\ntokenizer = RnaTokenizer.from_pretrained('multimolecule/rnamsm')\nmodel = RnaMsmForSequenceClassification.from_pretrained('multimolecule/rnamsm')\n\ntext = \"UAGCUUAUCAGACUGAUGUUGA\"\ninput = tokenizer(text, return_tensors='pt')\nlabel = torch.tensor([1])\n\noutput = model(**input, labels=label)\n</code></pre>"},{"location":"models/rnamsm/#nucleotide-classification-regression","title":"Nucleotide Classification / Regression","text":"<p>Note: This model is not fine-tuned for any specific task. You will need to fine-tune the model on a downstream task to use it for nucleotide classification or regression.</p> <p>Here is how to use this model as backbone to fine-tune for a nucleotide-level task in PyTorch:</p> Python<pre><code>import torch\nfrom multimolecule import RnaTokenizer, RnaMsmForNucleotideClassification\n\n\ntokenizer = RnaTokenizer.from_pretrained('multimolecule/rnamsm')\nmodel = RnaMsmForNucleotideClassification.from_pretrained('multimolecule/rnamsm')\n\ntext = \"UAGCUUAUCAGACUGAUGUUGA\"\ninput = tokenizer(text, return_tensors='pt')\nlabel = torch.randint(2, (len(text), ))\n\noutput = model(**input, labels=label)\n</code></pre>"},{"location":"models/rnamsm/#training-details","title":"Training Details","text":"<p>RNA-MSM used Masked Language Modeling (MLM) as the pre-training objective: taking a sequence, the model randomly masks 15% of the tokens in the input then run the entire masked sentence through the model and has to predict the masked tokens. This is comparable to the Cloze task in language modeling.</p>"},{"location":"models/rnamsm/#training-data","title":"Training Data","text":"<p>The RNA-MSM model was pre-trained on Rfam. Rfam database is a collection of RNA families, each represented by multiple sequence alignments, consensus secondary structures and covariance models. RNA-MSM used Rfam 14.7 which contains 4,069 RNA families.</p> <p>To avoid potential overfitting in structural inference, RNA-MSM excluded families with experimentally determined structures, such as ribosomal RNAs, transfer RNAs, and small nuclear RNAs. The final dataset contains 3,932 RNA families. The median value for the number of MSA sequences for these families by RNAcmap3 is 2,184.</p> <p>To increase the number of homologous sequences, RNA-MSM used an automatic pipeline, RNAcmap3, for homolog search and sequence alignment. RNAcmap3 is a pipeline that combines the BLAST-N, INFERNAL, Easel, RNAfold and evolutionary coupling tools to generate homologous sequences.</p> <p>RNA-MSM preprocessed all tokens by replacing \u201cT\u201ds with \u201cU\u201ds and substituting \u201cR\u201d, \u201cY\u201d, \u201cK\u201d, \u201cM\u201d, \u201cS\u201d, \u201cW\u201d, \u201cB\u201d, \u201cD\u201d, \u201cH\u201d, \u201cV\u201d, \u201cN\u201d with \u201cX\u201d.</p> <p>Note that <code>RnaTokenizer</code> will convert \u201cT\u201ds to \u201cU\u201ds for you, you may disable this behaviour by passing <code>replace_T_with_U=False</code>. <code>RnaTokenizer</code> does not perform other substitutions.</p>"},{"location":"models/rnamsm/#training-procedure","title":"Training Procedure","text":""},{"location":"models/rnamsm/#preprocessing","title":"Preprocessing","text":"<p>RNA-MSM used masked language modeling (MLM) as the pre-training objective. The masking procedure is similar to the one used in BERT:</p> <ul> <li>15% of the tokens are masked.</li> <li>In 80% of the cases, the masked tokens are replaced by <code>&lt;mask&gt;</code>.</li> <li>In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.</li> <li>In the 10% remaining cases, the masked tokens are left as is.</li> </ul>"},{"location":"models/rnamsm/#pretraining","title":"Pretraining","text":"<p>The model was trained on 8 NVIDIA V100 GPUs with 32GiB memories.</p> <ul> <li>Learning rate: 3e-4</li> <li>Weight decay: 3e-4</li> <li>Optimizer: Adam</li> <li>Learning rate warm-up: 16,000 steps</li> <li>Epochs: 300</li> <li>Batch Size: 1</li> <li>Dropout: 0.1</li> </ul>"},{"location":"models/rnamsm/#citation","title":"Citation","text":"<p>BibTeX:</p> BibTeX<pre><code>@article{zhang2023multiple,\n    author = {Zhang, Yikun and Lang, Mei and Jiang, Jiuhong and Gao, Zhiqiang and Xu, Fan and Litfin, Thomas and Chen, Ke and Singh, Jaswinder and Huang, Xiansong and Song, Guoli and Tian, Yonghong and Zhan, Jian and Chen, Jie and Zhou, Yaoqi},\n    title = \"{Multiple sequence alignment-based RNA language model and its application to structural inference}\",\n    journal = {Nucleic Acids Research},\n    volume = {52},\n    number = {1},\n    pages = {e3-e3},\n    year = {2023},\n    month = {11},\n    abstract = \"{Compared with proteins, DNA and RNA are more difficult languages to interpret because four-letter coded DNA/RNA sequences have less information content than 20-letter coded protein sequences. While BERT (Bidirectional Encoder Representations from Transformers)-like language models have been developed for RNA, they are ineffective at capturing the evolutionary information from homologous sequences because\u00a0unlike proteins, RNA sequences are less conserved. Here, we have developed an unsupervised multiple sequence alignment-based RNA language model (RNA-MSM) by utilizing homologous sequences from an automatic pipeline, RNAcmap, as it can provide significantly more homologous sequences than manually annotated Rfam. We demonstrate that the resulting unsupervised, two-dimensional attention maps and one-dimensional embeddings from RNA-MSM contain structural information. In fact, they can be directly mapped with high accuracy to 2D base pairing probabilities and 1D solvent accessibilities, respectively. Further fine-tuning led to significantly improved performance on these two downstream tasks compared with existing state-of-the-art techniques including SPOT-RNA2 and RNAsnap2. By comparison, RNA-FM, a BERT-based RNA language model, performs worse than one-hot encoding with its embedding in base pair and solvent-accessible surface area prediction. We anticipate that the pre-trained RNA-MSM model can be fine-tuned on many other tasks related to RNA structure and function.}\",\n    issn = {0305-1048},\n    doi = {10.1093/nar/gkad1031},\n    url = {https://doi.org/10.1093/nar/gkad1031},\n    eprint = {https://academic.oup.com/nar/article-pdf/52/1/e3/55443207/gkad1031.pdf},\n}\n</code></pre>"},{"location":"models/rnamsm/#contact","title":"Contact","text":"<p>Please use GitHub issues of MultiMolecule for any questions or comments on the model card.</p> <p>Please contact the authors of the RNA-MSM paper for questions or comments on the paper/model.</p>"},{"location":"models/rnamsm/#license","title":"License","text":"<p>This model is licensed under the AGPL-3.0 License.</p> Text Only<pre><code>SPDX-License-Identifier: AGPL-3.0-or-later\n</code></pre>"},{"location":"models/rnamsm/#multimolecule.models.rnamsm","title":"<code>multimolecule.models.rnamsm</code>","text":""},{"location":"models/rnamsm/#multimolecule.models.rnamsm.RnaTokenizer","title":"<code>RnaTokenizer</code>","text":"<p>               Bases: <code>Tokenizer</code></p> <p>Constructs an RNA tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>alphabet</code> <code>List[str] | None</code> <p>List of tokens to use. Defaults to IUPAC nucleotide code.</p> <code>None</code> <code>nmers</code> <code>int</code> <p>Size of nmers to tokenize. Defaults to 1.</p> <code>1</code> <code>codon</code> <code>bool</code> <p>Whether to tokenize into codons. Defaults to False.</p> <code>False</code> <code>replace_T_with_U</code> <code>bool</code> <p>Whether to replace T with U. Defaults to True.</p> <code>True</code> <code>do_upper_case</code> <code>bool</code> <p>Whether to convert input to uppercase. Defaults to True.</p> <code>True</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import RnaTokenizer\n&gt;&gt;&gt; tokenizer = RnaTokenizer()\n&gt;&gt;&gt; tokenizer('&lt;pad&gt;&lt;cls&gt;&lt;eos&gt;&lt;unk&gt;&lt;mask&gt;&lt;null&gt;ACGUNXVHDBMRWSYK.*-')[\"input_ids\"]\n[1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 2]\n&gt;&gt;&gt; tokenizer('acgu')[\"input_ids\"]\n[1, 6, 7, 8, 9, 2]\n&gt;&gt;&gt; tokenizer('acgt')[\"input_ids\"]\n[1, 6, 7, 8, 9, 2]\n&gt;&gt;&gt; tokenizer = RnaTokenizer(replace_T_with_U=False)\n&gt;&gt;&gt; tokenizer('acgt')[\"input_ids\"]\n[1, 6, 7, 8, 3, 2]\n&gt;&gt;&gt; tokenizer = RnaTokenizer(nmers=3)\n&gt;&gt;&gt; tokenizer('uagcuuauc')[\"input_ids\"]\n[1, 83, 17, 64, 49, 96, 84, 22, 2]\n&gt;&gt;&gt; tokenizer = RnaTokenizer(codon=True)\n&gt;&gt;&gt; tokenizer('uagcuuauc')[\"input_ids\"]\n[1, 83, 49, 22, 2]\n&gt;&gt;&gt; tokenizer('uagcuuauca')[\"input_ids\"]\nTraceback (most recent call last):\nValueError: length of input sequence  must be a multiple of 3 for codon tokenization, but got 10\n</code></pre> Source code in <code>multimolecule/tokenisers/rna/tokenization_rna.py</code> Python<pre><code>class RnaTokenizer(Tokenizer):\n    \"\"\"\n    Constructs an RNA tokenizer.\n\n    Args:\n        alphabet (List[str] | None, optional): List of tokens to use.\n            Defaults to [IUPAC nucleotide code](https://www.bioinformatics.org/sms2/iupac.html).\n        nmers (int, optional): Size of nmers to tokenize.\n            Defaults to 1.\n        codon (bool, optional): Whether to tokenize into codons.\n            Defaults to False.\n        replace_T_with_U (bool, optional): Whether to replace T with U.\n            Defaults to True.\n        do_upper_case (bool, optional): Whether to convert input to uppercase.\n            Defaults to True.\n\n    Examples:\n        &gt;&gt;&gt; from multimolecule import RnaTokenizer\n        &gt;&gt;&gt; tokenizer = RnaTokenizer()\n        &gt;&gt;&gt; tokenizer('&lt;pad&gt;&lt;cls&gt;&lt;eos&gt;&lt;unk&gt;&lt;mask&gt;&lt;null&gt;ACGUNXVHDBMRWSYK.*-')[\"input_ids\"]\n        [1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 2]\n        &gt;&gt;&gt; tokenizer('acgu')[\"input_ids\"]\n        [1, 6, 7, 8, 9, 2]\n        &gt;&gt;&gt; tokenizer('acgt')[\"input_ids\"]\n        [1, 6, 7, 8, 9, 2]\n        &gt;&gt;&gt; tokenizer = RnaTokenizer(replace_T_with_U=False)\n        &gt;&gt;&gt; tokenizer('acgt')[\"input_ids\"]\n        [1, 6, 7, 8, 3, 2]\n        &gt;&gt;&gt; tokenizer = RnaTokenizer(nmers=3)\n        &gt;&gt;&gt; tokenizer('uagcuuauc')[\"input_ids\"]\n        [1, 83, 17, 64, 49, 96, 84, 22, 2]\n        &gt;&gt;&gt; tokenizer = RnaTokenizer(codon=True)\n        &gt;&gt;&gt; tokenizer('uagcuuauc')[\"input_ids\"]\n        [1, 83, 49, 22, 2]\n        &gt;&gt;&gt; tokenizer('uagcuuauca')[\"input_ids\"]\n        Traceback (most recent call last):\n        ValueError: length of input sequence  must be a multiple of 3 for codon tokenization, but got 10\n    \"\"\"\n\n    model_input_names = [\"input_ids\", \"attention_mask\"]\n\n    def __init__(\n        self,\n        alphabet: List[str] | None = None,\n        nmers: int = 1,\n        codon: bool = False,\n        replace_T_with_U: bool = True,\n        bos_token: str = \"&lt;cls&gt;\",\n        cls_token: str = \"&lt;cls&gt;\",\n        pad_token: str = \"&lt;pad&gt;\",\n        eos_token: str = \"&lt;eos&gt;\",\n        sep_token: str = \"&lt;eos&gt;\",\n        unk_token: str = \"&lt;unk&gt;\",\n        mask_token: str = \"&lt;mask&gt;\",\n        additional_special_tokens: List | Tuple | None = None,\n        do_upper_case: bool = True,\n        **kwargs,\n    ):\n        if codon and nmers &gt; 1:\n            raise ValueError(\"Codon and nmers cannot be used together.\")\n        if codon:\n            nmers = 3  # set to 3 to get correct vocab\n        super().__init__(\n            alphabet=get_vocab_list(alphabet, nmers),\n            bos_token=bos_token,\n            cls_token=cls_token,\n            pad_token=pad_token,\n            eos_token=eos_token,\n            sep_token=sep_token,\n            unk_token=unk_token,\n            mask_token=mask_token,\n            additional_special_tokens=additional_special_tokens,\n            do_upper_case=do_upper_case,\n            **kwargs,\n        )\n        self.replace_T_with_U = replace_T_with_U\n        self.nmers = nmers\n        self.condon = codon\n\n    def _tokenize(self, text: str, **kwargs):\n        if self.do_upper_case:\n            text = text.upper()\n        if self.replace_T_with_U:\n            text = text.replace(\"T\", \"U\")\n        if self.condon:\n            if len(text) % 3 != 0:\n                raise ValueError(\n                    f\"length of input sequence  must be a multiple of 3 for codon tokenization, but got {len(text)}\"\n                )\n            return [text[i : i + 3] for i in range(0, len(text), 3)]\n        if self.nmers &gt; 1:\n            return [text[i : i + self.nmers] for i in range(len(text) - self.nmers + 1)]  # noqa: E203\n        return list(text)\n</code></pre>"},{"location":"models/rnamsm/#multimolecule.models.rnamsm.RnaMsmConfig","title":"<code>RnaMsmConfig</code>","text":"<p>               Bases: <code>PreTrainedConfig</code></p> <p>This is the configuration class to store the configuration of a [<code>RnaMsmModel</code>]. It is used to instantiate a RnaMsm model according to the specified arguments, defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of the RnaMsm yikunpku/RNA-MSM architecture.</p> <p>Configuration objects inherit from [<code>PreTrainedConfig</code>] and can be used to control the model outputs. Read the documentation from [<code>PreTrainedConfig</code>] for more information.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_size</code> <code>`int`, *optional*, defaults to 25</code> <p>Vocabulary size of the RnaMsm model. Defines the number of different tokens that can be represented by the <code>inputs_ids</code> passed when calling [<code>RnaMsmModel</code>].</p> <code>25</code> <code>hidden_size</code> <code>`int`, *optional*, defaults to 768</code> <p>Dimensionality of the encoder layers and the pooler layer.</p> <code>768</code> <code>num_hidden_layers</code> <code>`int`, *optional*, defaults to 10</code> <p>Number of hidden layers in the Transformer encoder.</p> <code>10</code> <code>num_attention_heads</code> <code>`int`, *optional*, defaults to 12</code> <p>Number of attention heads for each attention layer in the Transformer encoder.</p> <code>12</code> <code>intermediate_size</code> <code>`int`, *optional*, defaults to 3072</code> <p>Dimensionality of the \u201cintermediate\u201d (often named feed-forward) layer in the Transformer encoder.</p> <code>3072</code> <code>hidden_dropout</code> <code>`float`, *optional*, defaults to 0.1</code> <p>The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.</p> <code>0.1</code> <code>attention_dropout</code> <code>`float`, *optional*, defaults to 0.1</code> <p>The dropout ratio for the attention probabilities.</p> <code>0.1</code> <code>max_position_embeddings</code> <code>`int`, *optional*, defaults to 1024</code> <p>The maximum sequence length that this model might ever be used with. Typically set this to something large just in case (e.g., 512 or 1024 or 2048).</p> <code>1024</code> <code>initializer_range</code> <code>`float`, *optional*, defaults to 0.02</code> <p>The standard deviation of the truncated_normal_initializer for initializing all weight matrices.</p> <code>0.02</code> <code>layer_norm_eps</code> <code>`float`, *optional*, defaults to 1e-12</code> <p>The epsilon used by the layer normalization layers.</p> <code>1e-12</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import RnaMsmModel, RnaMsmConfig\n</code></pre> Python Console Session<pre><code>&gt;&gt;&gt; # Initializing a RNA-MSM multimolecule/rnamsm style configuration\n&gt;&gt;&gt; configuration = RnaMsmConfig()\n</code></pre> Python Console Session<pre><code>&gt;&gt;&gt; # Initializing a model (with random weights) from the multimolecule/rnamsm style configuration\n&gt;&gt;&gt; model = RnaMsmModel(configuration)\n</code></pre> Python Console Session<pre><code>&gt;&gt;&gt; # Accessing the model configuration\n&gt;&gt;&gt; configuration = model.config\n</code></pre> Source code in <code>multimolecule/models/rnamsm/configuration_rnamsm.py</code> Python<pre><code>class RnaMsmConfig(PreTrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`RnaMsmModel`]. It is used to instantiate a\n    RnaMsm model according to the specified arguments, defining the model architecture. Instantiating a configuration\n    with the defaults will yield a similar configuration to that of the RnaMsm\n    [yikunpku/RNA-MSM](https://github.com/yikunpku/RNA-MSM) architecture.\n\n    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n    documentation from [`PreTrainedConfig`] for more information.\n\n\n    Args:\n        vocab_size (`int`, *optional*, defaults to 25):\n            Vocabulary size of the RnaMsm model. Defines the number of different tokens that can be represented by the\n            `inputs_ids` passed when calling [`RnaMsmModel`].\n        hidden_size (`int`, *optional*, defaults to 768):\n            Dimensionality of the encoder layers and the pooler layer.\n        num_hidden_layers (`int`, *optional*, defaults to 10):\n            Number of hidden layers in the Transformer encoder.\n        num_attention_heads (`int`, *optional*, defaults to 12):\n            Number of attention heads for each attention layer in the Transformer encoder.\n        intermediate_size (`int`, *optional*, defaults to 3072):\n            Dimensionality of the \"intermediate\" (often named feed-forward) layer in the Transformer encoder.\n        hidden_dropout (`float`, *optional*, defaults to 0.1):\n            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n        attention_dropout (`float`, *optional*, defaults to 0.1):\n            The dropout ratio for the attention probabilities.\n        max_position_embeddings (`int`, *optional*, defaults to 1024):\n            The maximum sequence length that this model might ever be used with. Typically set this to something large\n            just in case (e.g., 512 or 1024 or 2048).\n        initializer_range (`float`, *optional*, defaults to 0.02):\n            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n        layer_norm_eps (`float`, *optional*, defaults to 1e-12):\n            The epsilon used by the layer normalization layers.\n\n    Examples:\n        &gt;&gt;&gt; from multimolecule import RnaMsmModel, RnaMsmConfig\n\n        &gt;&gt;&gt; # Initializing a RNA-MSM multimolecule/rnamsm style configuration\n        &gt;&gt;&gt; configuration = RnaMsmConfig()\n\n        &gt;&gt;&gt; # Initializing a model (with random weights) from the multimolecule/rnamsm style configuration\n        &gt;&gt;&gt; model = RnaMsmModel(configuration)\n\n        &gt;&gt;&gt; # Accessing the model configuration\n        &gt;&gt;&gt; configuration = model.config\n    \"\"\"\n\n    model_type = \"rnamsm\"\n\n    def __init__(\n        self,\n        vocab_size=25,\n        hidden_size=768,\n        num_hidden_layers=10,\n        num_attention_heads=12,\n        intermediate_size=3072,\n        hidden_act=\"gelu\",\n        hidden_dropout=0.1,\n        attention_dropout=0.1,\n        max_position_embeddings=1024,\n        initializer_range=0.02,\n        layer_norm_eps=1e-12,\n        position_embedding_type=\"absolute\",\n        use_cache=True,\n        max_tokens_per_msa=2**14,\n        layer_type=\"standard\",\n        attention_type=\"standard\",\n        embed_positions_msa=True,\n        attention_bias=True,\n        head=None,\n        lm_head=None,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n\n        self.vocab_size = vocab_size\n        self.hidden_size = hidden_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.intermediate_size = intermediate_size\n        self.hidden_act = hidden_act\n        self.hidden_dropout = hidden_dropout\n        self.attention_dropout = attention_dropout\n        self.max_position_embeddings = max_position_embeddings\n        self.initializer_range = initializer_range\n        self.layer_norm_eps = layer_norm_eps\n        self.position_embedding_type = position_embedding_type\n        self.use_cache = use_cache\n        self.max_tokens_per_msa = max_tokens_per_msa\n        self.layer_type = layer_type\n        self.attention_type = attention_type\n        self.embed_positions_msa = embed_positions_msa\n        self.attention_bias = attention_bias\n        self.head = HeadConfig(**head if head is not None else {})\n        self.lm_head = MaskedLMHeadConfig(**lm_head if lm_head is not None else {})\n</code></pre>"},{"location":"models/rnamsm/#multimolecule.models.rnamsm.RnaMsmForMaskedLM","title":"<code>RnaMsmForMaskedLM</code>","text":"<p>               Bases: <code>RnaMsmPreTrainedModel</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import RnaMsmConfig, RnaMsmForMaskedLM, RnaTokenizer\n&gt;&gt;&gt; config = RnaMsmConfig()\n&gt;&gt;&gt; model = RnaMsmForMaskedLM(config)\n&gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n&gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n&gt;&gt;&gt; output = model(**input)\n</code></pre> Source code in <code>multimolecule/models/rnamsm/modeling_rnamsm.py</code> Python<pre><code>class RnaMsmForMaskedLM(RnaMsmPreTrainedModel):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; from multimolecule import RnaMsmConfig, RnaMsmForMaskedLM, RnaTokenizer\n        &gt;&gt;&gt; config = RnaMsmConfig()\n        &gt;&gt;&gt; model = RnaMsmForMaskedLM(config)\n        &gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n        &gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n        &gt;&gt;&gt; output = model(**input)\n    \"\"\"\n\n    def __init__(self, config: RnaMsmConfig):\n        super().__init__(config)\n        self.rnamsm = RnaMsmModel(config, add_pooling_layer=False)\n        self.lm_head = MaskedLMHead(config, weight=self.rnamsm.embeddings.word_embeddings.weight)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(\n        self,\n        input_ids: Tensor | NestedTensor,\n        attention_mask: Tensor | None = None,\n        position_ids: Tensor | None = None,\n        inputs_embeds: Tensor | NestedTensor | None = None,\n        labels: Tensor | None = None,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n    ) -&gt; Tuple[Tensor, ...] | RnaMsmForMaskedLMOutput:\n        outputs = self.rnamsm(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        output = self.lm_head(outputs, labels)\n        logits, loss = output.logits, output.loss\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return RnaMsmForMaskedLMOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            col_attentions=outputs.col_attentions,\n            row_attentions=outputs.row_attentions,\n        )\n</code></pre>"},{"location":"models/rnamsm/#multimolecule.models.rnamsm.RnaMsmForNucleotideClassification","title":"<code>RnaMsmForNucleotideClassification</code>","text":"<p>               Bases: <code>RnaMsmPreTrainedModel</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import RnaMsmConfig, RnaMsmForNucleotideClassification, RnaTokenizer\n&gt;&gt;&gt; config = RnaMsmConfig()\n&gt;&gt;&gt; model = RnaMsmForNucleotideClassification(config)\n&gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n&gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n&gt;&gt;&gt; output = model(**input)\n</code></pre> Source code in <code>multimolecule/models/rnamsm/modeling_rnamsm.py</code> Python<pre><code>class RnaMsmForNucleotideClassification(RnaMsmPreTrainedModel):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; from multimolecule import RnaMsmConfig, RnaMsmForNucleotideClassification, RnaTokenizer\n        &gt;&gt;&gt; config = RnaMsmConfig()\n        &gt;&gt;&gt; model = RnaMsmForNucleotideClassification(config)\n        &gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n        &gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n        &gt;&gt;&gt; output = model(**input)\n    \"\"\"\n\n    def __init__(self, config: RnaMsmConfig):\n        super().__init__(config)\n        self.num_labels = config.head.num_labels\n        self.rnamsm = RnaMsmModel(config, add_pooling_layer=False)\n        self.nucleotide_head = NucleotideClassificationHead(config)\n        self.head_config = self.nucleotide_head.config\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(\n        self,\n        input_ids: Tensor | NestedTensor,\n        attention_mask: Tensor | None = None,\n        position_ids: Tensor | None = None,\n        inputs_embeds: Tensor | NestedTensor | None = None,\n        labels: Tensor | None = None,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n        **kwargs,\n    ) -&gt; Tuple[Tensor, ...] | RnaMsmForTokenClassifierOutput:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n            `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        if kwargs:\n            warn(\n                f\"Additional keyword arguments `{', '.join(kwargs)}` are detected in \"\n                f\"`{self.__class__.__name__}.forward`, they will be ignored.\\n\"\n                \"This is provided for backward compatibility and may lead to unexpected behavior.\"\n            )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.rnamsm(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        output = self.nucleotide_head(outputs, attention_mask, input_ids, labels)\n        logits, loss = output.logits, output.loss\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return RnaMsmForTokenClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            col_attentions=outputs.col_attentions,\n            row_attentions=outputs.row_attentions,\n        )\n</code></pre>"},{"location":"models/rnamsm/#multimolecule.models.rnamsm.RnaMsmForNucleotideClassification.forward","title":"<code>forward(input_ids, attention_mask=None, position_ids=None, inputs_embeds=None, labels=None, output_attentions=False, output_hidden_states=False, return_dict=True, **kwargs)</code>","text":"<p>labels (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, optional):     Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ...,     config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If     <code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).</p> Source code in <code>multimolecule/models/rnamsm/modeling_rnamsm.py</code> Python<pre><code>def forward(\n    self,\n    input_ids: Tensor | NestedTensor,\n    attention_mask: Tensor | None = None,\n    position_ids: Tensor | None = None,\n    inputs_embeds: Tensor | NestedTensor | None = None,\n    labels: Tensor | None = None,\n    output_attentions: bool = False,\n    output_hidden_states: bool = False,\n    return_dict: bool = True,\n    **kwargs,\n) -&gt; Tuple[Tensor, ...] | RnaMsmForTokenClassifierOutput:\n    r\"\"\"\n    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n        `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).\n    \"\"\"\n    if kwargs:\n        warn(\n            f\"Additional keyword arguments `{', '.join(kwargs)}` are detected in \"\n            f\"`{self.__class__.__name__}.forward`, they will be ignored.\\n\"\n            \"This is provided for backward compatibility and may lead to unexpected behavior.\"\n        )\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n    outputs = self.rnamsm(\n        input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        inputs_embeds=inputs_embeds,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n    output = self.nucleotide_head(outputs, attention_mask, input_ids, labels)\n    logits, loss = output.logits, output.loss\n\n    if not return_dict:\n        output = (logits,) + outputs[2:]\n        return ((loss,) + output) if loss is not None else output\n\n    return RnaMsmForTokenClassifierOutput(\n        loss=loss,\n        logits=logits,\n        hidden_states=outputs.hidden_states,\n        col_attentions=outputs.col_attentions,\n        row_attentions=outputs.row_attentions,\n    )\n</code></pre>"},{"location":"models/rnamsm/#multimolecule.models.rnamsm.RnaMsmForPretraining","title":"<code>RnaMsmForPretraining</code>","text":"<p>               Bases: <code>RnaMsmPreTrainedModel</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import RnaMsmConfig, RnaMsmForPretraining, RnaTokenizer\n&gt;&gt;&gt; config = RnaMsmConfig()\n&gt;&gt;&gt; model = RnaMsmForPretraining(config)\n&gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n&gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n&gt;&gt;&gt; output = model(**input)\n</code></pre> Source code in <code>multimolecule/models/rnamsm/modeling_rnamsm.py</code> Python<pre><code>class RnaMsmForPretraining(RnaMsmPreTrainedModel):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; from multimolecule import RnaMsmConfig, RnaMsmForPretraining, RnaTokenizer\n        &gt;&gt;&gt; config = RnaMsmConfig()\n        &gt;&gt;&gt; model = RnaMsmForPretraining(config)\n        &gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n        &gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n        &gt;&gt;&gt; output = model(**input)\n    \"\"\"\n\n    def __init__(self, config: RnaMsmConfig):\n        super().__init__(config)\n        self.rnamsm = RnaMsmModel(config, add_pooling_layer=False)\n        self.pretrain_head = RnaMsmPreTrainingHeads(config, weight=self.rnamsm.embeddings.word_embeddings.weight)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(\n        self,\n        input_ids: Tensor | NestedTensor,\n        attention_mask: Tensor | None = None,\n        position_ids: Tensor | None = None,\n        inputs_embeds: Tensor | NestedTensor | None = None,\n        labels: Tensor | None = None,\n        labels_contact: Tensor | None = None,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n    ) -&gt; Tuple[Tensor, ...] | RnaMsmForPretrainingOutput:\n        outputs = self.rnamsm(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            inputs_embeds=inputs_embeds,\n            output_attentions=True,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        logits, contact_map = self.pretrain_head(outputs, attention_mask, input_ids)\n\n        loss = None\n        if any(x is not None for x in [labels, labels_contact]):\n            loss_mlm = loss_contact = 0\n            if labels is not None:\n                loss_mlm = F.cross_entropy(logits.view(-1, self.config.vocab_size), labels.view(-1))\n            if labels_contact is not None:\n                loss_contact = F.mse_loss(contact_map.view(-1), labels_contact.view(-1))\n            loss = loss_mlm + loss_contact\n\n        if not return_dict:\n            output = (logits, contact_map) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return RnaMsmForPretrainingOutput(\n            loss=loss,\n            logits=logits,\n            contact_map=contact_map,\n            hidden_states=outputs.hidden_states,\n            col_attentions=outputs.col_attentions,\n            row_attentions=outputs.row_attentions,\n        )\n</code></pre>"},{"location":"models/rnamsm/#multimolecule.models.rnamsm.RnaMsmForSequenceClassification","title":"<code>RnaMsmForSequenceClassification</code>","text":"<p>               Bases: <code>RnaMsmPreTrainedModel</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import RnaMsmConfig, RnaMsmForSequenceClassification, RnaTokenizer\n&gt;&gt;&gt; config = RnaMsmConfig()\n&gt;&gt;&gt; model = RnaMsmForSequenceClassification(config)\n&gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n&gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n&gt;&gt;&gt; output = model(**input)\n</code></pre> Source code in <code>multimolecule/models/rnamsm/modeling_rnamsm.py</code> Python<pre><code>class RnaMsmForSequenceClassification(RnaMsmPreTrainedModel):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; from multimolecule import RnaMsmConfig, RnaMsmForSequenceClassification, RnaTokenizer\n        &gt;&gt;&gt; config = RnaMsmConfig()\n        &gt;&gt;&gt; model = RnaMsmForSequenceClassification(config)\n        &gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n        &gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n        &gt;&gt;&gt; output = model(**input)\n    \"\"\"\n\n    def __init__(self, config: RnaMsmConfig):\n        super().__init__(config)\n        self.num_labels = config.head.num_labels\n        self.rnamsm = RnaMsmModel(config, add_pooling_layer=True)\n        self.sequence_head = SequenceClassificationHead(config)\n        self.head_config = self.sequence_head.config\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(\n        self,\n        input_ids: Tensor | NestedTensor,\n        attention_mask: Tensor | None = None,\n        position_ids: Tensor | None = None,\n        inputs_embeds: Tensor | NestedTensor | None = None,\n        labels: Tensor | None = None,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n    ) -&gt; Tuple[Tensor, ...] | RnaMsmForSequenceClassifierOutput:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n            `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.rnamsm(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        output = self.sequence_head(outputs, labels)\n        logits, loss = output.logits, output.loss\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return RnaMsmForSequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            col_attentions=outputs.col_attentions,\n            row_attentions=outputs.row_attentions,\n        )\n</code></pre>"},{"location":"models/rnamsm/#multimolecule.models.rnamsm.RnaMsmForSequenceClassification.forward","title":"<code>forward(input_ids, attention_mask=None, position_ids=None, inputs_embeds=None, labels=None, output_attentions=False, output_hidden_states=False, return_dict=True)</code>","text":"<p>labels (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, optional):     Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ...,     config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If     <code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).</p> Source code in <code>multimolecule/models/rnamsm/modeling_rnamsm.py</code> Python<pre><code>def forward(\n    self,\n    input_ids: Tensor | NestedTensor,\n    attention_mask: Tensor | None = None,\n    position_ids: Tensor | None = None,\n    inputs_embeds: Tensor | NestedTensor | None = None,\n    labels: Tensor | None = None,\n    output_attentions: bool = False,\n    output_hidden_states: bool = False,\n    return_dict: bool = True,\n) -&gt; Tuple[Tensor, ...] | RnaMsmForSequenceClassifierOutput:\n    r\"\"\"\n    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n        `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).\n    \"\"\"\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n    outputs = self.rnamsm(\n        input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        inputs_embeds=inputs_embeds,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n    output = self.sequence_head(outputs, labels)\n    logits, loss = output.logits, output.loss\n\n    if not return_dict:\n        output = (logits,) + outputs[2:]\n        return ((loss,) + output) if loss is not None else output\n\n    return RnaMsmForSequenceClassifierOutput(\n        loss=loss,\n        logits=logits,\n        hidden_states=outputs.hidden_states,\n        col_attentions=outputs.col_attentions,\n        row_attentions=outputs.row_attentions,\n    )\n</code></pre>"},{"location":"models/rnamsm/#multimolecule.models.rnamsm.RnaMsmForTokenClassification","title":"<code>RnaMsmForTokenClassification</code>","text":"<p>               Bases: <code>RnaMsmPreTrainedModel</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import RnaMsmConfig, RnaMsmForTokenClassification, RnaTokenizer\n&gt;&gt;&gt; config = RnaMsmConfig()\n&gt;&gt;&gt; model = RnaMsmForTokenClassification(config)\n&gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n&gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n&gt;&gt;&gt; output = model(**input)\n</code></pre> Source code in <code>multimolecule/models/rnamsm/modeling_rnamsm.py</code> Python<pre><code>class RnaMsmForTokenClassification(RnaMsmPreTrainedModel):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; from multimolecule import RnaMsmConfig, RnaMsmForTokenClassification, RnaTokenizer\n        &gt;&gt;&gt; config = RnaMsmConfig()\n        &gt;&gt;&gt; model = RnaMsmForTokenClassification(config)\n        &gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n        &gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n        &gt;&gt;&gt; output = model(**input)\n    \"\"\"\n\n    def __init__(self, config: RnaMsmConfig):\n        super().__init__(config)\n        self.num_labels = config.head.num_labels\n        self.rnamsm = RnaMsmModel(config, add_pooling_layer=False)\n        self.token_head = TokenClassificationHead(config)\n        self.head_config = self.token_head.config\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(\n        self,\n        input_ids: Tensor | NestedTensor,\n        attention_mask: Tensor | None = None,\n        position_ids: Tensor | None = None,\n        inputs_embeds: Tensor | NestedTensor | None = None,\n        labels: Tensor | None = None,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        return_dict: bool = True,\n    ) -&gt; Tuple[Tensor, ...] | RnaMsmForTokenClassifierOutput:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n            `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.rnamsm(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        output = self.token_head(outputs, attention_mask, input_ids, labels)\n        logits, loss = output.logits, output.loss\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return RnaMsmForTokenClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            col_attentions=outputs.col_attentions,\n            row_attentions=outputs.row_attentions,\n        )\n</code></pre>"},{"location":"models/rnamsm/#multimolecule.models.rnamsm.RnaMsmForTokenClassification.forward","title":"<code>forward(input_ids, attention_mask=None, position_ids=None, inputs_embeds=None, labels=None, output_attentions=False, output_hidden_states=False, return_dict=True)</code>","text":"<p>labels (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, optional):     Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ...,     config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If     <code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).</p> Source code in <code>multimolecule/models/rnamsm/modeling_rnamsm.py</code> Python<pre><code>def forward(\n    self,\n    input_ids: Tensor | NestedTensor,\n    attention_mask: Tensor | None = None,\n    position_ids: Tensor | None = None,\n    inputs_embeds: Tensor | NestedTensor | None = None,\n    labels: Tensor | None = None,\n    output_attentions: bool = False,\n    output_hidden_states: bool = False,\n    return_dict: bool = True,\n) -&gt; Tuple[Tensor, ...] | RnaMsmForTokenClassifierOutput:\n    r\"\"\"\n    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n        `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).\n    \"\"\"\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n    outputs = self.rnamsm(\n        input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        inputs_embeds=inputs_embeds,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n    output = self.token_head(outputs, attention_mask, input_ids, labels)\n    logits, loss = output.logits, output.loss\n\n    if not return_dict:\n        output = (logits,) + outputs[2:]\n        return ((loss,) + output) if loss is not None else output\n\n    return RnaMsmForTokenClassifierOutput(\n        loss=loss,\n        logits=logits,\n        hidden_states=outputs.hidden_states,\n        col_attentions=outputs.col_attentions,\n        row_attentions=outputs.row_attentions,\n    )\n</code></pre>"},{"location":"models/rnamsm/#multimolecule.models.rnamsm.RnaMsmModel","title":"<code>RnaMsmModel</code>","text":"<p>               Bases: <code>RnaMsmPreTrainedModel</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import RnaMsmConfig, RnaMsmModel, RnaTokenizer\n&gt;&gt;&gt; config = RnaMsmConfig()\n&gt;&gt;&gt; model = RnaMsmModel(config)\n&gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n&gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n&gt;&gt;&gt; output = model(**input)\n</code></pre> Source code in <code>multimolecule/models/rnamsm/modeling_rnamsm.py</code> Python<pre><code>class RnaMsmModel(RnaMsmPreTrainedModel):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; from multimolecule import RnaMsmConfig, RnaMsmModel, RnaTokenizer\n        &gt;&gt;&gt; config = RnaMsmConfig()\n        &gt;&gt;&gt; model = RnaMsmModel(config)\n        &gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n        &gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n        &gt;&gt;&gt; output = model(**input)\n    \"\"\"\n\n    def __init__(self, config: RnaMsmConfig, add_pooling_layer: bool = True):\n        super().__init__(config)\n        self.pad_token_id = config.pad_token_id\n        self.embeddings = RnaMsmEmbeddings(config)\n        self.encoder = RnaMsmEncoder(config)\n        self.pooler = RnaMsmPooler(config) if add_pooling_layer else None\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(\n        self,\n        input_ids: Tensor | NestedTensor,\n        attention_mask: Tensor | None = None,\n        position_ids: Tensor | None = None,\n        inputs_embeds: Tensor | NestedTensor | None = None,\n        output_attentions: bool | None = None,\n        output_hidden_states: bool | None = None,\n        return_dict: bool | None = None,\n    ) -&gt; Tuple[Tensor, ...] | RnaMsmModelOutputWithPooling:\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if isinstance(input_ids, NestedTensor):\n            input_ids, attention_mask = input_ids.tensor, input_ids.mask\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n        if input_ids is not None:\n            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n        elif inputs_embeds is None:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n        if attention_mask is None:\n            attention_mask = (\n                input_ids.ne(self.pad_token_id) if self.pad_token_id is not None else torch.ones_like(input_ids)\n            )\n\n        unsqueeze_input = input_ids.ndim == 2\n        if unsqueeze_input:\n            input_ids = input_ids.unsqueeze(1)\n        if attention_mask.ndim == 2:\n            attention_mask = attention_mask.unsqueeze(1)\n\n        embedding_output = self.embeddings(\n            input_ids=input_ids,\n            position_ids=position_ids,\n            attention_mask=attention_mask,\n            inputs_embeds=inputs_embeds,\n        )\n        encoder_outputs = self.encoder(\n            embedding_output,\n            attention_mask=attention_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        sequence_output = encoder_outputs[0]\n        if unsqueeze_input:\n            sequence_output = sequence_output.squeeze(1)\n        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n\n        if not return_dict:\n            return (sequence_output, pooled_output) + encoder_outputs[1:]\n\n        return RnaMsmModelOutputWithPooling(\n            last_hidden_state=sequence_output,\n            pooler_output=pooled_output,\n            hidden_states=encoder_outputs.hidden_states,\n            col_attentions=encoder_outputs.col_attentions,\n            row_attentions=encoder_outputs.row_attentions,\n        )\n</code></pre>"},{"location":"models/rnamsm/#multimolecule.models.rnamsm.RnaMsmPreTrainedModel","title":"<code>RnaMsmPreTrainedModel</code>","text":"<p>               Bases: <code>PreTrainedModel</code></p> <p>An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.</p> Source code in <code>multimolecule/models/rnamsm/modeling_rnamsm.py</code> Python<pre><code>class RnaMsmPreTrainedModel(PreTrainedModel):\n    \"\"\"\n    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n    models.\n    \"\"\"\n\n    config_class = RnaMsmConfig\n    base_model_prefix = \"rnamsm\"\n    supports_gradient_checkpointing = True\n    _no_split_modules = [\"RnaMsmLayer\", \"RnaMsmAxialLayer\", \"RnaMsmPkmLayer\", \"RnaMsmEmbeddings\"]\n\n    # Copied from transformers.models.bert.modeling_bert.BertPreTrainedModel._init_weights\n    def _init_weights(self, module: nn.Module):\n        \"\"\"Initialize the weights\"\"\"\n        if isinstance(module, nn.Linear):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm) and module.elementwise_affine:\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n</code></pre>"},{"location":"models/splicebert/","title":"SpliceBERT","text":"<p>Pre-trained model on messenger RNA precursor (pre-mRNA) using a masked language modeling (MLM) objective.</p>"},{"location":"models/splicebert/#disclaimer","title":"Disclaimer","text":"<p>This is an UNOFFICIAL implementation of the Self-supervised learning on millions of pre-mRNA sequences improves sequence-based RNA splicing prediction by Ken Chen, et al.</p> <p>The OFFICIAL repository of SpliceBERT is at chenkenbio/SpliceBERT.</p> <p>The team releasing SpliceBERT did not write this model card for this model so this model card has been written by the MultiMolecule team.</p>"},{"location":"models/splicebert/#model-details","title":"Model Details","text":"<p>SpliceBERT is a bert-style model pre-trained on a large corpus of messenger RNA precursor sequences in a self-supervised fashion. This means that the model was trained on the raw nucleotides of RNA sequences only, with an automatic process to generate inputs and labels from those texts. Please refer to the Training Details section for more information on the training process.</p>"},{"location":"models/splicebert/#variations","title":"Variations","text":"<ul> <li><code>multimolecule/splicebert</code>: The SpliceBERT model.</li> <li><code>multimolecule/splicebert.510nt</code>: The intermediate SpliceBERT model.</li> <li><code>multimolecule/splicebert-human.510nt</code>: The intermediate SpliceBERT model pre-trained on human data only.</li> </ul>"},{"location":"models/splicebert/#model-specification","title":"Model Specification","text":"Variants Num Layers Hidden Size Num Heads Intermediate Size Num Parameters (M) FLOPs (G) MACs (G) Max Num Tokens splicebert 6 512 16 2048 19.72 5.04 2.52 1024 splicebert.510nt 19.45 510 splicebert-human.510nt"},{"location":"models/splicebert/#links","title":"Links","text":"<ul> <li>Code: multimolecule.splicebert</li> <li>Data: UCSC Genome Browser</li> <li>Paper: Self-supervised learning on millions of pre-mRNA sequences improves sequence-based RNA splicing prediction</li> <li>Developed by: Ken Chen, Yue Zhou, Maolin Ding, Yu Wang, Zhixiang Ren, Yuedong Yang</li> <li>Model type: BERT - FlashAttention</li> <li>Original Repository: https://github.com/chenkenbio/SpliceBERT</li> </ul>"},{"location":"models/splicebert/#usage","title":"Usage","text":"<p>The model file depends on the <code>multimolecule</code> library. You can install it using pip:</p> Bash<pre><code>pip install multimolecule\n</code></pre>"},{"location":"models/splicebert/#direct-use","title":"Direct Use","text":"<p>You can use this model directly with a pipeline for masked language modeling:</p> Python<pre><code>&gt;&gt;&gt; import multimolecule  # you must import multimolecule to register models\n&gt;&gt;&gt; from transformers import pipeline\n&gt;&gt;&gt; unmasker = pipeline('fill-mask', model='multimolecule/splicebert')\n&gt;&gt;&gt; unmasker(\"uagc&lt;mask&gt;uaucagacugauguuga\")\n\n[{'score': 0.10049360245466232,\n  'token': 6,\n  'token_str': 'A',\n  'sequence': 'U A G C A U A U C A G A C U G A U G U U G A'},\n {'score': 0.09413677453994751,\n  'token': 18,\n  'token_str': 'W',\n  'sequence': 'U A G C W U A U C A G A C U G A U G U U G A'},\n {'score': 0.0881819948554039,\n  'token': 9,\n  'token_str': 'U',\n  'sequence': 'U A G C U U A U C A G A C U G A U G U U G A'},\n {'score': 0.07516232132911682,\n  'token': 13,\n  'token_str': 'H',\n  'sequence': 'U A G C H U A U C A G A C U G A U G U U G A'},\n {'score': 0.0693921446800232,\n  'token': 16,\n  'token_str': 'M',\n  'sequence': 'U A G C M U A U C A G A C U G A U G U U G A'}]\n</code></pre>"},{"location":"models/splicebert/#downstream-use","title":"Downstream Use","text":""},{"location":"models/splicebert/#extract-features","title":"Extract Features","text":"<p>Here is how to use this model to get the features of a given sequence in PyTorch:</p> Python<pre><code>from multimolecule import RnaTokenizer, SpliceBertModel\n\n\ntokenizer = RnaTokenizer.from_pretrained('multimolecule/splicebert')\nmodel = SpliceBertModel.from_pretrained('multimolecule/splicebert')\n\ntext = \"UAGCUUAUCAGACUGAUGUUGA\"\ninput = tokenizer(text, return_tensors='pt')\n\noutput = model(**input)\n</code></pre>"},{"location":"models/splicebert/#sequence-classification-regression","title":"Sequence Classification / Regression","text":"<p>Note: This model is not fine-tuned for any specific task. You will need to fine-tune the model on a downstream task to use it for sequence classification or regression.</p> <p>Here is how to use this model as backbone to fine-tune for a sequence-level task in PyTorch:</p> Python<pre><code>import torch\nfrom multimolecule import RnaTokenizer, SpliceBertForSequenceClassification\n\n\ntokenizer = RnaTokenizer.from_pretrained('multimolecule/splicebert')\nmodel = SpliceBertModelForSequenceClassification.from_pretrained('multimolecule/splicebert')\n\ntext = \"UAGCUUAUCAGACUGAUGUUGA\"\ninput = tokenizer(text, return_tensors='pt')\nlabel = torch.tensor([1])\n\noutput = model(**input, labels=label)\n</code></pre>"},{"location":"models/splicebert/#nucleotide-classification-regression","title":"Nucleotide Classification / Regression","text":"<p>Note: This model is not fine-tuned for any specific task. You will need to fine-tune the model on a downstream task to use it for nucleotide classification or regression.</p> <p>Here is how to use this model as backbone to fine-tune for a nucleotide-level task in PyTorch:</p> Python<pre><code>import torch\nfrom multimolecule import RnaTokenizer, SpliceBertForNucleotideClassification\n\n\ntokenizer = RnaTokenizer.from_pretrained('multimolecule/splicebert')\nmodel = SpliceBertModelForNucleotideClassification.from_pretrained('multimolecule/splicebert')\n\ntext = \"UAGCUUAUCAGACUGAUGUUGA\"\ninput = tokenizer(text, return_tensors='pt')\nlabel = torch.randint(2, (len(text), ))\n\noutput = model(**input, labels=label)\n</code></pre>"},{"location":"models/splicebert/#training-details","title":"Training Details","text":"<p>SpliceBERT used Masked Language Modeling (MLM) as the pre-training objective: taking a sequence, the model randomly masks 15% of the tokens in the input then run the entire masked sentence through the model and has to predict the masked tokens. This is comparable to the Cloze task in language modeling.</p>"},{"location":"models/splicebert/#training-data","title":"Training Data","text":"<p>The SpliceBERT model was pre-trained on messenger RNA precursor sequences from UCSC Genome Browser. UCSC Genome Browser provides visualization, analysis, and download of comprehensive vertebrate genome data with aligned annotation tracks (known genes, predicted genes, ESTs, mRNAs, CpG islands, etc.).</p> <p>SpliceBERT collected reference genomes and gene annotations from the UCSC Genome Browser for 72 vertebrate species. It applied bedtools getfasta to extract pre-mRNA sequences from the reference genomes based on the gene annotations. The pre-mRNA sequences are then used to pre-train SpliceBERT. The pre-training data contains 2 million pre-mRNA sequences with a total length of 65 billion nucleotides.</p> <p>Note <code>RnaTokenizer</code> will convert \u201cT\u201ds to \u201cU\u201ds for you, you may disable this behaviour by passing <code>replace_T_with_U=False</code>.</p>"},{"location":"models/splicebert/#training-procedure","title":"Training Procedure","text":""},{"location":"models/splicebert/#preprocessing","title":"Preprocessing","text":"<p>SpliceBERT used masked language modeling (MLM) as the pre-training objective. The masking procedure is similar to the one used in BERT:</p> <ul> <li>15% of the tokens are masked.</li> <li>In 80% of the cases, the masked tokens are replaced by <code>&lt;mask&gt;</code>.</li> <li>In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.</li> <li>In the 10% remaining cases, the masked tokens are left as is.</li> </ul>"},{"location":"models/splicebert/#pretraining","title":"Pretraining","text":"<p>The model was trained on 8 NVIDIA V100 GPUs.</p> <ul> <li>Learning rate: 1e-4</li> <li>Learning rate scheduler: ReduceLROnPlateau(patience=3)</li> <li>Optimizer: AdamW</li> </ul> <p>SpliceBERT trained model in a two-stage training process:</p> <ol> <li>Pre-train with sequences of a fixed length of 510 nucleotides.</li> <li>Pre-train with sequences of a variable length between 64 and 1024 nucleotides.</li> </ol> <p>The intermediate model after the first stage is available as <code>multimolecule/splicebert.510nt</code>.</p> <p>SpliceBERT also pre-trained a model on human data only to validate the contribution of multi-species pre-training. The intermediate model after the first stage is available as <code>multimolecule/splicebert-human.510nt</code>.</p>"},{"location":"models/splicebert/#citation","title":"Citation","text":"<p>BibTeX:</p> BibTeX<pre><code>@article {chen2023self,\n    author = {Chen, Ken and Zhou, Yue and Ding, Maolin and Wang, Yu and Ren, Zhixiang and Yang, Yuedong},\n    title = {Self-supervised learning on millions of pre-mRNA sequences improves sequence-based RNA splicing prediction},\n    elocation-id = {2023.01.31.526427},\n    year = {2023},\n    doi = {10.1101/2023.01.31.526427},\n    publisher = {Cold Spring Harbor Laboratory},\n    abstract = {RNA splicing is an important post-transcriptional process of gene expression in eukaryotic cells. Predicting RNA splicing from primary sequences can facilitate the interpretation of genomic variants. In this study, we developed a novel self-supervised pre-trained language model, SpliceBERT, to improve sequence-based RNA splicing prediction. Pre-training on pre-mRNA sequences from vertebrates enables SpliceBERT to capture evolutionary conservation information and characterize the unique property of splice sites. SpliceBERT also improves zero-shot prediction of variant effects on splicing by considering sequence context information, and achieves superior performance for predicting branchpoint in the human genome and splice sites across species. Our study highlighted the importance of pre-training genomic language models on a diverse range of species and suggested that pre-trained language models were promising for deciphering the sequence logic of RNA splicing.Competing Interest StatementThe authors have declared no competing interest.},\n    URL = {https://www.biorxiv.org/content/early/2023/05/09/2023.01.31.526427},\n    eprint = {https://www.biorxiv.org/content/early/2023/05/09/2023.01.31.526427.full.pdf},\n    journal = {bioRxiv}\n}\n</code></pre>"},{"location":"models/splicebert/#contact","title":"Contact","text":"<p>Please use GitHub issues of MultiMolecule for any questions or comments on the model card.</p> <p>Please contact the authors of the SpliceBERT paper for questions or comments on the paper/model.</p>"},{"location":"models/splicebert/#license","title":"License","text":"<p>This model is licensed under the AGPL-3.0 License.</p> Text Only<pre><code>SPDX-License-Identifier: AGPL-3.0-or-later\n</code></pre>"},{"location":"models/splicebert/#multimolecule.models.splicebert","title":"<code>multimolecule.models.splicebert</code>","text":""},{"location":"models/splicebert/#multimolecule.models.splicebert.RnaTokenizer","title":"<code>RnaTokenizer</code>","text":"<p>               Bases: <code>Tokenizer</code></p> <p>Constructs an RNA tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>alphabet</code> <code>List[str] | None</code> <p>List of tokens to use. Defaults to IUPAC nucleotide code.</p> <code>None</code> <code>nmers</code> <code>int</code> <p>Size of nmers to tokenize. Defaults to 1.</p> <code>1</code> <code>codon</code> <code>bool</code> <p>Whether to tokenize into codons. Defaults to False.</p> <code>False</code> <code>replace_T_with_U</code> <code>bool</code> <p>Whether to replace T with U. Defaults to True.</p> <code>True</code> <code>do_upper_case</code> <code>bool</code> <p>Whether to convert input to uppercase. Defaults to True.</p> <code>True</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import RnaTokenizer\n&gt;&gt;&gt; tokenizer = RnaTokenizer()\n&gt;&gt;&gt; tokenizer('&lt;pad&gt;&lt;cls&gt;&lt;eos&gt;&lt;unk&gt;&lt;mask&gt;&lt;null&gt;ACGUNXVHDBMRWSYK.*-')[\"input_ids\"]\n[1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 2]\n&gt;&gt;&gt; tokenizer('acgu')[\"input_ids\"]\n[1, 6, 7, 8, 9, 2]\n&gt;&gt;&gt; tokenizer('acgt')[\"input_ids\"]\n[1, 6, 7, 8, 9, 2]\n&gt;&gt;&gt; tokenizer = RnaTokenizer(replace_T_with_U=False)\n&gt;&gt;&gt; tokenizer('acgt')[\"input_ids\"]\n[1, 6, 7, 8, 3, 2]\n&gt;&gt;&gt; tokenizer = RnaTokenizer(nmers=3)\n&gt;&gt;&gt; tokenizer('uagcuuauc')[\"input_ids\"]\n[1, 83, 17, 64, 49, 96, 84, 22, 2]\n&gt;&gt;&gt; tokenizer = RnaTokenizer(codon=True)\n&gt;&gt;&gt; tokenizer('uagcuuauc')[\"input_ids\"]\n[1, 83, 49, 22, 2]\n&gt;&gt;&gt; tokenizer('uagcuuauca')[\"input_ids\"]\nTraceback (most recent call last):\nValueError: length of input sequence  must be a multiple of 3 for codon tokenization, but got 10\n</code></pre> Source code in <code>multimolecule/tokenisers/rna/tokenization_rna.py</code> Python<pre><code>class RnaTokenizer(Tokenizer):\n    \"\"\"\n    Constructs an RNA tokenizer.\n\n    Args:\n        alphabet (List[str] | None, optional): List of tokens to use.\n            Defaults to [IUPAC nucleotide code](https://www.bioinformatics.org/sms2/iupac.html).\n        nmers (int, optional): Size of nmers to tokenize.\n            Defaults to 1.\n        codon (bool, optional): Whether to tokenize into codons.\n            Defaults to False.\n        replace_T_with_U (bool, optional): Whether to replace T with U.\n            Defaults to True.\n        do_upper_case (bool, optional): Whether to convert input to uppercase.\n            Defaults to True.\n\n    Examples:\n        &gt;&gt;&gt; from multimolecule import RnaTokenizer\n        &gt;&gt;&gt; tokenizer = RnaTokenizer()\n        &gt;&gt;&gt; tokenizer('&lt;pad&gt;&lt;cls&gt;&lt;eos&gt;&lt;unk&gt;&lt;mask&gt;&lt;null&gt;ACGUNXVHDBMRWSYK.*-')[\"input_ids\"]\n        [1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 2]\n        &gt;&gt;&gt; tokenizer('acgu')[\"input_ids\"]\n        [1, 6, 7, 8, 9, 2]\n        &gt;&gt;&gt; tokenizer('acgt')[\"input_ids\"]\n        [1, 6, 7, 8, 9, 2]\n        &gt;&gt;&gt; tokenizer = RnaTokenizer(replace_T_with_U=False)\n        &gt;&gt;&gt; tokenizer('acgt')[\"input_ids\"]\n        [1, 6, 7, 8, 3, 2]\n        &gt;&gt;&gt; tokenizer = RnaTokenizer(nmers=3)\n        &gt;&gt;&gt; tokenizer('uagcuuauc')[\"input_ids\"]\n        [1, 83, 17, 64, 49, 96, 84, 22, 2]\n        &gt;&gt;&gt; tokenizer = RnaTokenizer(codon=True)\n        &gt;&gt;&gt; tokenizer('uagcuuauc')[\"input_ids\"]\n        [1, 83, 49, 22, 2]\n        &gt;&gt;&gt; tokenizer('uagcuuauca')[\"input_ids\"]\n        Traceback (most recent call last):\n        ValueError: length of input sequence  must be a multiple of 3 for codon tokenization, but got 10\n    \"\"\"\n\n    model_input_names = [\"input_ids\", \"attention_mask\"]\n\n    def __init__(\n        self,\n        alphabet: List[str] | None = None,\n        nmers: int = 1,\n        codon: bool = False,\n        replace_T_with_U: bool = True,\n        bos_token: str = \"&lt;cls&gt;\",\n        cls_token: str = \"&lt;cls&gt;\",\n        pad_token: str = \"&lt;pad&gt;\",\n        eos_token: str = \"&lt;eos&gt;\",\n        sep_token: str = \"&lt;eos&gt;\",\n        unk_token: str = \"&lt;unk&gt;\",\n        mask_token: str = \"&lt;mask&gt;\",\n        additional_special_tokens: List | Tuple | None = None,\n        do_upper_case: bool = True,\n        **kwargs,\n    ):\n        if codon and nmers &gt; 1:\n            raise ValueError(\"Codon and nmers cannot be used together.\")\n        if codon:\n            nmers = 3  # set to 3 to get correct vocab\n        super().__init__(\n            alphabet=get_vocab_list(alphabet, nmers),\n            bos_token=bos_token,\n            cls_token=cls_token,\n            pad_token=pad_token,\n            eos_token=eos_token,\n            sep_token=sep_token,\n            unk_token=unk_token,\n            mask_token=mask_token,\n            additional_special_tokens=additional_special_tokens,\n            do_upper_case=do_upper_case,\n            **kwargs,\n        )\n        self.replace_T_with_U = replace_T_with_U\n        self.nmers = nmers\n        self.condon = codon\n\n    def _tokenize(self, text: str, **kwargs):\n        if self.do_upper_case:\n            text = text.upper()\n        if self.replace_T_with_U:\n            text = text.replace(\"T\", \"U\")\n        if self.condon:\n            if len(text) % 3 != 0:\n                raise ValueError(\n                    f\"length of input sequence  must be a multiple of 3 for codon tokenization, but got {len(text)}\"\n                )\n            return [text[i : i + 3] for i in range(0, len(text), 3)]\n        if self.nmers &gt; 1:\n            return [text[i : i + self.nmers] for i in range(len(text) - self.nmers + 1)]  # noqa: E203\n        return list(text)\n</code></pre>"},{"location":"models/splicebert/#multimolecule.models.splicebert.SpliceBertConfig","title":"<code>SpliceBertConfig</code>","text":"<p>               Bases: <code>PreTrainedConfig</code></p> <p>This is the configuration class to store the configuration of a [<code>SpliceBertModel</code>]. It is used to instantiate a SpliceBert model according to the specified arguments, defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of the SpliceBert biomed-AI/SpliceBERT architecture.</p> <p>Configuration objects inherit from [<code>PreTrainedConfig</code>] and can be used to control the model outputs. Read the documentation from [<code>PreTrainedConfig</code>] for more information.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_size</code> <code>`int`, *optional*, defaults to 25</code> <p>Vocabulary size of the SpliceBert model. Defines the number of different tokens that can be represented by the <code>inputs_ids</code> passed when calling [<code>SpliceBertModel</code>].</p> <code>25</code> <code>hidden_size</code> <code>`int`, *optional*, defaults to 512</code> <p>Dimensionality of the encoder layers and the pooler layer.</p> <code>512</code> <code>num_hidden_layers</code> <code>`int`, *optional*, defaults to 6</code> <p>Number of hidden layers in the Transformer encoder.</p> <code>6</code> <code>num_attention_heads</code> <code>`int`, *optional*, defaults to 16</code> <p>Number of attention heads for each attention layer in the Transformer encoder.</p> <code>16</code> <code>intermediate_size</code> <code>`int`, *optional*, defaults to 2048</code> <p>Dimensionality of the \u201cintermediate\u201d (often named feed-forward) layer in the Transformer encoder.</p> <code>2048</code> <code>hidden_dropout</code> <code>`float`, *optional*, defaults to 0.1</code> <p>The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.</p> <code>0.1</code> <code>attention_dropout</code> <code>`float`, *optional*, defaults to 0.1</code> <p>The dropout ratio for the attention probabilities.</p> <code>0.1</code> <code>max_position_embeddings</code> <code>`int`, *optional*, defaults to 1026</code> <p>The maximum sequence length that this model might ever be used with. Typically set this to something large just in case (e.g., 512 or 1024 or 2048).</p> <code>1026</code> <code>initializer_range</code> <code>`float`, *optional*, defaults to 0.02</code> <p>The standard deviation of the truncated_normal_initializer for initializing all weight matrices.</p> <code>0.02</code> <code>layer_norm_eps</code> <code>`float`, *optional*, defaults to 1e-12</code> <p>The epsilon used by the layer normalization layers.</p> <code>1e-12</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import SpliceBertModel, SpliceBertConfig\n</code></pre> Python Console Session<pre><code>&gt;&gt;&gt; # Initializing a SpliceBERT multimolecule/splicebert style configuration\n&gt;&gt;&gt; configuration = SpliceBertConfig()\n</code></pre> Python Console Session<pre><code>&gt;&gt;&gt; # Initializing a model (with random weights) from the multimolecule/splicebert style configuration\n&gt;&gt;&gt; model = SpliceBertModel(configuration)\n</code></pre> Python Console Session<pre><code>&gt;&gt;&gt; # Accessing the model configuration\n&gt;&gt;&gt; configuration = model.config\n</code></pre> Source code in <code>multimolecule/models/splicebert/configuration_splicebert.py</code> Python<pre><code>class SpliceBertConfig(PreTrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`SpliceBertModel`]. It is used to instantiate a\n    SpliceBert model according to the specified arguments, defining the model architecture. Instantiating a\n    configuration with the defaults will yield a similar configuration to that of the SpliceBert\n    [biomed-AI/SpliceBERT](https://github.com/biomed-AI/SpliceBERT) architecture.\n\n    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n    documentation from [`PreTrainedConfig`] for more information.\n\n\n    Args:\n        vocab_size (`int`, *optional*, defaults to 25):\n            Vocabulary size of the SpliceBert model. Defines the number of different tokens that can be represented by\n            the `inputs_ids` passed when calling [`SpliceBertModel`].\n        hidden_size (`int`, *optional*, defaults to 512):\n            Dimensionality of the encoder layers and the pooler layer.\n        num_hidden_layers (`int`, *optional*, defaults to 6):\n            Number of hidden layers in the Transformer encoder.\n        num_attention_heads (`int`, *optional*, defaults to 16):\n            Number of attention heads for each attention layer in the Transformer encoder.\n        intermediate_size (`int`, *optional*, defaults to 2048):\n            Dimensionality of the \"intermediate\" (often named feed-forward) layer in the Transformer encoder.\n        hidden_dropout (`float`, *optional*, defaults to 0.1):\n            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n        attention_dropout (`float`, *optional*, defaults to 0.1):\n            The dropout ratio for the attention probabilities.\n        max_position_embeddings (`int`, *optional*, defaults to 1026):\n            The maximum sequence length that this model might ever be used with. Typically set this to something large\n            just in case (e.g., 512 or 1024 or 2048).\n        initializer_range (`float`, *optional*, defaults to 0.02):\n            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n        layer_norm_eps (`float`, *optional*, defaults to 1e-12):\n            The epsilon used by the layer normalization layers.\n\n    Examples:\n        &gt;&gt;&gt; from multimolecule import SpliceBertModel, SpliceBertConfig\n\n        &gt;&gt;&gt; # Initializing a SpliceBERT multimolecule/splicebert style configuration\n        &gt;&gt;&gt; configuration = SpliceBertConfig()\n\n        &gt;&gt;&gt; # Initializing a model (with random weights) from the multimolecule/splicebert style configuration\n        &gt;&gt;&gt; model = SpliceBertModel(configuration)\n\n        &gt;&gt;&gt; # Accessing the model configuration\n        &gt;&gt;&gt; configuration = model.config\n    \"\"\"\n\n    model_type = \"splicebert\"\n\n    def __init__(\n        self,\n        vocab_size=25,\n        hidden_size=512,\n        num_hidden_layers=6,\n        num_attention_heads=16,\n        intermediate_size=2048,\n        hidden_act=\"gelu\",\n        hidden_dropout=0.1,\n        attention_dropout=0.1,\n        max_position_embeddings=1026,\n        initializer_range=0.02,\n        layer_norm_eps=1e-12,\n        position_embedding_type=\"absolute\",\n        use_cache=True,\n        head=None,\n        lm_head=None,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n\n        self.vocab_size = vocab_size\n        self.type_vocab_size = 2\n        self.hidden_size = hidden_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.intermediate_size = intermediate_size\n        self.hidden_act = hidden_act\n        self.hidden_dropout = hidden_dropout\n        self.attention_dropout = attention_dropout\n        self.max_position_embeddings = max_position_embeddings\n        self.initializer_range = initializer_range\n        self.layer_norm_eps = layer_norm_eps\n        self.position_embedding_type = position_embedding_type\n        self.use_cache = use_cache\n        self.head = HeadConfig(**head if head is not None else {})\n        self.lm_head = MaskedLMHeadConfig(**lm_head if lm_head is not None else {})\n</code></pre>"},{"location":"models/splicebert/#multimolecule.models.splicebert.SpliceBertForMaskedLM","title":"<code>SpliceBertForMaskedLM</code>","text":"<p>               Bases: <code>SpliceBertPreTrainedModel</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import SpliceBertConfig, SpliceBertForMaskedLM, RnaTokenizer\n&gt;&gt;&gt; config = SpliceBertConfig()\n&gt;&gt;&gt; model = SpliceBertForMaskedLM(config)\n&gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n&gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n&gt;&gt;&gt; output = model(**input)\n</code></pre> Source code in <code>multimolecule/models/splicebert/modeling_splicebert.py</code> Python<pre><code>class SpliceBertForMaskedLM(SpliceBertPreTrainedModel):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; from multimolecule import SpliceBertConfig, SpliceBertForMaskedLM, RnaTokenizer\n        &gt;&gt;&gt; config = SpliceBertConfig()\n        &gt;&gt;&gt; model = SpliceBertForMaskedLM(config)\n        &gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n        &gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n        &gt;&gt;&gt; output = model(**input)\n    \"\"\"\n\n    _tied_weights_keys = [\"lm_head.decoder.bias\", \"lm_head.decoder.weight\"]\n\n    def __init__(self, config: SpliceBertConfig):\n        super().__init__(config)\n        if config.is_decoder:\n            logger.warning(\n                \"If you want to use `SpliceBertForMaskedLM` make sure `config.is_decoder=False` for \"\n                \"bi-directional self-attention.\"\n            )\n        self.splicebert = SpliceBertModel(config, add_pooling_layer=False)\n        self.lm_head = MaskedLMHead(config)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_output_embeddings(self):\n        return self.lm_head.decoder\n\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head.decoder = new_embeddings\n\n    def forward(\n        self,\n        input_ids: Tensor | NestedTensor,\n        attention_mask: Tensor | None = None,\n        position_ids: Tensor | None = None,\n        head_mask: Tensor | None = None,\n        inputs_embeds: Tensor | NestedTensor | None = None,\n        encoder_hidden_states: Tensor | None = None,\n        encoder_attention_mask: Tensor | None = None,\n        labels: Tensor | None = None,\n        output_attentions: bool | None = None,\n        output_hidden_states: bool | None = None,\n        return_dict: bool | None = None,\n    ) -&gt; Tuple[Tensor, ...] | MaskedLMOutput:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n            config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n            loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n        \"\"\"\n\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.splicebert(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        output = self.lm_head(outputs, labels)\n        logits, loss = output.logits, output.loss\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return MaskedLMOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"models/splicebert/#multimolecule.models.splicebert.SpliceBertForMaskedLM.forward","title":"<code>forward(input_ids, attention_mask=None, position_ids=None, head_mask=None, inputs_embeds=None, encoder_hidden_states=None, encoder_attention_mask=None, labels=None, output_attentions=None, output_hidden_states=None, return_dict=None)</code>","text":"<p>labels (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, optional):     Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ...,     config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the     loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code></p> Source code in <code>multimolecule/models/splicebert/modeling_splicebert.py</code> Python<pre><code>def forward(\n    self,\n    input_ids: Tensor | NestedTensor,\n    attention_mask: Tensor | None = None,\n    position_ids: Tensor | None = None,\n    head_mask: Tensor | None = None,\n    inputs_embeds: Tensor | NestedTensor | None = None,\n    encoder_hidden_states: Tensor | None = None,\n    encoder_attention_mask: Tensor | None = None,\n    labels: Tensor | None = None,\n    output_attentions: bool | None = None,\n    output_hidden_states: bool | None = None,\n    return_dict: bool | None = None,\n) -&gt; Tuple[Tensor, ...] | MaskedLMOutput:\n    r\"\"\"\n    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n        Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n        config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n        loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n    \"\"\"\n\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n    outputs = self.splicebert(\n        input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        head_mask=head_mask,\n        inputs_embeds=inputs_embeds,\n        encoder_hidden_states=encoder_hidden_states,\n        encoder_attention_mask=encoder_attention_mask,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n    output = self.lm_head(outputs, labels)\n    logits, loss = output.logits, output.loss\n\n    if not return_dict:\n        output = (logits,) + outputs[2:]\n        return ((loss,) + output) if loss is not None else output\n\n    return MaskedLMOutput(\n        loss=loss,\n        logits=logits,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"models/splicebert/#multimolecule.models.splicebert.SpliceBertForNucleotideClassification","title":"<code>SpliceBertForNucleotideClassification</code>","text":"<p>               Bases: <code>SpliceBertPreTrainedModel</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import SpliceBertConfig, SpliceBertForNucleotideClassification, RnaTokenizer\n&gt;&gt;&gt; config = SpliceBertConfig()\n&gt;&gt;&gt; model = SpliceBertForNucleotideClassification(config)\n&gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n&gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n&gt;&gt;&gt; output = model(**input)\n</code></pre> Source code in <code>multimolecule/models/splicebert/modeling_splicebert.py</code> Python<pre><code>class SpliceBertForNucleotideClassification(SpliceBertPreTrainedModel):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; from multimolecule import SpliceBertConfig, SpliceBertForNucleotideClassification, RnaTokenizer\n        &gt;&gt;&gt; config = SpliceBertConfig()\n        &gt;&gt;&gt; model = SpliceBertForNucleotideClassification(config)\n        &gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n        &gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n        &gt;&gt;&gt; output = model(**input)\n    \"\"\"\n\n    def __init__(self, config: SpliceBertConfig):\n        super().__init__(config)\n        self.num_labels = config.head.num_labels\n        self.splicebert = SpliceBertModel(config, add_pooling_layer=False)\n        self.nucleotide_head = NucleotideClassificationHead(config)\n        self.head_config = self.nucleotide_head.config\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(\n        self,\n        input_ids: Tensor | NestedTensor,\n        attention_mask: Tensor | None = None,\n        position_ids: Tensor | None = None,\n        head_mask: Tensor | None = None,\n        inputs_embeds: Tensor | NestedTensor | None = None,\n        labels: Tensor | None = None,\n        output_attentions: bool | None = None,\n        output_hidden_states: bool | None = None,\n        return_dict: bool | None = None,\n        **kwargs,\n    ) -&gt; Tuple[Tensor, ...] | TokenClassifierOutput:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n            `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        if kwargs:\n            warn(\n                f\"Additional keyword arguments `{', '.join(kwargs)}` are detected in \"\n                f\"`{self.__class__.__name__}.forward`, they will be ignored.\\n\"\n                \"This is provided for backward compatibility and may lead to unexpected behavior.\"\n            )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.splicebert(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        output = self.nucleotide_head(outputs, attention_mask, input_ids, labels)\n        logits, loss = output.logits, output.loss\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return TokenClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"models/splicebert/#multimolecule.models.splicebert.SpliceBertForNucleotideClassification.forward","title":"<code>forward(input_ids, attention_mask=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None, output_attentions=None, output_hidden_states=None, return_dict=None, **kwargs)</code>","text":"<p>labels (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, optional):     Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ...,     config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If     <code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).</p> Source code in <code>multimolecule/models/splicebert/modeling_splicebert.py</code> Python<pre><code>def forward(\n    self,\n    input_ids: Tensor | NestedTensor,\n    attention_mask: Tensor | None = None,\n    position_ids: Tensor | None = None,\n    head_mask: Tensor | None = None,\n    inputs_embeds: Tensor | NestedTensor | None = None,\n    labels: Tensor | None = None,\n    output_attentions: bool | None = None,\n    output_hidden_states: bool | None = None,\n    return_dict: bool | None = None,\n    **kwargs,\n) -&gt; Tuple[Tensor, ...] | TokenClassifierOutput:\n    r\"\"\"\n    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n        `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).\n    \"\"\"\n    if kwargs:\n        warn(\n            f\"Additional keyword arguments `{', '.join(kwargs)}` are detected in \"\n            f\"`{self.__class__.__name__}.forward`, they will be ignored.\\n\"\n            \"This is provided for backward compatibility and may lead to unexpected behavior.\"\n        )\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n    outputs = self.splicebert(\n        input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        head_mask=head_mask,\n        inputs_embeds=inputs_embeds,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n    output = self.nucleotide_head(outputs, attention_mask, input_ids, labels)\n    logits, loss = output.logits, output.loss\n\n    if not return_dict:\n        output = (logits,) + outputs[2:]\n        return ((loss,) + output) if loss is not None else output\n\n    return TokenClassifierOutput(\n        loss=loss,\n        logits=logits,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"models/splicebert/#multimolecule.models.splicebert.SpliceBertForPretraining","title":"<code>SpliceBertForPretraining</code>","text":"<p>               Bases: <code>SpliceBertPreTrainedModel</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import SpliceBertConfig, SpliceBertForPretraining, RnaTokenizer\n&gt;&gt;&gt; config = SpliceBertConfig()\n&gt;&gt;&gt; model = SpliceBertForPretraining(config)\n&gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n&gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n&gt;&gt;&gt; output = model(**input)\n</code></pre> Source code in <code>multimolecule/models/splicebert/modeling_splicebert.py</code> Python<pre><code>class SpliceBertForPretraining(SpliceBertPreTrainedModel):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; from multimolecule import SpliceBertConfig, SpliceBertForPretraining, RnaTokenizer\n        &gt;&gt;&gt; config = SpliceBertConfig()\n        &gt;&gt;&gt; model = SpliceBertForPretraining(config)\n        &gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n        &gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n        &gt;&gt;&gt; output = model(**input)\n    \"\"\"\n\n    _tied_weights_keys = [\"pretrain_head.decoder.bias\", \"pretrain_head.decoder.weight\"]\n\n    def __init__(self, config: SpliceBertConfig):\n        super().__init__(config)\n        if config.is_decoder:\n            logger.warning(\n                \"If you want to use `SpliceBertForPretraining` make sure `config.is_decoder=False` for \"\n                \"bi-directional self-attention.\"\n            )\n        self.splicebert = SpliceBertModel(config, add_pooling_layer=False)\n        self.lm_head = MaskedLMHead(config)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(\n        self,\n        input_ids: Tensor | NestedTensor,\n        attention_mask: Tensor | None = None,\n        position_ids: Tensor | None = None,\n        head_mask: Tensor | None = None,\n        inputs_embeds: Tensor | NestedTensor | None = None,\n        encoder_hidden_states: Tensor | None = None,\n        encoder_attention_mask: Tensor | None = None,\n        labels: Tensor | None = None,\n        output_attentions: bool | None = None,\n        output_hidden_states: bool | None = None,\n        return_dict: bool | None = None,\n    ) -&gt; Tuple[Tensor, ...] | MaskedLMOutput:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n            config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n            loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n        \"\"\"\n\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.splicebert(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        output = self.lm_head(outputs, labels)\n        logits, loss = output.logits, output.loss\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return MaskedLMOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"models/splicebert/#multimolecule.models.splicebert.SpliceBertForPretraining.forward","title":"<code>forward(input_ids, attention_mask=None, position_ids=None, head_mask=None, inputs_embeds=None, encoder_hidden_states=None, encoder_attention_mask=None, labels=None, output_attentions=None, output_hidden_states=None, return_dict=None)</code>","text":"<p>labels (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, optional):     Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ...,     config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the     loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code></p> Source code in <code>multimolecule/models/splicebert/modeling_splicebert.py</code> Python<pre><code>def forward(\n    self,\n    input_ids: Tensor | NestedTensor,\n    attention_mask: Tensor | None = None,\n    position_ids: Tensor | None = None,\n    head_mask: Tensor | None = None,\n    inputs_embeds: Tensor | NestedTensor | None = None,\n    encoder_hidden_states: Tensor | None = None,\n    encoder_attention_mask: Tensor | None = None,\n    labels: Tensor | None = None,\n    output_attentions: bool | None = None,\n    output_hidden_states: bool | None = None,\n    return_dict: bool | None = None,\n) -&gt; Tuple[Tensor, ...] | MaskedLMOutput:\n    r\"\"\"\n    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n        Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n        config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n        loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n    \"\"\"\n\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n    outputs = self.splicebert(\n        input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        head_mask=head_mask,\n        inputs_embeds=inputs_embeds,\n        encoder_hidden_states=encoder_hidden_states,\n        encoder_attention_mask=encoder_attention_mask,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n    output = self.lm_head(outputs, labels)\n    logits, loss = output.logits, output.loss\n\n    if not return_dict:\n        output = (logits,) + outputs[2:]\n        return ((loss,) + output) if loss is not None else output\n\n    return MaskedLMOutput(\n        loss=loss,\n        logits=logits,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"models/splicebert/#multimolecule.models.splicebert.SpliceBertForSequenceClassification","title":"<code>SpliceBertForSequenceClassification</code>","text":"<p>               Bases: <code>SpliceBertPreTrainedModel</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import SpliceBertConfig, SpliceBertForSequenceClassification, RnaTokenizer\n&gt;&gt;&gt; config = SpliceBertConfig()\n&gt;&gt;&gt; model = SpliceBertForSequenceClassification(config)\n&gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n&gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n&gt;&gt;&gt; output = model(**input)\n</code></pre> Source code in <code>multimolecule/models/splicebert/modeling_splicebert.py</code> Python<pre><code>class SpliceBertForSequenceClassification(SpliceBertPreTrainedModel):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; from multimolecule import SpliceBertConfig, SpliceBertForSequenceClassification, RnaTokenizer\n        &gt;&gt;&gt; config = SpliceBertConfig()\n        &gt;&gt;&gt; model = SpliceBertForSequenceClassification(config)\n        &gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n        &gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n        &gt;&gt;&gt; output = model(**input)\n    \"\"\"\n\n    def __init__(self, config: SpliceBertConfig):\n        super().__init__(config)\n        self.num_labels = config.head.num_labels\n        self.splicebert = SpliceBertModel(config, add_pooling_layer=True)\n        self.sequence_head = SequenceClassificationHead(config)\n        self.head_config = self.sequence_head.config\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(\n        self,\n        input_ids: Tensor | NestedTensor,\n        attention_mask: Tensor | None = None,\n        position_ids: Tensor | None = None,\n        head_mask: Tensor | None = None,\n        inputs_embeds: Tensor | NestedTensor | None = None,\n        labels: Tensor | None = None,\n        output_attentions: bool | None = None,\n        output_hidden_states: bool | None = None,\n        return_dict: bool | None = None,\n    ) -&gt; Tuple[Tensor, ...] | SequenceClassifierOutput:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n            `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.splicebert(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        output = self.sequence_head(outputs, labels)\n        logits, loss = output.logits, output.loss\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"models/splicebert/#multimolecule.models.splicebert.SpliceBertForSequenceClassification.forward","title":"<code>forward(input_ids, attention_mask=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None, output_attentions=None, output_hidden_states=None, return_dict=None)</code>","text":"<p>labels (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, optional):     Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ...,     config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If     <code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).</p> Source code in <code>multimolecule/models/splicebert/modeling_splicebert.py</code> Python<pre><code>def forward(\n    self,\n    input_ids: Tensor | NestedTensor,\n    attention_mask: Tensor | None = None,\n    position_ids: Tensor | None = None,\n    head_mask: Tensor | None = None,\n    inputs_embeds: Tensor | NestedTensor | None = None,\n    labels: Tensor | None = None,\n    output_attentions: bool | None = None,\n    output_hidden_states: bool | None = None,\n    return_dict: bool | None = None,\n) -&gt; Tuple[Tensor, ...] | SequenceClassifierOutput:\n    r\"\"\"\n    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n        `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).\n    \"\"\"\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n    outputs = self.splicebert(\n        input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        head_mask=head_mask,\n        inputs_embeds=inputs_embeds,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n    output = self.sequence_head(outputs, labels)\n    logits, loss = output.logits, output.loss\n\n    if not return_dict:\n        output = (logits,) + outputs[2:]\n        return ((loss,) + output) if loss is not None else output\n\n    return SequenceClassifierOutput(\n        loss=loss,\n        logits=logits,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"models/splicebert/#multimolecule.models.splicebert.SpliceBertForTokenClassification","title":"<code>SpliceBertForTokenClassification</code>","text":"<p>               Bases: <code>SpliceBertPreTrainedModel</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import SpliceBertConfig, SpliceBertForTokenClassification, RnaTokenizer\n&gt;&gt;&gt; config = SpliceBertConfig()\n&gt;&gt;&gt; model = SpliceBertForTokenClassification(config)\n&gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n&gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n&gt;&gt;&gt; output = model(**input)\n</code></pre> Source code in <code>multimolecule/models/splicebert/modeling_splicebert.py</code> Python<pre><code>class SpliceBertForTokenClassification(SpliceBertPreTrainedModel):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; from multimolecule import SpliceBertConfig, SpliceBertForTokenClassification, RnaTokenizer\n        &gt;&gt;&gt; config = SpliceBertConfig()\n        &gt;&gt;&gt; model = SpliceBertForTokenClassification(config)\n        &gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n        &gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n        &gt;&gt;&gt; output = model(**input)\n    \"\"\"\n\n    def __init__(self, config: SpliceBertConfig):\n        super().__init__(config)\n        self.num_labels = config.head.num_labels\n        self.splicebert = SpliceBertModel(config, add_pooling_layer=False)\n        self.token_head = TokenClassificationHead(config)\n        self.head_config = self.token_head.config\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(\n        self,\n        input_ids: Tensor | NestedTensor,\n        attention_mask: Tensor | None = None,\n        position_ids: Tensor | None = None,\n        head_mask: Tensor | None = None,\n        inputs_embeds: Tensor | NestedTensor | None = None,\n        labels: Tensor | None = None,\n        output_attentions: bool | None = None,\n        output_hidden_states: bool | None = None,\n        return_dict: bool | None = None,\n    ) -&gt; Tuple[Tensor, ...] | TokenClassifierOutput:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.splicebert(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        output = self.token_head(outputs, attention_mask, input_ids, labels)\n        logits, loss = output.logits, output.loss\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return TokenClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"models/splicebert/#multimolecule.models.splicebert.SpliceBertForTokenClassification.forward","title":"<code>forward(input_ids, attention_mask=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None, output_attentions=None, output_hidden_states=None, return_dict=None)</code>","text":"<p>labels (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, optional):     Labels for computing the token classification loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>.</p> Source code in <code>multimolecule/models/splicebert/modeling_splicebert.py</code> Python<pre><code>def forward(\n    self,\n    input_ids: Tensor | NestedTensor,\n    attention_mask: Tensor | None = None,\n    position_ids: Tensor | None = None,\n    head_mask: Tensor | None = None,\n    inputs_embeds: Tensor | NestedTensor | None = None,\n    labels: Tensor | None = None,\n    output_attentions: bool | None = None,\n    output_hidden_states: bool | None = None,\n    return_dict: bool | None = None,\n) -&gt; Tuple[Tensor, ...] | TokenClassifierOutput:\n    r\"\"\"\n    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n        Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n    \"\"\"\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n    outputs = self.splicebert(\n        input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        head_mask=head_mask,\n        inputs_embeds=inputs_embeds,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n    output = self.token_head(outputs, attention_mask, input_ids, labels)\n    logits, loss = output.logits, output.loss\n\n    if not return_dict:\n        output = (logits,) + outputs[2:]\n        return ((loss,) + output) if loss is not None else output\n\n    return TokenClassifierOutput(\n        loss=loss,\n        logits=logits,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"models/splicebert/#multimolecule.models.splicebert.SpliceBertModel","title":"<code>SpliceBertModel</code>","text":"<p>               Bases: <code>SpliceBertPreTrainedModel</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import SpliceBertConfig, SpliceBertModel, RnaTokenizer\n&gt;&gt;&gt; config = SpliceBertConfig()\n&gt;&gt;&gt; model = SpliceBertModel(config)\n&gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n&gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n&gt;&gt;&gt; output = model(**input)\n</code></pre> Source code in <code>multimolecule/models/splicebert/modeling_splicebert.py</code> Python<pre><code>class SpliceBertModel(SpliceBertPreTrainedModel):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; from multimolecule import SpliceBertConfig, SpliceBertModel, RnaTokenizer\n        &gt;&gt;&gt; config = SpliceBertConfig()\n        &gt;&gt;&gt; model = SpliceBertModel(config)\n        &gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n        &gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n        &gt;&gt;&gt; output = model(**input)\n    \"\"\"\n\n    def __init__(self, config: SpliceBertConfig, add_pooling_layer: bool = True):\n        super().__init__(config)\n        self.pad_token_id = config.pad_token_id\n        self.embeddings = SpliceBertEmbeddings(config)\n        self.encoder = SpliceBertEncoder(config)\n        self.pooler = SpliceBertPooler(config) if add_pooling_layer else None\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.embeddings.word_embeddings\n\n    def set_input_embeddings(self, value):\n        self.embeddings.word_embeddings = value\n\n    def _prune_heads(self, heads_to_prune):\n        \"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"\n        for layer, heads in heads_to_prune.items():\n            self.encoder.layer[layer].attention.prune_heads(heads)\n\n    def forward(\n        self,\n        input_ids: Tensor | NestedTensor,\n        attention_mask: Tensor | None = None,\n        position_ids: Tensor | None = None,\n        head_mask: Tensor | None = None,\n        inputs_embeds: Tensor | NestedTensor | None = None,\n        encoder_hidden_states: Tensor | None = None,\n        encoder_attention_mask: Tensor | None = None,\n        past_key_values: Tuple[Tuple[torch.FloatTensor, torch.FloatTensor], ...] | None = None,\n        use_cache: bool | None = None,\n        output_attentions: bool | None = None,\n        output_hidden_states: bool | None = None,\n        return_dict: bool | None = None,\n    ) -&gt; Tuple[Tensor, ...] | BaseModelOutputWithPoolingAndCrossAttentions:\n        r\"\"\"\n        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n            the model is configured as a decoder.\n        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors\n            of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n\n            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n            `past_key_values`).\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if self.config.is_decoder:\n            use_cache = use_cache if use_cache is not None else self.config.use_cache\n        else:\n            use_cache = False\n\n        if isinstance(input_ids, NestedTensor):\n            input_ids, attention_mask = input_ids.tensor, input_ids.mask\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n        if input_ids is not None:\n            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n            input_shape = input_ids.size()\n        elif inputs_embeds is not None:\n            input_shape = inputs_embeds.size()[:-1]\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n        batch_size, seq_length = input_shape\n        device = input_ids.device if input_ids is not None else inputs_embeds.device  # type: ignore[union-attr]\n\n        # past_key_values_length\n        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n\n        if attention_mask is None:\n            attention_mask = (\n                input_ids.ne(self.pad_token_id)\n                if self.pad_token_id is not None\n                else torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n            )\n\n        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n        # ourselves in which case we just need to make it broadcastable to all heads.\n        extended_attention_mask: Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n\n        # If a 2D or 3D attention mask is provided for the cross-attention\n        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n        if self.config.is_decoder and encoder_hidden_states is not None:\n            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n            if encoder_attention_mask is None:\n                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n        else:\n            encoder_extended_attention_mask = None\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n\n        embedding_output = self.embeddings(\n            input_ids=input_ids,\n            position_ids=position_ids,\n            inputs_embeds=inputs_embeds,\n            past_key_values_length=past_key_values_length,\n        )\n        encoder_outputs = self.encoder(\n            embedding_output,\n            attention_mask=extended_attention_mask,\n            head_mask=head_mask,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_extended_attention_mask,\n            past_key_values=past_key_values,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        sequence_output = encoder_outputs[0]\n        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n\n        if not return_dict:\n            return (sequence_output, pooled_output) + encoder_outputs[1:]\n\n        return BaseModelOutputWithPoolingAndCrossAttentions(\n            last_hidden_state=sequence_output,\n            pooler_output=pooled_output,\n            past_key_values=encoder_outputs.past_key_values,\n            hidden_states=encoder_outputs.hidden_states,\n            attentions=encoder_outputs.attentions,\n            cross_attentions=encoder_outputs.cross_attentions,\n        )\n</code></pre>"},{"location":"models/splicebert/#multimolecule.models.splicebert.SpliceBertModel.forward","title":"<code>forward(input_ids, attention_mask=None, position_ids=None, head_mask=None, inputs_embeds=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_values=None, use_cache=None, output_attentions=None, output_hidden_states=None, return_dict=None)</code>","text":"<p>encoder_hidden_states  (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, optional):     Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if     the model is configured as a decoder. encoder_attention_mask (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, optional):     Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in     the cross-attention if the model is configured as a decoder. Mask values selected in <code>[0, 1]</code>:</p> Text Only<pre><code>- 1 for tokens that are **not masked**,\n- 0 for tokens that are **masked**.\n</code></pre> <p>past_key_values (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors     of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>):     Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.</p> Text Only<pre><code>If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\ndon't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n`decoder_input_ids` of shape `(batch_size, sequence_length)`.\n</code></pre> <p>use_cache (<code>bool</code>, optional):     If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see     <code>past_key_values</code>).</p> Source code in <code>multimolecule/models/splicebert/modeling_splicebert.py</code> Python<pre><code>def forward(\n    self,\n    input_ids: Tensor | NestedTensor,\n    attention_mask: Tensor | None = None,\n    position_ids: Tensor | None = None,\n    head_mask: Tensor | None = None,\n    inputs_embeds: Tensor | NestedTensor | None = None,\n    encoder_hidden_states: Tensor | None = None,\n    encoder_attention_mask: Tensor | None = None,\n    past_key_values: Tuple[Tuple[torch.FloatTensor, torch.FloatTensor], ...] | None = None,\n    use_cache: bool | None = None,\n    output_attentions: bool | None = None,\n    output_hidden_states: bool | None = None,\n    return_dict: bool | None = None,\n) -&gt; Tuple[Tensor, ...] | BaseModelOutputWithPoolingAndCrossAttentions:\n    r\"\"\"\n    encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n        Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n        the model is configured as a decoder.\n    encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n        Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n        the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n\n        - 1 for tokens that are **not masked**,\n        - 0 for tokens that are **masked**.\n    past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors\n        of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n        Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n\n        If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n        don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n        `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n    use_cache (`bool`, *optional*):\n        If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n        `past_key_values`).\n    \"\"\"\n    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n    output_hidden_states = (\n        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n    )\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n    if self.config.is_decoder:\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n    else:\n        use_cache = False\n\n    if isinstance(input_ids, NestedTensor):\n        input_ids, attention_mask = input_ids.tensor, input_ids.mask\n    if input_ids is not None and inputs_embeds is not None:\n        raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n    if input_ids is not None:\n        self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n        input_shape = input_ids.size()\n    elif inputs_embeds is not None:\n        input_shape = inputs_embeds.size()[:-1]\n    else:\n        raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n    batch_size, seq_length = input_shape\n    device = input_ids.device if input_ids is not None else inputs_embeds.device  # type: ignore[union-attr]\n\n    # past_key_values_length\n    past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n\n    if attention_mask is None:\n        attention_mask = (\n            input_ids.ne(self.pad_token_id)\n            if self.pad_token_id is not None\n            else torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n        )\n\n    # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n    # ourselves in which case we just need to make it broadcastable to all heads.\n    extended_attention_mask: Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n\n    # If a 2D or 3D attention mask is provided for the cross-attention\n    # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n    if self.config.is_decoder and encoder_hidden_states is not None:\n        encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n        encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n        if encoder_attention_mask is None:\n            encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n        encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n    else:\n        encoder_extended_attention_mask = None\n\n    # Prepare head mask if needed\n    # 1.0 in head_mask indicate we keep the head\n    # attention_probs has shape bsz x n_heads x N x N\n    # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n    # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n    head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n\n    embedding_output = self.embeddings(\n        input_ids=input_ids,\n        position_ids=position_ids,\n        inputs_embeds=inputs_embeds,\n        past_key_values_length=past_key_values_length,\n    )\n    encoder_outputs = self.encoder(\n        embedding_output,\n        attention_mask=extended_attention_mask,\n        head_mask=head_mask,\n        encoder_hidden_states=encoder_hidden_states,\n        encoder_attention_mask=encoder_extended_attention_mask,\n        past_key_values=past_key_values,\n        use_cache=use_cache,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n    sequence_output = encoder_outputs[0]\n    pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n\n    if not return_dict:\n        return (sequence_output, pooled_output) + encoder_outputs[1:]\n\n    return BaseModelOutputWithPoolingAndCrossAttentions(\n        last_hidden_state=sequence_output,\n        pooler_output=pooled_output,\n        past_key_values=encoder_outputs.past_key_values,\n        hidden_states=encoder_outputs.hidden_states,\n        attentions=encoder_outputs.attentions,\n        cross_attentions=encoder_outputs.cross_attentions,\n    )\n</code></pre>"},{"location":"models/splicebert/#multimolecule.models.splicebert.SpliceBertPreTrainedModel","title":"<code>SpliceBertPreTrainedModel</code>","text":"<p>               Bases: <code>PreTrainedModel</code></p> <p>An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.</p> Source code in <code>multimolecule/models/splicebert/modeling_splicebert.py</code> Python<pre><code>class SpliceBertPreTrainedModel(PreTrainedModel):\n    \"\"\"\n    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n    models.\n    \"\"\"\n\n    config_class = SpliceBertConfig\n    base_model_prefix = \"splicebert\"\n    supports_gradient_checkpointing = True\n    _no_split_modules = [\"SpliceBertLayer\", \"SpliceBertEmbeddings\"]\n\n    # Copied from transformers.models.bert.modeling_bert.BertPreTrainedModel._init_weights\n    def _init_weights(self, module: nn.Module):\n        \"\"\"Initialize the weights\"\"\"\n        if isinstance(module, nn.Linear):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n    def _set_gradient_checkpointing(self, module, value=False):\n        if isinstance(module, SpliceBertEncoder):\n            module.gradient_checkpointing = value\n</code></pre>"},{"location":"models/utrbert/","title":"3UTRBERT","text":"<p>Pre-trained model on 3\u2019 untranslated region (3\u2019UTR) using a masked language modeling (MLM) objective.</p>"},{"location":"models/utrbert/#disclaimer","title":"Disclaimer","text":"<p>This is an UNOFFICIAL implementation of the Deciphering 3\u2019 UTR mediated gene regulation using interpretable deep representation learning by Yuning Yang, Gen Li, et al.</p> <p>The OFFICIAL repository of 3UTRBERT is at yangyn533/3UTRBERT.</p> <p>The team releasing 3UTRBERT did not write this model card for this model so this model card has been written by the MultiMolecule team.</p>"},{"location":"models/utrbert/#model-details","title":"Model Details","text":"<p>3UTRBERT is a bert-style model pre-trained on a large corpus of 3\u2019 untranslated regions (3\u2019UTRs) in a self-supervised fashion. This means that the model was trained on the raw nucleotides of RNA sequences only, with an automatic process to generate inputs and labels from those texts. Please refer to the Training Details section for more information on the training process.</p>"},{"location":"models/utrbert/#variations","title":"Variations","text":"<ul> <li><code>multimolecule/utrbert-3mer</code>: The 3UTRBERT model pre-trained on 3-mer data.</li> <li><code>multimolecule/utrbert-4mer</code>: The 3UTRBERT model pre-trained on 4-mer data.</li> <li><code>multimolecule/utrbert-5mer</code>: The 3UTRBERT model pre-trained on 5-mer data.</li> <li><code>multimolecule/utrbert-6mer</code>: The 3UTRBERT model pre-trained on 6-mer data.</li> </ul>"},{"location":"models/utrbert/#model-specification","title":"Model Specification","text":"Variants Num Layers Hidden Size Num Heads Intermediate Size Num Parameters (M) FLOPs (G) MACs (G) Max Num Tokens UTRBERT-3mer 12 768 12 3072 86.14 22.36 11.17 512 UTRBERT-4mer 86.53 UTRBERT-5mer 88.45 UTRBERT-6mer 98.05"},{"location":"models/utrbert/#links","title":"Links","text":"<ul> <li>Code: multimolecule.utrbert</li> <li>Data: GENCODE</li> <li>Paper: Deciphering 3\u2019 UTR mediated gene regulation using interpretable deep representation learning</li> <li>Developed by: Yuning Yang, Gen Li, Kuan Pang, Wuxinhao Cao, Xiangtao Li, Zhaolei Zhang</li> <li>Model type: BERT - FlashAttention</li> <li>Original Repository: https://github.com/yangyn533/3UTRBERT</li> </ul>"},{"location":"models/utrbert/#usage","title":"Usage","text":"<p>The model file depends on the <code>multimolecule</code> library. You can install it using pip:</p> Bash<pre><code>pip install multimolecule\n</code></pre>"},{"location":"models/utrbert/#direct-use","title":"Direct Use","text":"<p>Note: Default transformers pipeline does not support K-mer tokenization.</p> <p>You can use this model directly with a pipeline for masked language modeling:</p> Python<pre><code>&gt;&gt;&gt; import multimolecule  # you must import multimolecule to register models\n&gt;&gt;&gt; from transformers import pipeline\n&gt;&gt;&gt; unmasker = pipeline('fill-mask', model='multimolecule/utrbert-3mer')\n&gt;&gt;&gt; unmasker(\"uagc&lt;mask&gt;&lt;mask&gt;&lt;mask&gt;ucagacugauguuga\")[1]\n\n[{'score': 0.5108312964439392,\n  'token': 49,\n  'token_str': 'CUU',\n  'sequence': '&lt;cls&gt; UAG AGC &lt;mask&gt; CUU &lt;mask&gt; UCA CAG AGA GAC ACU CUG UGA GAU AUG UGU GUU UUG UGA &lt;eos&gt;'},\n {'score': 0.3299442529678345,\n  'token': 39,\n  'token_str': 'CCU',\n  'sequence': '&lt;cls&gt; UAG AGC &lt;mask&gt; CCU &lt;mask&gt; UCA CAG AGA GAC ACU CUG UGA GAU AUG UGU GUU UUG UGA &lt;eos&gt;'},\n {'score': 0.09744979441165924,\n  'token': 34,\n  'token_str': 'CAU',\n  'sequence': '&lt;cls&gt; UAG AGC &lt;mask&gt; CAU &lt;mask&gt; UCA CAG AGA GAC ACU CUG UGA GAU AUG UGU GUU UUG UGA &lt;eos&gt;'},\n {'score': 0.01074671559035778,\n  'token': 64,\n  'token_str': 'GCU',\n  'sequence': '&lt;cls&gt; UAG AGC &lt;mask&gt; GCU &lt;mask&gt; UCA CAG AGA GAC ACU CUG UGA GAU AUG UGU GUU UUG UGA &lt;eos&gt;'},\n {'score': 0.010300246998667717,\n  'token': 24,\n  'token_str': 'AUU',\n  'sequence': '&lt;cls&gt; UAG AGC &lt;mask&gt; AUU &lt;mask&gt; UCA CAG AGA GAC ACU CUG UGA GAU AUG UGU GUU UUG UGA &lt;eos&gt;'}]\n</code></pre>"},{"location":"models/utrbert/#downstream-use","title":"Downstream Use","text":""},{"location":"models/utrbert/#extract-features","title":"Extract Features","text":"<p>Here is how to use this model to get the features of a given sequence in PyTorch:</p> Python<pre><code>from multimolecule import RnaTokenizer, UtrBertModel\n\n\ntokenizer = RnaTokenizer.from_pretrained('multimolecule/utrbert-3mer')\nmodel = UtrBertModel.from_pretrained('multimolecule/utrbert-3mer')\n\ntext = \"UAGCUUAUCAGACUGAUGUUGA\"\ninput = tokenizer(text, return_tensors='pt')\n\noutput = model(**input)\n</code></pre>"},{"location":"models/utrbert/#sequence-classification-regression","title":"Sequence Classification / Regression","text":"<p>Note: This model is not fine-tuned for any specific task. You will need to fine-tune the model on a downstream task to use it for sequence classification or regression.</p> <p>Here is how to use this model as backbone to fine-tune for a sequence-level task in PyTorch:</p> Python<pre><code>import torch\nfrom multimolecule import RnaTokenizer, UtrBertForSequenceClassification\n\n\ntokenizer = RnaTokenizer.from_pretrained('multimolecule/utrbert-3mer')\nmodel = UtrBertModelForSequenceClassification.from_pretrained('multimolecule/utrbert-3mer')\n\ntext = \"UAGCUUAUCAGACUGAUGUUGA\"\ninput = tokenizer(text, return_tensors='pt')\nlabel = torch.tensor([1])\n\noutput = model(**input, labels=label)\n</code></pre>"},{"location":"models/utrbert/#nucleotide-classification-regression","title":"Nucleotide Classification / Regression","text":"<p>Note: This model is not fine-tuned for any specific task. You will need to fine-tune the model on a downstream task to use it for nucleotide classification or regression.</p> <p>Here is how to use this model as backbone to fine-tune for a nucleotide-level task in PyTorch:</p> Python<pre><code>import torch\nfrom multimolecule import RnaTokenizer, UtrBertForNucleotideClassification\n\n\ntokenizer = RnaTokenizer.from_pretrained('multimolecule/utrbert-3mer')\nmodel = UtrBertModelForNucleotideClassification.from_pretrained('multimolecule/utrbert-3mer')\n\ntext = \"UAGCUUAUCAGACUGAUGUUGA\"\ninput = tokenizer(text, return_tensors='pt')\nlabel = torch.randint(2, (len(text), ))\n\noutput = model(**input, labels=label)\n</code></pre>"},{"location":"models/utrbert/#training-details","title":"Training Details","text":"<p>3UTRBERT used Masked Language Modeling (MLM) as the pre-training objective: taking a sequence, the model randomly masks 15% of the tokens in the input then run the entire masked sentence through the model and has to predict the masked tokens. This is comparable to the Cloze task in language modeling.</p>"},{"location":"models/utrbert/#training-data","title":"Training Data","text":"<p>The 3UTRBERT model was pre-trained on human mRNA transcript sequences from GENCODE. GENCODE aims to identify all gene features in the human genome using a combination of computational analysis, manual annotation, and experimental validation. The GENCODE release 40 used by this work contains 61,544 genes, and 246,624 transcripts.</p> <p>3UTRBERT collected the human mRNA transcript sequences from GENCODE, including 108,573 unique mRNA transcripts. Only the longest transcript of each gene was used in the pre-training process. 3UTRBERT only used the 3\u2019 untranslated regions (3\u2019UTRs) of the mRNA transcripts for pre-training to avoid codon constrains in the CDS region, and to reduce increased complexity of the entire mRNA transcripts. The average length of the 3\u2019UTRs was 1,227 nucleotides, while the median length was 631 nucleotides. Each 3\u2019UTR sequence was cut to non-overlapping patches of 510 nucleotides. The remaining sequences were padded to the same length.</p> <p>Note <code>RnaTokenizer</code> will convert \u201cT\u201ds to \u201cU\u201ds for you, you may disable this behaviour by passing <code>replace_T_with_U=False</code>.</p>"},{"location":"models/utrbert/#training-procedure","title":"Training Procedure","text":""},{"location":"models/utrbert/#preprocessing","title":"Preprocessing","text":"<p>3UTRBERT used masked language modeling (MLM) as the pre-training objective. The masking procedure is similar to the one used in BERT:</p> <ul> <li>15% of the tokens are masked.</li> <li>In 80% of the cases, the masked tokens are replaced by <code>&lt;mask&gt;</code>.</li> <li>In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.</li> <li>In the 10% remaining cases, the masked tokens are left as is.</li> </ul> <p>Since 3UTRBERT used k-mer tokenizer, it masks the entire k-mer instead of individual nucleotides to avoid information leakage.</p> <p>For example, if the k-mer is 3, the sequence <code>\"UAGCGUAU\"</code> will be tokenized as <code>[\"UAG\", \"AGC\", \"GCG\", \"CGU\", \"GUA\", \"UAU\"]</code>. If the nucleotide <code>\"C\"</code> is masked, the adjacent tokens will also be masked, resulting <code>[\"UAG\", \"&lt;mask&gt;\", \"&lt;mask&gt;\", \"&lt;mask&gt;\", \"GUA\", \"UAU\"]</code>.</p>"},{"location":"models/utrbert/#pretraining","title":"Pretraining","text":"<p>The model was trained on 4 NVIDIA Quadro RTX 6000 GPUs with 24GiB memories.</p> <ul> <li>Batch size: 128</li> <li>Learning rate: 3e-4</li> <li>Weight decay: 0.01</li> <li>Optimizer: AdamW(\u03b21=0.9, \u03b22=0.98, e=1e-6)</li> <li>Steps: 200,000</li> <li>Learning rate scheduler: Linear</li> <li>Learning rate warm-up: 10,000 steps</li> </ul>"},{"location":"models/utrbert/#citation","title":"Citation","text":"<p>BibTeX:</p> BibTeX<pre><code>@article {yang2023deciphering,\n    author = {Yang, Yuning and Li, Gen and Pang, Kuan and Cao, Wuxinhao and Li, Xiangtao and Zhang, Zhaolei},\n    title = {Deciphering 3{\\textquoteright} UTR mediated gene regulation using interpretable deep representation learning},\n    elocation-id = {2023.09.08.556883},\n    year = {2023},\n    doi = {10.1101/2023.09.08.556883},\n    publisher = {Cold Spring Harbor Laboratory},\n    abstract = {The 3{\\textquoteright}untranslated regions (3{\\textquoteright}UTRs) of messenger RNAs contain many important cis-regulatory elements that are under functional and evolutionary constraints. We hypothesize that these constraints are similar to grammars and syntaxes in human languages and can be modeled by advanced natural language models such as Transformers, which has been very effective in modeling protein sequence and structures. Here we describe 3UTRBERT, which implements an attention-based language model, i.e., Bidirectional Encoder Representations from Transformers (BERT). 3UTRBERT was pre-trained on aggregated 3{\\textquoteright}UTR sequences of human mRNAs in a task-agnostic manner; the pre-trained model was then fine-tuned for specific downstream tasks such as predicting RBP binding sites, m6A RNA modification sites, and predicting RNA sub-cellular localizations. Benchmark results showed that 3UTRBERT generally outperformed other contemporary methods in each of these tasks. We also showed that the self-attention mechanism within 3UTRBERT allows direct visualization of the semantic relationship between sequence elements.Competing Interest StatementThe authors have declared no competing interest.},\n    URL = {https://www.biorxiv.org/content/early/2023/09/12/2023.09.08.556883},\n    eprint = {https://www.biorxiv.org/content/early/2023/09/12/2023.09.08.556883.full.pdf},\n    journal = {bioRxiv}\n}\n</code></pre>"},{"location":"models/utrbert/#contact","title":"Contact","text":"<p>Please use GitHub issues of MultiMolecule for any questions or comments on the model card.</p> <p>Please contact the authors of the 3UTRBERT paper for questions or comments on the paper/model.</p>"},{"location":"models/utrbert/#license","title":"License","text":"<p>This model is licensed under the AGPL-3.0 License.</p> Text Only<pre><code>SPDX-License-Identifier: AGPL-3.0-or-later\n</code></pre>"},{"location":"models/utrbert/#multimolecule.models.utrbert","title":"<code>multimolecule.models.utrbert</code>","text":""},{"location":"models/utrbert/#multimolecule.models.utrbert.RnaTokenizer","title":"<code>RnaTokenizer</code>","text":"<p>               Bases: <code>Tokenizer</code></p> <p>Constructs an RNA tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>alphabet</code> <code>List[str] | None</code> <p>List of tokens to use. Defaults to IUPAC nucleotide code.</p> <code>None</code> <code>nmers</code> <code>int</code> <p>Size of nmers to tokenize. Defaults to 1.</p> <code>1</code> <code>codon</code> <code>bool</code> <p>Whether to tokenize into codons. Defaults to False.</p> <code>False</code> <code>replace_T_with_U</code> <code>bool</code> <p>Whether to replace T with U. Defaults to True.</p> <code>True</code> <code>do_upper_case</code> <code>bool</code> <p>Whether to convert input to uppercase. Defaults to True.</p> <code>True</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import RnaTokenizer\n&gt;&gt;&gt; tokenizer = RnaTokenizer()\n&gt;&gt;&gt; tokenizer('&lt;pad&gt;&lt;cls&gt;&lt;eos&gt;&lt;unk&gt;&lt;mask&gt;&lt;null&gt;ACGUNXVHDBMRWSYK.*-')[\"input_ids\"]\n[1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 2]\n&gt;&gt;&gt; tokenizer('acgu')[\"input_ids\"]\n[1, 6, 7, 8, 9, 2]\n&gt;&gt;&gt; tokenizer('acgt')[\"input_ids\"]\n[1, 6, 7, 8, 9, 2]\n&gt;&gt;&gt; tokenizer = RnaTokenizer(replace_T_with_U=False)\n&gt;&gt;&gt; tokenizer('acgt')[\"input_ids\"]\n[1, 6, 7, 8, 3, 2]\n&gt;&gt;&gt; tokenizer = RnaTokenizer(nmers=3)\n&gt;&gt;&gt; tokenizer('uagcuuauc')[\"input_ids\"]\n[1, 83, 17, 64, 49, 96, 84, 22, 2]\n&gt;&gt;&gt; tokenizer = RnaTokenizer(codon=True)\n&gt;&gt;&gt; tokenizer('uagcuuauc')[\"input_ids\"]\n[1, 83, 49, 22, 2]\n&gt;&gt;&gt; tokenizer('uagcuuauca')[\"input_ids\"]\nTraceback (most recent call last):\nValueError: length of input sequence  must be a multiple of 3 for codon tokenization, but got 10\n</code></pre> Source code in <code>multimolecule/tokenisers/rna/tokenization_rna.py</code> Python<pre><code>class RnaTokenizer(Tokenizer):\n    \"\"\"\n    Constructs an RNA tokenizer.\n\n    Args:\n        alphabet (List[str] | None, optional): List of tokens to use.\n            Defaults to [IUPAC nucleotide code](https://www.bioinformatics.org/sms2/iupac.html).\n        nmers (int, optional): Size of nmers to tokenize.\n            Defaults to 1.\n        codon (bool, optional): Whether to tokenize into codons.\n            Defaults to False.\n        replace_T_with_U (bool, optional): Whether to replace T with U.\n            Defaults to True.\n        do_upper_case (bool, optional): Whether to convert input to uppercase.\n            Defaults to True.\n\n    Examples:\n        &gt;&gt;&gt; from multimolecule import RnaTokenizer\n        &gt;&gt;&gt; tokenizer = RnaTokenizer()\n        &gt;&gt;&gt; tokenizer('&lt;pad&gt;&lt;cls&gt;&lt;eos&gt;&lt;unk&gt;&lt;mask&gt;&lt;null&gt;ACGUNXVHDBMRWSYK.*-')[\"input_ids\"]\n        [1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 2]\n        &gt;&gt;&gt; tokenizer('acgu')[\"input_ids\"]\n        [1, 6, 7, 8, 9, 2]\n        &gt;&gt;&gt; tokenizer('acgt')[\"input_ids\"]\n        [1, 6, 7, 8, 9, 2]\n        &gt;&gt;&gt; tokenizer = RnaTokenizer(replace_T_with_U=False)\n        &gt;&gt;&gt; tokenizer('acgt')[\"input_ids\"]\n        [1, 6, 7, 8, 3, 2]\n        &gt;&gt;&gt; tokenizer = RnaTokenizer(nmers=3)\n        &gt;&gt;&gt; tokenizer('uagcuuauc')[\"input_ids\"]\n        [1, 83, 17, 64, 49, 96, 84, 22, 2]\n        &gt;&gt;&gt; tokenizer = RnaTokenizer(codon=True)\n        &gt;&gt;&gt; tokenizer('uagcuuauc')[\"input_ids\"]\n        [1, 83, 49, 22, 2]\n        &gt;&gt;&gt; tokenizer('uagcuuauca')[\"input_ids\"]\n        Traceback (most recent call last):\n        ValueError: length of input sequence  must be a multiple of 3 for codon tokenization, but got 10\n    \"\"\"\n\n    model_input_names = [\"input_ids\", \"attention_mask\"]\n\n    def __init__(\n        self,\n        alphabet: List[str] | None = None,\n        nmers: int = 1,\n        codon: bool = False,\n        replace_T_with_U: bool = True,\n        bos_token: str = \"&lt;cls&gt;\",\n        cls_token: str = \"&lt;cls&gt;\",\n        pad_token: str = \"&lt;pad&gt;\",\n        eos_token: str = \"&lt;eos&gt;\",\n        sep_token: str = \"&lt;eos&gt;\",\n        unk_token: str = \"&lt;unk&gt;\",\n        mask_token: str = \"&lt;mask&gt;\",\n        additional_special_tokens: List | Tuple | None = None,\n        do_upper_case: bool = True,\n        **kwargs,\n    ):\n        if codon and nmers &gt; 1:\n            raise ValueError(\"Codon and nmers cannot be used together.\")\n        if codon:\n            nmers = 3  # set to 3 to get correct vocab\n        super().__init__(\n            alphabet=get_vocab_list(alphabet, nmers),\n            bos_token=bos_token,\n            cls_token=cls_token,\n            pad_token=pad_token,\n            eos_token=eos_token,\n            sep_token=sep_token,\n            unk_token=unk_token,\n            mask_token=mask_token,\n            additional_special_tokens=additional_special_tokens,\n            do_upper_case=do_upper_case,\n            **kwargs,\n        )\n        self.replace_T_with_U = replace_T_with_U\n        self.nmers = nmers\n        self.condon = codon\n\n    def _tokenize(self, text: str, **kwargs):\n        if self.do_upper_case:\n            text = text.upper()\n        if self.replace_T_with_U:\n            text = text.replace(\"T\", \"U\")\n        if self.condon:\n            if len(text) % 3 != 0:\n                raise ValueError(\n                    f\"length of input sequence  must be a multiple of 3 for codon tokenization, but got {len(text)}\"\n                )\n            return [text[i : i + 3] for i in range(0, len(text), 3)]\n        if self.nmers &gt; 1:\n            return [text[i : i + self.nmers] for i in range(len(text) - self.nmers + 1)]  # noqa: E203\n        return list(text)\n</code></pre>"},{"location":"models/utrbert/#multimolecule.models.utrbert.UtrBertConfig","title":"<code>UtrBertConfig</code>","text":"<p>               Bases: <code>PreTrainedConfig</code></p> <p>This is the configuration class to store the configuration of a [<code>UtrBertModel</code>]. It is used to instantiate a 3UTRBERT model according to the specified arguments, defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of the 3UTRBERT yangyn533/3UTRBERT architecture.</p> <p>Configuration objects inherit from [<code>PreTrainedConfig</code>] and can be used to control the model outputs. Read the documentation from [<code>PreTrainedConfig</code>] for more information.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_size</code> <code>`int`, *optional*</code> <p>Vocabulary size of the UTRBERT model. Defines the number of different tokens that can be represented by the <code>inputs_ids</code> passed when calling [<code>BertModel</code>].</p> <code>None</code> <code>nmers</code> <code>`int`, *optional*</code> <p>kmer size of the UTRBERT model. Defines the vocabulary size of the model.</p> <code>None</code> <code>hidden_size</code> <code>`int`, *optional*, defaults to 768</code> <p>Dimensionality of the encoder layers and the pooler layer.</p> <code>768</code> <code>num_hidden_layers</code> <code>`int`, *optional*, defaults to 12</code> <p>Number of hidden layers in the Transformer encoder.</p> <code>12</code> <code>num_attention_heads</code> <code>`int`, *optional*, defaults to 12</code> <p>Number of attention heads for each attention layer in the Transformer encoder.</p> <code>12</code> <code>intermediate_size</code> <code>`int`, *optional*, defaults to 3072</code> <p>Dimensionality of the \u201cintermediate\u201d (often named feed-forward) layer in the Transformer encoder.</p> <code>3072</code> <code>hidden_act</code> <code>`str` or `Callable`, *optional*, defaults to `\"gelu\"`</code> <p>The non-linear activation function (function or string) in the encoder and pooler. If string, <code>\"gelu\"</code>, <code>\"relu\"</code>, <code>\"silu\"</code> and <code>\"gelu_new\"</code> are supported.</p> <code>'gelu'</code> <code>hidden_dropout</code> <code>`float`, *optional*, defaults to 0.1</code> <p>The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.</p> <code>0.1</code> <code>attention_dropout</code> <code>`float`, *optional*, defaults to 0.1</code> <p>The dropout ratio for the attention probabilities.</p> <code>0.1</code> <code>max_position_embeddings</code> <code>`int`, *optional*, defaults to 512</code> <p>The maximum sequence length that this model might ever be used with. Typically set this to something large just in case (e.g., 512 or 1024 or 2048).</p> <code>512</code> <code>type_vocab_size</code> <code>`int`, *optional*, defaults to 2</code> <p>The vocabulary size of the <code>token_type_ids</code> passed when calling [<code>BertModel</code>] or [<code>TFBertModel</code>].</p> required <code>initializer_range</code> <code>`float`, *optional*, defaults to 0.02</code> <p>The standard deviation of the truncated_normal_initializer for initializing all weight matrices.</p> <code>0.02</code> <code>layer_norm_eps</code> <code>`float`, *optional*, defaults to 1e-12</code> <p>The epsilon used by the layer normalization layers.</p> <code>1e-12</code> <code>position_embedding_type</code> <code>`str`, *optional*, defaults to `\"absolute\"`</code> <p>Type of position embedding. Choose one of <code>\"absolute\"</code>, <code>\"relative_key\"</code>, <code>\"relative_key_query\"</code>. For positional embeddings use <code>\"absolute\"</code>. For more information on <code>\"relative_key\"</code>, please refer to Self-Attention with Relative Position Representations (Shaw et al.). For more information on <code>\"relative_key_query\"</code>, please refer to Method 4 in Improve Transformer Models with Better Relative Position Embeddings (Huang et al.).</p> <code>'absolute'</code> <code>is_decoder</code> <code>`bool`, *optional*, defaults to `False`</code> <p>Whether the model is used as a decoder or not. If <code>False</code>, the model is used as an encoder.</p> required <code>use_cache</code> <code>`bool`, *optional*, defaults to `True`</code> <p>Whether or not the model should return the last key/values attentions (not used by all models). Only relevant if <code>config.is_decoder=True</code>.</p> <code>True</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import UtrBertConfig, UtrBertModel\n</code></pre> Python Console Session<pre><code>&gt;&gt;&gt; # Initializing a UtrBERT multimolecule/utrbert style configuration\n&gt;&gt;&gt; configuration = UtrBertConfig(vocab_size=25, nmers=1)\n</code></pre> Python Console Session<pre><code>&gt;&gt;&gt; # Initializing a model (with random weights) from the multimolecule/utrbert style configuration\n&gt;&gt;&gt; model = UtrBertModel(configuration)\n</code></pre> Python Console Session<pre><code>&gt;&gt;&gt; # Accessing the model configuration\n&gt;&gt;&gt; configuration = model.config\n</code></pre> Source code in <code>multimolecule/models/utrbert/configuration_utrbert.py</code> Python<pre><code>class UtrBertConfig(PreTrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`UtrBertModel`]. It is used to instantiate a\n    3UTRBERT model according to the specified arguments, defining the model architecture. Instantiating a configuration\n    with the defaults will yield a similar configuration to that of the 3UTRBERT\n    [yangyn533/3UTRBERT](https://github.com/yangyn533/3UTRBERT) architecture.\n\n    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n    documentation from [`PreTrainedConfig`] for more information.\n\n\n    Args:\n        vocab_size (`int`, *optional*):\n            Vocabulary size of the UTRBERT model. Defines the number of different tokens that can be represented by the\n            `inputs_ids` passed when calling [`BertModel`].\n        nmers (`int`, *optional*):\n            kmer size of the UTRBERT model. Defines the vocabulary size of the model.\n        hidden_size (`int`, *optional*, defaults to 768):\n            Dimensionality of the encoder layers and the pooler layer.\n        num_hidden_layers (`int`, *optional*, defaults to 12):\n            Number of hidden layers in the Transformer encoder.\n        num_attention_heads (`int`, *optional*, defaults to 12):\n            Number of attention heads for each attention layer in the Transformer encoder.\n        intermediate_size (`int`, *optional*, defaults to 3072):\n            Dimensionality of the \"intermediate\" (often named feed-forward) layer in the Transformer encoder.\n        hidden_act (`str` or `Callable`, *optional*, defaults to `\"gelu\"`):\n            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n            `\"relu\"`, `\"silu\"` and `\"gelu_new\"` are supported.\n        hidden_dropout (`float`, *optional*, defaults to 0.1):\n            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n        attention_dropout (`float`, *optional*, defaults to 0.1):\n            The dropout ratio for the attention probabilities.\n        max_position_embeddings (`int`, *optional*, defaults to 512):\n            The maximum sequence length that this model might ever be used with. Typically set this to something large\n            just in case (e.g., 512 or 1024 or 2048).\n        type_vocab_size (`int`, *optional*, defaults to 2):\n            The vocabulary size of the `token_type_ids` passed when calling [`BertModel`] or [`TFBertModel`].\n        initializer_range (`float`, *optional*, defaults to 0.02):\n            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n        layer_norm_eps (`float`, *optional*, defaults to 1e-12):\n            The epsilon used by the layer normalization layers.\n        position_embedding_type (`str`, *optional*, defaults to `\"absolute\"`):\n            Type of position embedding. Choose one of `\"absolute\"`, `\"relative_key\"`, `\"relative_key_query\"`. For\n            positional embeddings use `\"absolute\"`. For more information on `\"relative_key\"`, please refer to\n            [Self-Attention with Relative Position Representations (Shaw et al.)](https://arxiv.org/abs/1803.02155).\n            For more information on `\"relative_key_query\"`, please refer to *Method 4* in [Improve Transformer Models\n            with Better Relative Position Embeddings (Huang et al.)](https://arxiv.org/abs/2009.13658).\n        is_decoder (`bool`, *optional*, defaults to `False`):\n            Whether the model is used as a decoder or not. If `False`, the model is used as an encoder.\n        use_cache (`bool`, *optional*, defaults to `True`):\n            Whether or not the model should return the last key/values attentions (not used by all models). Only\n            relevant if `config.is_decoder=True`.\n\n    Examples:\n        &gt;&gt;&gt; from multimolecule import UtrBertConfig, UtrBertModel\n\n        &gt;&gt;&gt; # Initializing a UtrBERT multimolecule/utrbert style configuration\n        &gt;&gt;&gt; configuration = UtrBertConfig(vocab_size=25, nmers=1)\n\n        &gt;&gt;&gt; # Initializing a model (with random weights) from the multimolecule/utrbert style configuration\n        &gt;&gt;&gt; model = UtrBertModel(configuration)\n\n        &gt;&gt;&gt; # Accessing the model configuration\n        &gt;&gt;&gt; configuration = model.config\n    \"\"\"\n\n    model_type = \"utrbert\"\n\n    def __init__(\n        self,\n        vocab_size=None,\n        nmers=None,\n        hidden_size=768,\n        num_hidden_layers=12,\n        num_attention_heads=12,\n        intermediate_size=3072,\n        hidden_act=\"gelu\",\n        hidden_dropout=0.1,\n        attention_dropout=0.1,\n        max_position_embeddings=512,\n        initializer_range=0.02,\n        layer_norm_eps=1e-12,\n        position_embedding_type=\"absolute\",\n        use_cache=True,\n        head=None,\n        lm_head=None,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n\n        self.vocab_size = vocab_size\n        self.type_vocab_size = 2\n        self.nmers = nmers\n        self.hidden_size = hidden_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.hidden_act = hidden_act\n        self.intermediate_size = intermediate_size\n        self.hidden_dropout = hidden_dropout\n        self.attention_dropout = attention_dropout\n        self.max_position_embeddings = max_position_embeddings\n        self.initializer_range = initializer_range\n        self.layer_norm_eps = layer_norm_eps\n        self.position_embedding_type = position_embedding_type\n        self.use_cache = use_cache\n        self.head = HeadConfig(**head if head is not None else {})\n        self.lm_head = MaskedLMHeadConfig(**lm_head if lm_head is not None else {})\n</code></pre>"},{"location":"models/utrbert/#multimolecule.models.utrbert.UtrBertForMaskedLM","title":"<code>UtrBertForMaskedLM</code>","text":"<p>               Bases: <code>UtrBertPreTrainedModel</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import UtrBertConfig, UtrBertForMaskedLM, RnaTokenizer\n&gt;&gt;&gt; tokenizer = RnaTokenizer(nmers=2)\n&gt;&gt;&gt; config = UtrBertConfig(vocab_size=tokenizer.vocab_size)\n&gt;&gt;&gt; model = UtrBertForMaskedLM(config)\n&gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n&gt;&gt;&gt; output = model(**input)\n</code></pre> Source code in <code>multimolecule/models/utrbert/modeling_utrbert.py</code> Python<pre><code>class UtrBertForMaskedLM(UtrBertPreTrainedModel):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; from multimolecule import UtrBertConfig, UtrBertForMaskedLM, RnaTokenizer\n        &gt;&gt;&gt; tokenizer = RnaTokenizer(nmers=2)\n        &gt;&gt;&gt; config = UtrBertConfig(vocab_size=tokenizer.vocab_size)\n        &gt;&gt;&gt; model = UtrBertForMaskedLM(config)\n        &gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n        &gt;&gt;&gt; output = model(**input)\n    \"\"\"\n\n    _tied_weights_keys = [\"predictions.decoder.bias\", \"cls.predictions.decoder.weight\"]\n\n    def __init__(self, config: UtrBertConfig):\n        super().__init__(config)\n        if config.is_decoder:\n            logger.warning(\n                \"If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for \"\n                \"bi-directional self-attention.\"\n            )\n        self.utrbert = UtrBertModel(config, add_pooling_layer=False)\n        self.lm_head = MaskedLMHead(config)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_output_embeddings(self):\n        return self.lm_head.decoder\n\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head.decoder = new_embeddings\n\n    def forward(\n        self,\n        input_ids: Tensor | NestedTensor,\n        attention_mask: Tensor | None = None,\n        position_ids: Tensor | None = None,\n        head_mask: Tensor | None = None,\n        inputs_embeds: Tensor | NestedTensor | None = None,\n        encoder_hidden_states: Tensor | None = None,\n        encoder_attention_mask: Tensor | None = None,\n        labels: Tensor | None = None,\n        output_attentions: bool | None = None,\n        output_hidden_states: bool | None = None,\n        return_dict: bool | None = None,\n    ) -&gt; Tuple[Tensor, ...] | MaskedLMOutput:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n            config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n            loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n        \"\"\"\n\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.utrbert(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        output = self.lm_head(outputs, labels)\n        logits, loss = output.logits, output.loss\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return MaskedLMOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"models/utrbert/#multimolecule.models.utrbert.UtrBertForMaskedLM.forward","title":"<code>forward(input_ids, attention_mask=None, position_ids=None, head_mask=None, inputs_embeds=None, encoder_hidden_states=None, encoder_attention_mask=None, labels=None, output_attentions=None, output_hidden_states=None, return_dict=None)</code>","text":"<p>labels (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, optional):     Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ...,     config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the     loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code></p> Source code in <code>multimolecule/models/utrbert/modeling_utrbert.py</code> Python<pre><code>def forward(\n    self,\n    input_ids: Tensor | NestedTensor,\n    attention_mask: Tensor | None = None,\n    position_ids: Tensor | None = None,\n    head_mask: Tensor | None = None,\n    inputs_embeds: Tensor | NestedTensor | None = None,\n    encoder_hidden_states: Tensor | None = None,\n    encoder_attention_mask: Tensor | None = None,\n    labels: Tensor | None = None,\n    output_attentions: bool | None = None,\n    output_hidden_states: bool | None = None,\n    return_dict: bool | None = None,\n) -&gt; Tuple[Tensor, ...] | MaskedLMOutput:\n    r\"\"\"\n    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n        Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n        config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n        loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n    \"\"\"\n\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n    outputs = self.utrbert(\n        input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        head_mask=head_mask,\n        inputs_embeds=inputs_embeds,\n        encoder_hidden_states=encoder_hidden_states,\n        encoder_attention_mask=encoder_attention_mask,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n    output = self.lm_head(outputs, labels)\n    logits, loss = output.logits, output.loss\n\n    if not return_dict:\n        output = (logits,) + outputs[2:]\n        return ((loss,) + output) if loss is not None else output\n\n    return MaskedLMOutput(\n        loss=loss,\n        logits=logits,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"models/utrbert/#multimolecule.models.utrbert.UtrBertForNucleotideClassification","title":"<code>UtrBertForNucleotideClassification</code>","text":"<p>               Bases: <code>UtrBertPreTrainedModel</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import UtrBertConfig, UtrBertForNucleotideClassification, RnaTokenizer\n&gt;&gt;&gt; tokenizer = RnaTokenizer(nmers=2)\n&gt;&gt;&gt; config = UtrBertConfig(vocab_size=tokenizer.vocab_size, nmers=2)\n&gt;&gt;&gt; model = UtrBertForTokenClassification(config)\n&gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n&gt;&gt;&gt; output = model(**input)\n</code></pre> Source code in <code>multimolecule/models/utrbert/modeling_utrbert.py</code> Python<pre><code>class UtrBertForNucleotideClassification(UtrBertPreTrainedModel):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; from multimolecule import UtrBertConfig, UtrBertForNucleotideClassification, RnaTokenizer\n        &gt;&gt;&gt; tokenizer = RnaTokenizer(nmers=2)\n        &gt;&gt;&gt; config = UtrBertConfig(vocab_size=tokenizer.vocab_size, nmers=2)\n        &gt;&gt;&gt; model = UtrBertForTokenClassification(config)\n        &gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n        &gt;&gt;&gt; output = model(**input)\n    \"\"\"\n\n    def __init__(self, config: UtrBertConfig):\n        super().__init__(config)\n        self.num_labels = config.head.num_labels\n        self.utrbert = UtrBertModel(config, add_pooling_layer=False)\n        self.nucleotide_head = NucleotideKMerHead(config)\n        self.head_config = self.nucleotide_head.config\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(\n        self,\n        input_ids: Tensor | NestedTensor,\n        attention_mask: Tensor | None = None,\n        position_ids: Tensor | None = None,\n        head_mask: Tensor | None = None,\n        inputs_embeds: Tensor | NestedTensor | None = None,\n        labels: Tensor | None = None,\n        output_attentions: bool | None = None,\n        output_hidden_states: bool | None = None,\n        return_dict: bool | None = None,\n        **kwargs,\n    ) -&gt; Tuple[Tensor, ...] | TokenClassifierOutput:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n            `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        if kwargs:\n            warn(\n                f\"Additional keyword arguments `{', '.join(kwargs)}` are detected in \"\n                f\"`{self.__class__.__name__}.forward`, they will be ignored.\\n\"\n                \"This is provided for backward compatibility and may lead to unexpected behavior.\"\n            )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.utrbert(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        output = self.nucleotide_head(outputs, attention_mask, input_ids, labels)\n        logits, loss = output.logits, output.loss\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return TokenClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"models/utrbert/#multimolecule.models.utrbert.UtrBertForNucleotideClassification.forward","title":"<code>forward(input_ids, attention_mask=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None, output_attentions=None, output_hidden_states=None, return_dict=None, **kwargs)</code>","text":"<p>labels (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, optional):     Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ...,     config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If     <code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).</p> Source code in <code>multimolecule/models/utrbert/modeling_utrbert.py</code> Python<pre><code>def forward(\n    self,\n    input_ids: Tensor | NestedTensor,\n    attention_mask: Tensor | None = None,\n    position_ids: Tensor | None = None,\n    head_mask: Tensor | None = None,\n    inputs_embeds: Tensor | NestedTensor | None = None,\n    labels: Tensor | None = None,\n    output_attentions: bool | None = None,\n    output_hidden_states: bool | None = None,\n    return_dict: bool | None = None,\n    **kwargs,\n) -&gt; Tuple[Tensor, ...] | TokenClassifierOutput:\n    r\"\"\"\n    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n        `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).\n    \"\"\"\n    if kwargs:\n        warn(\n            f\"Additional keyword arguments `{', '.join(kwargs)}` are detected in \"\n            f\"`{self.__class__.__name__}.forward`, they will be ignored.\\n\"\n            \"This is provided for backward compatibility and may lead to unexpected behavior.\"\n        )\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n    outputs = self.utrbert(\n        input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        head_mask=head_mask,\n        inputs_embeds=inputs_embeds,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n    output = self.nucleotide_head(outputs, attention_mask, input_ids, labels)\n    logits, loss = output.logits, output.loss\n\n    if not return_dict:\n        output = (logits,) + outputs[2:]\n        return ((loss,) + output) if loss is not None else output\n\n    return TokenClassifierOutput(\n        loss=loss,\n        logits=logits,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"models/utrbert/#multimolecule.models.utrbert.UtrBertForPretraining","title":"<code>UtrBertForPretraining</code>","text":"<p>               Bases: <code>UtrBertPreTrainedModel</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import UtrBertConfig, UtrBertForPretraining, RnaTokenizer\n&gt;&gt;&gt; tokenizer = RnaTokenizer(nmers=3)\n&gt;&gt;&gt; config = UtrBertConfig(vocab_size=tokenizer.vocab_size)\n&gt;&gt;&gt; model = UtrBertForPretraining(config)\n&gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n&gt;&gt;&gt; output = model(**input)\n</code></pre> Source code in <code>multimolecule/models/utrbert/modeling_utrbert.py</code> Python<pre><code>class UtrBertForPretraining(UtrBertPreTrainedModel):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; from multimolecule import UtrBertConfig, UtrBertForPretraining, RnaTokenizer\n        &gt;&gt;&gt; tokenizer = RnaTokenizer(nmers=3)\n        &gt;&gt;&gt; config = UtrBertConfig(vocab_size=tokenizer.vocab_size)\n        &gt;&gt;&gt; model = UtrBertForPretraining(config)\n        &gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n        &gt;&gt;&gt; output = model(**input)\n    \"\"\"\n\n    _tied_weights_keys = [\"lm_head.decoder.bias\", \"lm_head.decoder.weight\"]\n\n    def __init__(self, config: UtrBertConfig):\n        super().__init__(config)\n        if config.is_decoder:\n            logger.warning(\n                \"If you want to use `UtrBertForPretraining` make sure `config.is_decoder=False` for \"\n                \"bi-directional self-attention.\"\n            )\n        self.utrbert = UtrBertModel(config, add_pooling_layer=True)\n        self.lm_head = MaskedLMHead(config)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(\n        self,\n        input_ids: Tensor | NestedTensor,\n        attention_mask: Tensor | None = None,\n        position_ids: Tensor | None = None,\n        head_mask: Tensor | None = None,\n        inputs_embeds: Tensor | NestedTensor | None = None,\n        labels: Tensor | None = None,\n        output_attentions: bool | None = None,\n        output_hidden_states: bool | None = None,\n        return_dict: bool | None = None,\n    ) -&gt; Tuple[Tensor, ...] | MaskedLMOutput:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n            config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked),\n            the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.utrbert(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        output = self.lm_head(outputs, labels)\n        logits, loss = output.logits, output.loss\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return MaskedLMOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"models/utrbert/#multimolecule.models.utrbert.UtrBertForPretraining.forward","title":"<code>forward(input_ids, attention_mask=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None, output_attentions=None, output_hidden_states=None, return_dict=None)</code>","text":"<p>labels (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, optional):     Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ...,     config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked),     the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code></p> Source code in <code>multimolecule/models/utrbert/modeling_utrbert.py</code> Python<pre><code>def forward(\n    self,\n    input_ids: Tensor | NestedTensor,\n    attention_mask: Tensor | None = None,\n    position_ids: Tensor | None = None,\n    head_mask: Tensor | None = None,\n    inputs_embeds: Tensor | NestedTensor | None = None,\n    labels: Tensor | None = None,\n    output_attentions: bool | None = None,\n    output_hidden_states: bool | None = None,\n    return_dict: bool | None = None,\n) -&gt; Tuple[Tensor, ...] | MaskedLMOutput:\n    r\"\"\"\n    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n        Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n        config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked),\n        the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n    \"\"\"\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n    outputs = self.utrbert(\n        input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        head_mask=head_mask,\n        inputs_embeds=inputs_embeds,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n    output = self.lm_head(outputs, labels)\n    logits, loss = output.logits, output.loss\n\n    if not return_dict:\n        output = (logits,) + outputs[2:]\n        return ((loss,) + output) if loss is not None else output\n\n    return MaskedLMOutput(\n        loss=loss,\n        logits=logits,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"models/utrbert/#multimolecule.models.utrbert.UtrBertForSequenceClassification","title":"<code>UtrBertForSequenceClassification</code>","text":"<p>               Bases: <code>UtrBertPreTrainedModel</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import UtrBertConfig, UtrBertForSequenceClassification, RnaTokenizer\n&gt;&gt;&gt; tokenizer = RnaTokenizer(nmers=4)\n&gt;&gt;&gt; config = UtrBertConfig(vocab_size=tokenizer.vocab_size)\n&gt;&gt;&gt; model = UtrBertForSequenceClassification(config)\n&gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n&gt;&gt;&gt; output = model(**input)\n</code></pre> Source code in <code>multimolecule/models/utrbert/modeling_utrbert.py</code> Python<pre><code>class UtrBertForSequenceClassification(UtrBertPreTrainedModel):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; from multimolecule import UtrBertConfig, UtrBertForSequenceClassification, RnaTokenizer\n        &gt;&gt;&gt; tokenizer = RnaTokenizer(nmers=4)\n        &gt;&gt;&gt; config = UtrBertConfig(vocab_size=tokenizer.vocab_size)\n        &gt;&gt;&gt; model = UtrBertForSequenceClassification(config)\n        &gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n        &gt;&gt;&gt; output = model(**input)\n    \"\"\"\n\n    def __init__(self, config: UtrBertConfig):\n        super().__init__(config)\n        self.num_labels = config.head.num_labels\n        self.utrbert = UtrBertModel(config)\n        self.sequence_head = SequenceClassificationHead(config)\n        self.head_config = self.sequence_head.config\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(\n        self,\n        input_ids: Tensor | NestedTensor,\n        attention_mask: Tensor | None = None,\n        position_ids: Tensor | None = None,\n        head_mask: Tensor | None = None,\n        inputs_embeds: Tensor | NestedTensor | None = None,\n        labels: Tensor | None = None,\n        output_attentions: bool | None = None,\n        output_hidden_states: bool | None = None,\n        return_dict: bool | None = None,\n    ) -&gt; Tuple[Tensor, ...] | SequenceClassifierOutput:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n            `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.utrbert(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        output = self.sequence_head(outputs, labels)\n        logits, loss = output.logits, output.loss\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"models/utrbert/#multimolecule.models.utrbert.UtrBertForSequenceClassification.forward","title":"<code>forward(input_ids, attention_mask=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None, output_attentions=None, output_hidden_states=None, return_dict=None)</code>","text":"<p>labels (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, optional):     Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ...,     config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If     <code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).</p> Source code in <code>multimolecule/models/utrbert/modeling_utrbert.py</code> Python<pre><code>def forward(\n    self,\n    input_ids: Tensor | NestedTensor,\n    attention_mask: Tensor | None = None,\n    position_ids: Tensor | None = None,\n    head_mask: Tensor | None = None,\n    inputs_embeds: Tensor | NestedTensor | None = None,\n    labels: Tensor | None = None,\n    output_attentions: bool | None = None,\n    output_hidden_states: bool | None = None,\n    return_dict: bool | None = None,\n) -&gt; Tuple[Tensor, ...] | SequenceClassifierOutput:\n    r\"\"\"\n    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n        `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).\n    \"\"\"\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n    outputs = self.utrbert(\n        input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        head_mask=head_mask,\n        inputs_embeds=inputs_embeds,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n    output = self.sequence_head(outputs, labels)\n    logits, loss = output.logits, output.loss\n\n    if not return_dict:\n        output = (logits,) + outputs[2:]\n        return ((loss,) + output) if loss is not None else output\n\n    return SequenceClassifierOutput(\n        loss=loss,\n        logits=logits,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"models/utrbert/#multimolecule.models.utrbert.UtrBertForTokenClassification","title":"<code>UtrBertForTokenClassification</code>","text":"<p>               Bases: <code>UtrBertPreTrainedModel</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import UtrBertConfig, UtrBertForTokenClassification, RnaTokenizer\n&gt;&gt;&gt; tokenizer = RnaTokenizer(nmers=2)\n&gt;&gt;&gt; config = UtrBertConfig(vocab_size=tokenizer.vocab_size, nmers=2)\n&gt;&gt;&gt; model = UtrBertForTokenClassification(config)\n&gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n&gt;&gt;&gt; output = model(**input)\n</code></pre> Source code in <code>multimolecule/models/utrbert/modeling_utrbert.py</code> Python<pre><code>class UtrBertForTokenClassification(UtrBertPreTrainedModel):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; from multimolecule import UtrBertConfig, UtrBertForTokenClassification, RnaTokenizer\n        &gt;&gt;&gt; tokenizer = RnaTokenizer(nmers=2)\n        &gt;&gt;&gt; config = UtrBertConfig(vocab_size=tokenizer.vocab_size, nmers=2)\n        &gt;&gt;&gt; model = UtrBertForTokenClassification(config)\n        &gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n        &gt;&gt;&gt; output = model(**input)\n    \"\"\"\n\n    def __init__(self, config: UtrBertConfig):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.utrbert = UtrBertModel(config, add_pooling_layer=False)\n        self.token_head = TokenKMerHead(config)\n        self.head_config = self.token_head.config\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(\n        self,\n        input_ids: Tensor | NestedTensor,\n        attention_mask: Tensor | None = None,\n        position_ids: Tensor | None = None,\n        head_mask: Tensor | None = None,\n        inputs_embeds: Tensor | NestedTensor | None = None,\n        labels: Tensor | None = None,\n        output_attentions: bool | None = None,\n        output_hidden_states: bool | None = None,\n        return_dict: bool | None = None,\n    ) -&gt; Tuple[Tensor, ...] | TokenClassifierOutput:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.utrbert(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        output = self.token_head(outputs, attention_mask, input_ids, labels)\n        logits, loss = output.logits, output.loss\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return TokenClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"models/utrbert/#multimolecule.models.utrbert.UtrBertForTokenClassification.forward","title":"<code>forward(input_ids, attention_mask=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None, output_attentions=None, output_hidden_states=None, return_dict=None)</code>","text":"<p>labels (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, optional):     Labels for computing the token classification loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>.</p> Source code in <code>multimolecule/models/utrbert/modeling_utrbert.py</code> Python<pre><code>def forward(\n    self,\n    input_ids: Tensor | NestedTensor,\n    attention_mask: Tensor | None = None,\n    position_ids: Tensor | None = None,\n    head_mask: Tensor | None = None,\n    inputs_embeds: Tensor | NestedTensor | None = None,\n    labels: Tensor | None = None,\n    output_attentions: bool | None = None,\n    output_hidden_states: bool | None = None,\n    return_dict: bool | None = None,\n) -&gt; Tuple[Tensor, ...] | TokenClassifierOutput:\n    r\"\"\"\n    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n        Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n    \"\"\"\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n    outputs = self.utrbert(\n        input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        head_mask=head_mask,\n        inputs_embeds=inputs_embeds,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n    output = self.token_head(outputs, attention_mask, input_ids, labels)\n    logits, loss = output.logits, output.loss\n\n    if not return_dict:\n        output = (logits,) + outputs[2:]\n        return ((loss,) + output) if loss is not None else output\n\n    return TokenClassifierOutput(\n        loss=loss,\n        logits=logits,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"models/utrbert/#multimolecule.models.utrbert.UtrBertModel","title":"<code>UtrBertModel</code>","text":"<p>               Bases: <code>UtrBertPreTrainedModel</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import UtrBertConfig, UtrBertModel, RnaTokenizer\n&gt;&gt;&gt; tokenizer = RnaTokenizer(nmers=1)\n&gt;&gt;&gt; config = UtrBertConfig(vocab_size=tokenizer.vocab_size)\n&gt;&gt;&gt; model = UtrBertModel(config)\n&gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n&gt;&gt;&gt; output = model(**input)\n</code></pre> Source code in <code>multimolecule/models/utrbert/modeling_utrbert.py</code> Python<pre><code>class UtrBertModel(UtrBertPreTrainedModel):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; from multimolecule import UtrBertConfig, UtrBertModel, RnaTokenizer\n        &gt;&gt;&gt; tokenizer = RnaTokenizer(nmers=1)\n        &gt;&gt;&gt; config = UtrBertConfig(vocab_size=tokenizer.vocab_size)\n        &gt;&gt;&gt; model = UtrBertModel(config)\n        &gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n        &gt;&gt;&gt; output = model(**input)\n    \"\"\"\n\n    def __init__(self, config: UtrBertConfig, add_pooling_layer: bool = True):\n        super().__init__(config)\n        self.pad_token_id = config.pad_token_id\n        self.embeddings = UtrBertEmbeddings(config)\n        self.encoder = UtrBertEncoder(config)\n        self.pooler = UtrBertPooler(config) if add_pooling_layer else None\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.embeddings.word_embeddings\n\n    def set_input_embeddings(self, value):\n        self.embeddings.word_embeddings = value\n\n    def _prune_heads(self, heads_to_prune):\n        \"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"\n        for layer, heads in heads_to_prune.items():\n            self.encoder.layer[layer].attention.prune_heads(heads)\n\n    def forward(\n        self,\n        input_ids: Tensor | NestedTensor,\n        attention_mask: Tensor | None = None,\n        position_ids: Tensor | None = None,\n        head_mask: Tensor | None = None,\n        inputs_embeds: Tensor | NestedTensor | None = None,\n        encoder_hidden_states: Tensor | None = None,\n        encoder_attention_mask: Tensor | None = None,\n        past_key_values: Tuple[Tuple[torch.FloatTensor, torch.FloatTensor], ...] | None = None,\n        use_cache: bool | None = None,\n        output_attentions: bool | None = None,\n        output_hidden_states: bool | None = None,\n        return_dict: bool | None = None,\n    ) -&gt; Tuple[Tensor, ...] | BaseModelOutputWithPoolingAndCrossAttentions:\n        r\"\"\"\n        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n            the model is configured as a decoder.\n        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors\n            of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n\n            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n            `past_key_values`).\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if self.config.is_decoder:\n            use_cache = use_cache if use_cache is not None else self.config.use_cache\n        else:\n            use_cache = False\n\n        if isinstance(input_ids, NestedTensor):\n            input_ids, attention_mask = input_ids.tensor, input_ids.mask\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n        if input_ids is not None:\n            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n            input_shape = input_ids.size()\n        elif inputs_embeds is not None:\n            input_shape = inputs_embeds.size()[:-1]\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n        batch_size, seq_length = input_shape\n        device = input_ids.device if input_ids is not None else inputs_embeds.device  # type: ignore[union-attr]\n\n        # past_key_values_length\n        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n\n        if attention_mask is None:\n            attention_mask = (\n                input_ids.ne(self.pad_token_id)\n                if self.pad_token_id is not None\n                else torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n            )\n\n        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n        # ourselves in which case we just need to make it broadcastable to all heads.\n        extended_attention_mask: Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n\n        # If a 2D or 3D attention mask is provided for the cross-attention\n        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n        if self.config.is_decoder and encoder_hidden_states is not None:\n            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n            if encoder_attention_mask is None:\n                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n        else:\n            encoder_extended_attention_mask = None\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n\n        embedding_output = self.embeddings(\n            input_ids=input_ids,\n            position_ids=position_ids,\n            inputs_embeds=inputs_embeds,\n            past_key_values_length=past_key_values_length,\n        )\n        encoder_outputs = self.encoder(\n            embedding_output,\n            attention_mask=extended_attention_mask,\n            head_mask=head_mask,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_extended_attention_mask,\n            past_key_values=past_key_values,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        sequence_output = encoder_outputs[0]\n        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n\n        if not return_dict:\n            return (sequence_output, pooled_output) + encoder_outputs[1:]\n\n        return BaseModelOutputWithPoolingAndCrossAttentions(\n            last_hidden_state=sequence_output,\n            pooler_output=pooled_output,\n            past_key_values=encoder_outputs.past_key_values,\n            hidden_states=encoder_outputs.hidden_states,\n            attentions=encoder_outputs.attentions,\n            cross_attentions=encoder_outputs.cross_attentions,\n        )\n</code></pre>"},{"location":"models/utrbert/#multimolecule.models.utrbert.UtrBertModel.forward","title":"<code>forward(input_ids, attention_mask=None, position_ids=None, head_mask=None, inputs_embeds=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_values=None, use_cache=None, output_attentions=None, output_hidden_states=None, return_dict=None)</code>","text":"<p>encoder_hidden_states  (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, optional):     Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if     the model is configured as a decoder. encoder_attention_mask (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, optional):     Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in     the cross-attention if the model is configured as a decoder. Mask values selected in <code>[0, 1]</code>:</p> Text Only<pre><code>- 1 for tokens that are **not masked**,\n- 0 for tokens that are **masked**.\n</code></pre> <p>past_key_values (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors     of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>):     Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.</p> Text Only<pre><code>If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\ndon't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n`decoder_input_ids` of shape `(batch_size, sequence_length)`.\n</code></pre> <p>use_cache (<code>bool</code>, optional):     If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see     <code>past_key_values</code>).</p> Source code in <code>multimolecule/models/utrbert/modeling_utrbert.py</code> Python<pre><code>def forward(\n    self,\n    input_ids: Tensor | NestedTensor,\n    attention_mask: Tensor | None = None,\n    position_ids: Tensor | None = None,\n    head_mask: Tensor | None = None,\n    inputs_embeds: Tensor | NestedTensor | None = None,\n    encoder_hidden_states: Tensor | None = None,\n    encoder_attention_mask: Tensor | None = None,\n    past_key_values: Tuple[Tuple[torch.FloatTensor, torch.FloatTensor], ...] | None = None,\n    use_cache: bool | None = None,\n    output_attentions: bool | None = None,\n    output_hidden_states: bool | None = None,\n    return_dict: bool | None = None,\n) -&gt; Tuple[Tensor, ...] | BaseModelOutputWithPoolingAndCrossAttentions:\n    r\"\"\"\n    encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n        Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n        the model is configured as a decoder.\n    encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n        Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n        the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n\n        - 1 for tokens that are **not masked**,\n        - 0 for tokens that are **masked**.\n    past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors\n        of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n        Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n\n        If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n        don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n        `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n    use_cache (`bool`, *optional*):\n        If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n        `past_key_values`).\n    \"\"\"\n    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n    output_hidden_states = (\n        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n    )\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n    if self.config.is_decoder:\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n    else:\n        use_cache = False\n\n    if isinstance(input_ids, NestedTensor):\n        input_ids, attention_mask = input_ids.tensor, input_ids.mask\n    if input_ids is not None and inputs_embeds is not None:\n        raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n    if input_ids is not None:\n        self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n        input_shape = input_ids.size()\n    elif inputs_embeds is not None:\n        input_shape = inputs_embeds.size()[:-1]\n    else:\n        raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n    batch_size, seq_length = input_shape\n    device = input_ids.device if input_ids is not None else inputs_embeds.device  # type: ignore[union-attr]\n\n    # past_key_values_length\n    past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n\n    if attention_mask is None:\n        attention_mask = (\n            input_ids.ne(self.pad_token_id)\n            if self.pad_token_id is not None\n            else torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n        )\n\n    # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n    # ourselves in which case we just need to make it broadcastable to all heads.\n    extended_attention_mask: Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n\n    # If a 2D or 3D attention mask is provided for the cross-attention\n    # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n    if self.config.is_decoder and encoder_hidden_states is not None:\n        encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n        encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n        if encoder_attention_mask is None:\n            encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n        encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n    else:\n        encoder_extended_attention_mask = None\n\n    # Prepare head mask if needed\n    # 1.0 in head_mask indicate we keep the head\n    # attention_probs has shape bsz x n_heads x N x N\n    # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n    # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n    head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n\n    embedding_output = self.embeddings(\n        input_ids=input_ids,\n        position_ids=position_ids,\n        inputs_embeds=inputs_embeds,\n        past_key_values_length=past_key_values_length,\n    )\n    encoder_outputs = self.encoder(\n        embedding_output,\n        attention_mask=extended_attention_mask,\n        head_mask=head_mask,\n        encoder_hidden_states=encoder_hidden_states,\n        encoder_attention_mask=encoder_extended_attention_mask,\n        past_key_values=past_key_values,\n        use_cache=use_cache,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n    sequence_output = encoder_outputs[0]\n    pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n\n    if not return_dict:\n        return (sequence_output, pooled_output) + encoder_outputs[1:]\n\n    return BaseModelOutputWithPoolingAndCrossAttentions(\n        last_hidden_state=sequence_output,\n        pooler_output=pooled_output,\n        past_key_values=encoder_outputs.past_key_values,\n        hidden_states=encoder_outputs.hidden_states,\n        attentions=encoder_outputs.attentions,\n        cross_attentions=encoder_outputs.cross_attentions,\n    )\n</code></pre>"},{"location":"models/utrbert/#multimolecule.models.utrbert.UtrBertPreTrainedModel","title":"<code>UtrBertPreTrainedModel</code>","text":"<p>               Bases: <code>PreTrainedModel</code></p> <p>An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.</p> Source code in <code>multimolecule/models/utrbert/modeling_utrbert.py</code> Python<pre><code>class UtrBertPreTrainedModel(PreTrainedModel):\n    \"\"\"\n    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n    models.\n    \"\"\"\n\n    config_class = UtrBertConfig\n    base_model_prefix = \"utrbert\"\n    supports_gradient_checkpointing = True\n    _no_split_modules = [\"UtrBertLayer\", \"UtrBertEmbeddings\"]\n\n    # Copied from transformers.models.bert.modeling_bert.BertPreTrainedModel._init_weights\n    def _init_weights(self, module: nn.Module):\n        \"\"\"Initialize the weights\"\"\"\n        if isinstance(module, nn.Linear):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n</code></pre>"},{"location":"models/utrlm/","title":"UTR-LM","text":"<p>Pre-trained model on 5\u2019 untranslated region (5\u2019UTR) using masked language modeling (MLM), Secondary Structure (SS), and Minimum Free Energy (MFE) objectives.</p>"},{"location":"models/utrlm/#statement","title":"Statement","text":"<p>A 5\u2019 UTR Language Model for Decoding Untranslated Regions of mRNA and Function Predictions is published in Nature Machine Intelligence, which is a Closed Access / Author-Fee Journal.</p> <p>Machine learning has been at the forefront of the movement for free and open access to research.</p> <p>We see no role for closed access or author-fee publication in the future of machine learning research and believe the adoption of these journals as an outlet of record for the machine learning community would be a retrograde step.</p> <p>The MoltiMolecule team is committed to the principles of open access and open science.</p> <p>We do NOT endorse the publication of manuscripts on Closed Access / Author-Fee Journals and encourage the community to support Open Access Journals.</p> <p>Please consider signing the Statement on Nature Machine Intelligence.</p>"},{"location":"models/utrlm/#disclaimer","title":"Disclaimer","text":"<p>This is an UNOFFICIAL implementation of the A 5\u2019 UTR Language Model for Decoding Untranslated Regions of mRNA and Function Predictions by Yanyi Chu, Dan Yu, et al.</p> <p>The OFFICIAL repository of UTR-LM is at a96123155/UTR-LM.</p> <p>The team releasing UTR-LM did not write this model card for this model so this model card has been written by the MultiMolecule team.</p>"},{"location":"models/utrlm/#model-details","title":"Model Details","text":"<p>UTR-LM is a bert-style model pre-trained on a large corpus of 5\u2019 untranslated regions (5\u2019UTRs) in a self-supervised fashion. This means that the model was trained on the raw nucleotides of RNA sequences only, with an automatic process to generate inputs and labels from those texts. Please refer to the Training Details section for more information on the training process.</p>"},{"location":"models/utrlm/#variations","title":"Variations","text":"<ul> <li><code>multimolecule/utrlm.te_el</code>: The UTR-LM model for Translation Efficiency of transcripts and mRNA Expression Level.</li> <li><code>multimolecule/utrlm.mrl</code>: The UTR-LM model for Mean Ribosome Loading.</li> </ul>"},{"location":"models/utrlm/#model-specification","title":"Model Specification","text":"Variants Num Layers Hidden Size Num Heads Intermediate Size Num Parameters (M) FLOPs (G) MACs (G) Max Num Tokens UTR-LM MRL 6 128 16 512 1.21 0.35 0.18 1022 UTR-LM TE_EL"},{"location":"models/utrlm/#links","title":"Links","text":"<ul> <li>Code: multimolecule.utrlm</li> <li>Data:</li> <li>Ensembl Genome Browser</li> <li>Human 5\u2032 UTR design and variant effect prediction from a massively parallel translation assay</li> <li>High-Throughput 5\u2019 UTR Engineering for Enhanced Protein Production in Non-Viral Gene Therapies</li> <li>Paper: A 5\u2019 UTR Language Model for Decoding Untranslated Regions of mRNA and Function Predictions</li> <li>Developed by: Yanyi Chu, Dan Yu, Yupeng Li, Kaixuan Huang, Yue Shen, Le Cong, Jason Zhang, Mengdi Wang</li> <li>Model type: BERT - ESM</li> <li>Original Repository: https://github.com/a96123155/UTR-LM</li> </ul>"},{"location":"models/utrlm/#usage","title":"Usage","text":"<p>The model file depends on the <code>multimolecule</code> library. You can install it using pip:</p> Bash<pre><code>pip install multimolecule\n</code></pre>"},{"location":"models/utrlm/#direct-use","title":"Direct Use","text":"<p>You can use this model directly with a pipeline for masked language modeling:</p> Python<pre><code>&gt;&gt;&gt; import multimolecule  # you must import multimolecule to register models\n&gt;&gt;&gt; from transformers import pipeline\n&gt;&gt;&gt; unmasker = pipeline('fill-mask', model='multimolecule/utrlm.te_el')\n&gt;&gt;&gt; unmasker(\"uagc&lt;mask&gt;uaucagacugauguuga\")\n\n[{'score': 0.062397848814725876,\n  'token': 12,\n  'token_str': 'V',\n  'sequence': 'U A G C V U A U C A G A C U G A U G U U G A'},\n {'score': 0.059949371963739395,\n  'token': 3,\n  'token_str': '&lt;unk&gt;',\n  'sequence': 'U A G C U A U C A G A C U G A U G U U G A'},\n {'score': 0.05202353373169899,\n  'token': 19,\n  'token_str': 'S',\n  'sequence': 'U A G C S U A U C A G A C U G A U G U U G A'},\n {'score': 0.05004888400435448,\n  'token': 4,\n  'token_str': '&lt;mask&gt;',\n  'sequence': 'U A G C U A U C A G A C U G A U G U U G A'},\n {'score': 0.04615812376141548,\n  'token': 23,\n  'token_str': '*',\n  'sequence': 'U A G C * U A U C A G A C U G A U G U U G A'}]\n</code></pre>"},{"location":"models/utrlm/#downstream-use","title":"Downstream Use","text":""},{"location":"models/utrlm/#extract-features","title":"Extract Features","text":"<p>Here is how to use this model to get the features of a given sequence in PyTorch:</p> Python<pre><code>from multimolecule import RnaTokenizer, UtrLmModel\n\n\ntokenizer = RnaTokenizer.from_pretrained('multimolecule/utrlm.te_el')\nmodel = UtrLmModel.from_pretrained('multimolecule/utrlm.te_el')\n\ntext = \"UAGCUUAUCAGACUGAUGUUGA\"\ninput = tokenizer(text, return_tensors='pt')\n\noutput = model(**input)\n</code></pre>"},{"location":"models/utrlm/#sequence-classification-regression","title":"Sequence Classification / Regression","text":"<p>Note: This model is not fine-tuned for any specific task. You will need to fine-tune the model on a downstream task to use it for sequence classification or regression.</p> <p>Here is how to use this model as backbone to fine-tune for a sequence-level task in PyTorch:</p> Python<pre><code>import torch\nfrom multimolecule import RnaTokenizer, UtrLmForSequenceClassification\n\n\ntokenizer = RnaTokenizer.from_pretrained('multimolecule/utrlm.te_el')\nmodel = UtrLmModelForSequenceClassification.from_pretrained('multimolecule/utrlm.te_el')\n\ntext = \"UAGCUUAUCAGACUGAUGUUGA\"\ninput = tokenizer(text, return_tensors='pt')\nlabel = torch.tensor([1])\n\noutput = model(**input, labels=label)\n</code></pre>"},{"location":"models/utrlm/#nucleotide-classification-regression","title":"Nucleotide Classification / Regression","text":"<p>Note: This model is not fine-tuned for any specific task. You will need to fine-tune the model on a downstream task to use it for nucleotide classification or regression.</p> <p>Here is how to use this model as backbone to fine-tune for a nucleotide-level task in PyTorch:</p> Python<pre><code>import torch\nfrom multimolecule import RnaTokenizer, UtrLmForNucleotideClassification\n\n\ntokenizer = RnaTokenizer.from_pretrained('multimolecule/utrlm.te_el')\nmodel = UtrLmModelForNucleotideClassification.from_pretrained('multimolecule/utrlm.te_el')\n\ntext = \"UAGCUUAUCAGACUGAUGUUGA\"\ninput = tokenizer(text, return_tensors='pt')\nlabel = torch.randint(2, (len(text), ))\n\noutput = model(**input, labels=label)\n</code></pre>"},{"location":"models/utrlm/#training-details","title":"Training Details","text":"<p>UTR-LM used a mixed training strategy with one self-supervised task and two supervised tasks, where the labels of both supervised tasks are calculated using [ViennaRNA](https://viennarna.readthedocs.io.</p> <ol> <li>Masked Language Modeling (MLM): taking a sequence, the model randomly masks 15% of the tokens in the input then run the entire masked sentence through the model and has to predict the masked tokens. This is comparable to the Cloze task in language modeling.</li> <li>Secondary Structure (SS): predicting the secondary structure of the <code>&lt;mask&gt;</code> token in the MLM task.</li> <li>Minimum Free Energy (MFE): predicting the minimum free energy of the 5\u2019 UTR sequence.</li> </ol>"},{"location":"models/utrlm/#training-data","title":"Training Data","text":"<p>The UTR-LM model was pre-trained on 5\u2019 UTR sequences from three sources:</p> <ul> <li>Ensembl Genome Browser: Ensembl is a genome browser for vertebrate genomes that supports research in comparative genomics, evolution, sequence variation and transcriptional regulation. UTR-LM used 5\u2019 UTR sequences from 5 species: human, rat, mouse, chicken, and zebrafish, since these species have high-quality and manual gene annotations.</li> <li>Human 5\u2032 UTR design and variant effect prediction from a massively parallel translation assay: Sample et al. proposed 8 distinct 5\u2019 UTR libraries, each containing random 50 nucleotide sequences, to evaluate translation rules using mean ribosome loading (MRL) measurements.</li> <li>High-Throughput 5\u2019 UTR Engineering for Enhanced Protein Production in Non-Viral Gene Therapies: Cao et al. analyzed endogenous human 5\u2019 UTRs, including data from 3 distinct cell lines/tissues: human embryonic kidney 293T (HEK), human prostate cancer cell (PC3), and human muscle tissue (Muscle).</li> </ul> <p>UTR-LM preprocessed the 5\u2019 UTR sequences in a 4-step pipeline:</p> <ol> <li>removed all coding sequence (CDS) and non-5\u2019 UTR fragments from the raw sequences.</li> <li>identified and removed duplicate sequences</li> <li>truncated the sequences to fit within a range of 30 to 1022 bp</li> <li>filtered out incorrect and low-quality sequences</li> </ol> <p>Note <code>RnaTokenizer</code> will convert \u201cT\u201ds to \u201cU\u201ds for you, you may disable this behaviour by passing <code>replace_T_with_U=False</code>.</p>"},{"location":"models/utrlm/#training-procedure","title":"Training Procedure","text":""},{"location":"models/utrlm/#preprocessing","title":"Preprocessing","text":"<p>UTR-LM used masked language modeling (MLM) as one of the pre-training objectives. The masking procedure is similar to the one used in BERT:</p> <ul> <li>15% of the tokens are masked.</li> <li>In 80% of the cases, the masked tokens are replaced by <code>&lt;mask&gt;</code>.</li> <li>In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.</li> <li>In the 10% remaining cases, the masked tokens are left as is.</li> </ul>"},{"location":"models/utrlm/#pretraining","title":"Pretraining","text":"<p>The model was trained on two clusters:</p> <ol> <li>4 NVIDIA V100 GPUs with 16GiB memories.</li> <li>4 NVIDIA P100 GPUs with 32GiB memories.</li> </ol>"},{"location":"models/utrlm/#citation","title":"Citation","text":"<p>BibTeX:</p> BibTeX<pre><code>@article {chu2023a,\n    author = {Chu, Yanyi and Yu, Dan and Li, Yupeng and Huang, Kaixuan and Shen, Yue and Cong, Le and Zhang, Jason and Wang, Mengdi},\n    title = {A 5{\\textquoteright} UTR Language Model for Decoding Untranslated Regions of mRNA and Function Predictions},\n    elocation-id = {2023.10.11.561938},\n    year = {2023},\n    doi = {10.1101/2023.10.11.561938},\n    publisher = {Cold Spring Harbor Laboratory},\n    abstract = {The 5{\\textquoteright} UTR, a regulatory region at the beginning of an mRNA molecule, plays a crucial role in regulating the translation process and impacts the protein expression level. Language models have showcased their effectiveness in decoding the functions of protein and genome sequences. Here, we introduced a language model for 5{\\textquoteright} UTR, which we refer to as the UTR-LM. The UTR-LM is pre-trained on endogenous 5{\\textquoteright} UTRs from multiple species and is further augmented with supervised information including secondary structure and minimum free energy. We fine-tuned the UTR-LM in a variety of downstream tasks. The model outperformed the best-known benchmark by up to 42\\% for predicting the Mean Ribosome Loading, and by up to 60\\% for predicting the Translation Efficiency and the mRNA Expression Level. The model also applies to identifying unannotated Internal Ribosome Entry Sites within the untranslated region and improves the AUPR from 0.37 to 0.52 compared to the best baseline. Further, we designed a library of 211 novel 5{\\textquoteright} UTRs with high predicted values of translation efficiency and evaluated them via a wet-lab assay. Experiment results confirmed that our top designs achieved a 32.5\\% increase in protein production level relative to well-established 5{\\textquoteright} UTR optimized for therapeutics.Competing Interest StatementThe authors have declared no competing interest.},\n    URL = {https://www.biorxiv.org/content/early/2023/10/14/2023.10.11.561938},\n    eprint = {https://www.biorxiv.org/content/early/2023/10/14/2023.10.11.561938.full.pdf},\n    journal = {bioRxiv}\n}\n</code></pre>"},{"location":"models/utrlm/#contact","title":"Contact","text":"<p>Please use GitHub issues of MultiMolecule for any questions or comments on the model card.</p> <p>Please contact the authors of the UTR-LM paper for questions or comments on the paper/model.</p>"},{"location":"models/utrlm/#license","title":"License","text":"<p>This model is licensed under the AGPL-3.0 License.</p> Text Only<pre><code>SPDX-License-Identifier: AGPL-3.0-or-later\n</code></pre>"},{"location":"models/utrlm/#multimolecule.models.utrlm","title":"<code>multimolecule.models.utrlm</code>","text":""},{"location":"models/utrlm/#multimolecule.models.utrlm.RnaTokenizer","title":"<code>RnaTokenizer</code>","text":"<p>               Bases: <code>Tokenizer</code></p> <p>Constructs an RNA tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>alphabet</code> <code>List[str] | None</code> <p>List of tokens to use. Defaults to IUPAC nucleotide code.</p> <code>None</code> <code>nmers</code> <code>int</code> <p>Size of nmers to tokenize. Defaults to 1.</p> <code>1</code> <code>codon</code> <code>bool</code> <p>Whether to tokenize into codons. Defaults to False.</p> <code>False</code> <code>replace_T_with_U</code> <code>bool</code> <p>Whether to replace T with U. Defaults to True.</p> <code>True</code> <code>do_upper_case</code> <code>bool</code> <p>Whether to convert input to uppercase. Defaults to True.</p> <code>True</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import RnaTokenizer\n&gt;&gt;&gt; tokenizer = RnaTokenizer()\n&gt;&gt;&gt; tokenizer('&lt;pad&gt;&lt;cls&gt;&lt;eos&gt;&lt;unk&gt;&lt;mask&gt;&lt;null&gt;ACGUNXVHDBMRWSYK.*-')[\"input_ids\"]\n[1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 2]\n&gt;&gt;&gt; tokenizer('acgu')[\"input_ids\"]\n[1, 6, 7, 8, 9, 2]\n&gt;&gt;&gt; tokenizer('acgt')[\"input_ids\"]\n[1, 6, 7, 8, 9, 2]\n&gt;&gt;&gt; tokenizer = RnaTokenizer(replace_T_with_U=False)\n&gt;&gt;&gt; tokenizer('acgt')[\"input_ids\"]\n[1, 6, 7, 8, 3, 2]\n&gt;&gt;&gt; tokenizer = RnaTokenizer(nmers=3)\n&gt;&gt;&gt; tokenizer('uagcuuauc')[\"input_ids\"]\n[1, 83, 17, 64, 49, 96, 84, 22, 2]\n&gt;&gt;&gt; tokenizer = RnaTokenizer(codon=True)\n&gt;&gt;&gt; tokenizer('uagcuuauc')[\"input_ids\"]\n[1, 83, 49, 22, 2]\n&gt;&gt;&gt; tokenizer('uagcuuauca')[\"input_ids\"]\nTraceback (most recent call last):\nValueError: length of input sequence  must be a multiple of 3 for codon tokenization, but got 10\n</code></pre> Source code in <code>multimolecule/tokenisers/rna/tokenization_rna.py</code> Python<pre><code>class RnaTokenizer(Tokenizer):\n    \"\"\"\n    Constructs an RNA tokenizer.\n\n    Args:\n        alphabet (List[str] | None, optional): List of tokens to use.\n            Defaults to [IUPAC nucleotide code](https://www.bioinformatics.org/sms2/iupac.html).\n        nmers (int, optional): Size of nmers to tokenize.\n            Defaults to 1.\n        codon (bool, optional): Whether to tokenize into codons.\n            Defaults to False.\n        replace_T_with_U (bool, optional): Whether to replace T with U.\n            Defaults to True.\n        do_upper_case (bool, optional): Whether to convert input to uppercase.\n            Defaults to True.\n\n    Examples:\n        &gt;&gt;&gt; from multimolecule import RnaTokenizer\n        &gt;&gt;&gt; tokenizer = RnaTokenizer()\n        &gt;&gt;&gt; tokenizer('&lt;pad&gt;&lt;cls&gt;&lt;eos&gt;&lt;unk&gt;&lt;mask&gt;&lt;null&gt;ACGUNXVHDBMRWSYK.*-')[\"input_ids\"]\n        [1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 2]\n        &gt;&gt;&gt; tokenizer('acgu')[\"input_ids\"]\n        [1, 6, 7, 8, 9, 2]\n        &gt;&gt;&gt; tokenizer('acgt')[\"input_ids\"]\n        [1, 6, 7, 8, 9, 2]\n        &gt;&gt;&gt; tokenizer = RnaTokenizer(replace_T_with_U=False)\n        &gt;&gt;&gt; tokenizer('acgt')[\"input_ids\"]\n        [1, 6, 7, 8, 3, 2]\n        &gt;&gt;&gt; tokenizer = RnaTokenizer(nmers=3)\n        &gt;&gt;&gt; tokenizer('uagcuuauc')[\"input_ids\"]\n        [1, 83, 17, 64, 49, 96, 84, 22, 2]\n        &gt;&gt;&gt; tokenizer = RnaTokenizer(codon=True)\n        &gt;&gt;&gt; tokenizer('uagcuuauc')[\"input_ids\"]\n        [1, 83, 49, 22, 2]\n        &gt;&gt;&gt; tokenizer('uagcuuauca')[\"input_ids\"]\n        Traceback (most recent call last):\n        ValueError: length of input sequence  must be a multiple of 3 for codon tokenization, but got 10\n    \"\"\"\n\n    model_input_names = [\"input_ids\", \"attention_mask\"]\n\n    def __init__(\n        self,\n        alphabet: List[str] | None = None,\n        nmers: int = 1,\n        codon: bool = False,\n        replace_T_with_U: bool = True,\n        bos_token: str = \"&lt;cls&gt;\",\n        cls_token: str = \"&lt;cls&gt;\",\n        pad_token: str = \"&lt;pad&gt;\",\n        eos_token: str = \"&lt;eos&gt;\",\n        sep_token: str = \"&lt;eos&gt;\",\n        unk_token: str = \"&lt;unk&gt;\",\n        mask_token: str = \"&lt;mask&gt;\",\n        additional_special_tokens: List | Tuple | None = None,\n        do_upper_case: bool = True,\n        **kwargs,\n    ):\n        if codon and nmers &gt; 1:\n            raise ValueError(\"Codon and nmers cannot be used together.\")\n        if codon:\n            nmers = 3  # set to 3 to get correct vocab\n        super().__init__(\n            alphabet=get_vocab_list(alphabet, nmers),\n            bos_token=bos_token,\n            cls_token=cls_token,\n            pad_token=pad_token,\n            eos_token=eos_token,\n            sep_token=sep_token,\n            unk_token=unk_token,\n            mask_token=mask_token,\n            additional_special_tokens=additional_special_tokens,\n            do_upper_case=do_upper_case,\n            **kwargs,\n        )\n        self.replace_T_with_U = replace_T_with_U\n        self.nmers = nmers\n        self.condon = codon\n\n    def _tokenize(self, text: str, **kwargs):\n        if self.do_upper_case:\n            text = text.upper()\n        if self.replace_T_with_U:\n            text = text.replace(\"T\", \"U\")\n        if self.condon:\n            if len(text) % 3 != 0:\n                raise ValueError(\n                    f\"length of input sequence  must be a multiple of 3 for codon tokenization, but got {len(text)}\"\n                )\n            return [text[i : i + 3] for i in range(0, len(text), 3)]\n        if self.nmers &gt; 1:\n            return [text[i : i + self.nmers] for i in range(len(text) - self.nmers + 1)]  # noqa: E203\n        return list(text)\n</code></pre>"},{"location":"models/utrlm/#multimolecule.models.utrlm.UtrLmConfig","title":"<code>UtrLmConfig</code>","text":"<p>               Bases: <code>PreTrainedConfig</code></p> <p>This is the configuration class to store the configuration of a [<code>UtrLmModel</code>]. It is used to instantiate a RNA-FM model according to the specified arguments, defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of the RNA-FM a96123155/UTR-LM architecture.</p> <p>Configuration objects inherit from [<code>PreTrainedConfig</code>] and can be used to control the model outputs. Read the documentation from [<code>PreTrainedConfig</code>] for more information.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_size</code> <code>`int`, *optional*, defaults to 25</code> <p>Vocabulary size of the RNA-FM model. Defines the number of different tokens that can be represented by the <code>inputs_ids</code> passed when calling [<code>UtrLmModel</code>].</p> <code>25</code> <code>hidden_size</code> <code>`int`, *optional*, defaults to 512</code> <p>Dimensionality of the encoder layers and the pooler layer.</p> <code>128</code> <code>num_hidden_layers</code> <code>`int`, *optional*, defaults to 6</code> <p>Number of hidden layers in the Transformer encoder.</p> <code>6</code> <code>num_attention_heads</code> <code>`int`, *optional*, defaults to 12</code> <p>Number of attention heads for each attention layer in the Transformer encoder.</p> <code>16</code> <code>intermediate_size</code> <code>`int`, *optional*, defaults to 2048</code> <p>Dimensionality of the \u201cintermediate\u201d (often named feed-forward) layer in the Transformer encoder.</p> <code>512</code> <code>hidden_dropout</code> <code>`float`, *optional*, defaults to 0.1</code> <p>The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.</p> <code>0.1</code> <code>attention_dropout</code> <code>`float`, *optional*, defaults to 0.1</code> <p>The dropout ratio for the attention probabilities.</p> <code>0.1</code> <code>max_position_embeddings</code> <code>`int`, *optional*, defaults to 1026</code> <p>The maximum sequence length that this model might ever be used with. Typically set this to something large just in case (e.g., 512 or 1024 or 2048).</p> <code>1026</code> <code>initializer_range</code> <code>`float`, *optional*, defaults to 0.02</code> <p>The standard deviation of the truncated_normal_initializer for initializing all weight matrices.</p> <code>0.02</code> <code>layer_norm_eps</code> <code>`float`, *optional*, defaults to 1e-12</code> <p>The epsilon used by the layer normalization layers.</p> <code>1e-12</code> <code>pad_token_id</code> <code>`int`, *optional*, defaults to 0</code> <p>The index of the padding token in the vocabulary. This must be included in the config because certain parts of the RnaBert code use this instead of the attention mask.</p> required <code>bos_token_id</code> <code>`int`, *optional*, defaults to 1</code> <p>The index of the bos token in the vocabulary. This must be included in the config because of the contact and other prediction heads removes the bos and padding token when predicting outputs.</p> required <code>mask_token_id</code> <code>`int`, *optional*, defaults to 4</code> <p>The index of the mask token in the vocabulary. This must be included in the config because of the \u201cmask-dropout\u201d scaling trick, which will scale the inputs depending on the number of masked tokens.</p> required <code>position_embedding_type</code> <code>`str`, *optional*, defaults to `\"absolute\"`</code> <p>Type of position embedding. Choose one of <code>\"absolute\"</code>, <code>\"relative_key\"</code>, <code>\"relative_key_query\", \"rotary\"</code>. For positional embeddings use <code>\"absolute\"</code>. For more information on <code>\"relative_key\"</code>, please refer to Self-Attention with Relative Position Representations (Shaw et al.). For more information on <code>\"relative_key_query\"</code>, please refer to Method 4 in Improve Transformer Models with Better Relative Position Embeddings (Huang et al.).</p> <code>'rotary'</code> <code>is_decoder</code> <code>`bool`, *optional*, defaults to `False`</code> <p>Whether the model is used as a decoder or not. If <code>False</code>, the model is used as an encoder.</p> required <code>use_cache</code> <code>`bool`, *optional*, defaults to `True`</code> <p>Whether or not the model should return the last key/values attentions (not used by all models). Only relevant if <code>config.is_decoder=True</code>.</p> <code>True</code> <code>emb_layer_norm_before</code> <code>`bool`, *optional*</code> <p>Whether to apply layer normalization after embeddings but before the main stem of the network.</p> <code>None</code> <code>token_dropout</code> <code>`bool`, defaults to `False`</code> <p>When this is enabled, masked tokens are treated as if they had been dropped out by input dropout.</p> <code>True</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import UtrLmModel, UtrLmConfig\n</code></pre> Python Console Session<pre><code>&gt;&gt;&gt; # Initializing a UTR-LM multimolecule/utrlm style configuration\n&gt;&gt;&gt; configuration = UtrLmConfig()\n</code></pre> Python Console Session<pre><code>&gt;&gt;&gt; # Initializing a model (with random weights) from the multimolecule/utrlm style configuration\n&gt;&gt;&gt; model = UtrLmModel(configuration)\n</code></pre> Python Console Session<pre><code>&gt;&gt;&gt; # Accessing the model configuration\n&gt;&gt;&gt; configuration = model.config\n</code></pre> Source code in <code>multimolecule/models/utrlm/configuration_utrlm.py</code> Python<pre><code>class UtrLmConfig(PreTrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`UtrLmModel`]. It is used to instantiate a RNA-FM\n    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n    defaults will yield a similar configuration to that of the RNA-FM\n    [a96123155/UTR-LM](https://github.com/a96123155/UTR-LM) architecture.\n\n    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n    documentation from [`PreTrainedConfig`] for more information.\n\n\n    Args:\n        vocab_size (`int`, *optional*, defaults to 25):\n            Vocabulary size of the RNA-FM model. Defines the number of different tokens that can be represented by the\n            `inputs_ids` passed when calling [`UtrLmModel`].\n        hidden_size (`int`, *optional*, defaults to 512):\n            Dimensionality of the encoder layers and the pooler layer.\n        num_hidden_layers (`int`, *optional*, defaults to 6):\n            Number of hidden layers in the Transformer encoder.\n        num_attention_heads (`int`, *optional*, defaults to 12):\n            Number of attention heads for each attention layer in the Transformer encoder.\n        intermediate_size (`int`, *optional*, defaults to 2048):\n            Dimensionality of the \"intermediate\" (often named feed-forward) layer in the Transformer encoder.\n        hidden_dropout (`float`, *optional*, defaults to 0.1):\n            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n        attention_dropout (`float`, *optional*, defaults to 0.1):\n            The dropout ratio for the attention probabilities.\n        max_position_embeddings (`int`, *optional*, defaults to 1026):\n            The maximum sequence length that this model might ever be used with. Typically set this to something large\n            just in case (e.g., 512 or 1024 or 2048).\n        initializer_range (`float`, *optional*, defaults to 0.02):\n            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n        layer_norm_eps (`float`, *optional*, defaults to 1e-12):\n            The epsilon used by the layer normalization layers.\n        pad_token_id (`int`, *optional*, defaults to 0):\n            The index of the padding token in the vocabulary. This must be included in the config because certain parts\n            of the RnaBert code use this instead of the attention mask.\n        bos_token_id (`int`, *optional*, defaults to 1):\n            The index of the bos token in the vocabulary. This must be included in the config because of the\n            contact and other prediction heads removes the bos and padding token when predicting outputs.\n        mask_token_id (`int`, *optional*, defaults to 4):\n            The index of the mask token in the vocabulary. This must be included in the config because of the\n            \"mask-dropout\" scaling trick, which will scale the inputs depending on the number of masked tokens.\n        position_embedding_type (`str`, *optional*, defaults to `\"absolute\"`):\n            Type of position embedding. Choose one of `\"absolute\"`, `\"relative_key\"`, `\"relative_key_query\", \"rotary\"`.\n            For positional embeddings use `\"absolute\"`. For more information on `\"relative_key\"`, please refer to\n            [Self-Attention with Relative Position Representations (Shaw et al.)](https://arxiv.org/abs/1803.02155).\n            For more information on `\"relative_key_query\"`, please refer to *Method 4* in [Improve Transformer Models\n            with Better Relative Position Embeddings (Huang et al.)](https://arxiv.org/abs/2009.13658).\n        is_decoder (`bool`, *optional*, defaults to `False`):\n            Whether the model is used as a decoder or not. If `False`, the model is used as an encoder.\n        use_cache (`bool`, *optional*, defaults to `True`):\n            Whether or not the model should return the last key/values attentions (not used by all models). Only\n            relevant if `config.is_decoder=True`.\n        emb_layer_norm_before (`bool`, *optional*):\n            Whether to apply layer normalization after embeddings but before the main stem of the network.\n        token_dropout (`bool`, defaults to `False`):\n            When this is enabled, masked tokens are treated as if they had been dropped out by input dropout.\n\n    Examples:\n        &gt;&gt;&gt; from multimolecule import UtrLmModel, UtrLmConfig\n\n        &gt;&gt;&gt; # Initializing a UTR-LM multimolecule/utrlm style configuration\n        &gt;&gt;&gt; configuration = UtrLmConfig()\n\n        &gt;&gt;&gt; # Initializing a model (with random weights) from the multimolecule/utrlm style configuration\n        &gt;&gt;&gt; model = UtrLmModel(configuration)\n\n        &gt;&gt;&gt; # Accessing the model configuration\n        &gt;&gt;&gt; configuration = model.config\n    \"\"\"\n\n    model_type = \"utrlm\"\n\n    def __init__(\n        self,\n        vocab_size=25,\n        hidden_size=128,\n        num_hidden_layers=6,\n        num_attention_heads=16,\n        intermediate_size=512,\n        hidden_act=\"gelu\",\n        hidden_dropout=0.1,\n        attention_dropout=0.1,\n        max_position_embeddings=1026,\n        initializer_range=0.02,\n        layer_norm_eps=1e-12,\n        position_embedding_type=\"rotary\",\n        use_cache=True,\n        emb_layer_norm_before=None,\n        token_dropout=True,\n        head=None,\n        lm_head=None,\n        ss_head=None,\n        mfe_head=None,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n\n        self.vocab_size = vocab_size\n        self.hidden_size = hidden_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.intermediate_size = intermediate_size\n        self.hidden_act = hidden_act\n        self.hidden_dropout = hidden_dropout\n        self.attention_dropout = attention_dropout\n        self.max_position_embeddings = max_position_embeddings\n        self.initializer_range = initializer_range\n        self.layer_norm_eps = layer_norm_eps\n        self.position_embedding_type = position_embedding_type\n        self.use_cache = use_cache\n        self.emb_layer_norm_before = emb_layer_norm_before\n        self.token_dropout = token_dropout\n        self.head = HeadConfig(**head if head is not None else {})\n        self.lm_head = MaskedLMHeadConfig(**lm_head if lm_head is not None else {})\n        self.ss_head = HeadConfig(**ss_head) if ss_head is not None else None\n        self.mfe_head = HeadConfig(**mfe_head) if mfe_head is not None else None\n</code></pre>"},{"location":"models/utrlm/#multimolecule.models.utrlm.UtrLmForMaskedLM","title":"<code>UtrLmForMaskedLM</code>","text":"<p>               Bases: <code>UtrLmPreTrainedModel</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import UtrLmConfig, UtrLmModel, RnaTokenizer\n&gt;&gt;&gt; config = UtrLmConfig()\n&gt;&gt;&gt; model = UtrLmModel(config)\n&gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n&gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n&gt;&gt;&gt; output = model(**input)\n</code></pre> Source code in <code>multimolecule/models/utrlm/modeling_utrlm.py</code> Python<pre><code>class UtrLmForMaskedLM(UtrLmPreTrainedModel):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; from multimolecule import UtrLmConfig, UtrLmModel, RnaTokenizer\n        &gt;&gt;&gt; config = UtrLmConfig()\n        &gt;&gt;&gt; model = UtrLmModel(config)\n        &gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n        &gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n        &gt;&gt;&gt; output = model(**input)\n    \"\"\"\n\n    _tied_weights_keys = [\"lm_head.decoder.weight\"]\n\n    def __init__(self, config: UtrLmConfig):\n        super().__init__(config)\n        if config.is_decoder:\n            logger.warning(\n                \"If you want to use `UtrLmForMaskedLM` make sure `config.is_decoder=False` for \"\n                \"bi-directional self-attention.\"\n            )\n        self.utrlm = UtrLmModel(config, add_pooling_layer=False)\n        self.lm_head = MaskedLMHead(config)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(\n        self,\n        input_ids: Tensor | NestedTensor,\n        attention_mask: Tensor | None = None,\n        position_ids: Tensor | None = None,\n        head_mask: Tensor | None = None,\n        inputs_embeds: Tensor | NestedTensor | None = None,\n        encoder_hidden_states: Tensor | None = None,\n        encoder_attention_mask: Tensor | None = None,\n        labels: Tensor | None = None,\n        output_attentions: bool | None = None,\n        output_hidden_states: bool | None = None,\n        return_dict: bool | None = None,\n    ) -&gt; Tuple[Tensor, ...] | MaskedLMOutput:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n            config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n            loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n        \"\"\"\n\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.utrlm(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        output = self.lm_head(outputs, labels)\n        logits, loss = output.logits, output.loss\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return MaskedLMOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"models/utrlm/#multimolecule.models.utrlm.UtrLmForMaskedLM.forward","title":"<code>forward(input_ids, attention_mask=None, position_ids=None, head_mask=None, inputs_embeds=None, encoder_hidden_states=None, encoder_attention_mask=None, labels=None, output_attentions=None, output_hidden_states=None, return_dict=None)</code>","text":"<p>labels (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, optional):     Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ...,     config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the     loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code></p> Source code in <code>multimolecule/models/utrlm/modeling_utrlm.py</code> Python<pre><code>def forward(\n    self,\n    input_ids: Tensor | NestedTensor,\n    attention_mask: Tensor | None = None,\n    position_ids: Tensor | None = None,\n    head_mask: Tensor | None = None,\n    inputs_embeds: Tensor | NestedTensor | None = None,\n    encoder_hidden_states: Tensor | None = None,\n    encoder_attention_mask: Tensor | None = None,\n    labels: Tensor | None = None,\n    output_attentions: bool | None = None,\n    output_hidden_states: bool | None = None,\n    return_dict: bool | None = None,\n) -&gt; Tuple[Tensor, ...] | MaskedLMOutput:\n    r\"\"\"\n    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n        Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n        config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n        loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n    \"\"\"\n\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n    outputs = self.utrlm(\n        input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        head_mask=head_mask,\n        inputs_embeds=inputs_embeds,\n        encoder_hidden_states=encoder_hidden_states,\n        encoder_attention_mask=encoder_attention_mask,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n    output = self.lm_head(outputs, labels)\n    logits, loss = output.logits, output.loss\n\n    if not return_dict:\n        output = (logits,) + outputs[2:]\n        return ((loss,) + output) if loss is not None else output\n\n    return MaskedLMOutput(\n        loss=loss,\n        logits=logits,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"models/utrlm/#multimolecule.models.utrlm.UtrLmForNucleotideClassification","title":"<code>UtrLmForNucleotideClassification</code>","text":"<p>               Bases: <code>UtrLmPreTrainedModel</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import UtrLmConfig, UtrLmForNucleotideClassification, RnaTokenizer\n&gt;&gt;&gt; config = UtrLmConfig()\n&gt;&gt;&gt; model = UtrLmForNucleotideClassification(config)\n&gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n&gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n&gt;&gt;&gt; output = model(**input)\n</code></pre> Source code in <code>multimolecule/models/utrlm/modeling_utrlm.py</code> Python<pre><code>class UtrLmForNucleotideClassification(UtrLmPreTrainedModel):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; from multimolecule import UtrLmConfig, UtrLmForNucleotideClassification, RnaTokenizer\n        &gt;&gt;&gt; config = UtrLmConfig()\n        &gt;&gt;&gt; model = UtrLmForNucleotideClassification(config)\n        &gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n        &gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n        &gt;&gt;&gt; output = model(**input)\n    \"\"\"\n\n    def __init__(self, config: UtrLmConfig):\n        super().__init__(config)\n        self.num_labels = config.head.num_labels\n        self.utrlm = UtrLmModel(config, add_pooling_layer=False)\n        self.nucleotide_head = NucleotideClassificationHead(config)\n        self.head_config = self.nucleotide_head.config\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(\n        self,\n        input_ids: Tensor | NestedTensor,\n        attention_mask: Tensor | None = None,\n        position_ids: Tensor | None = None,\n        head_mask: Tensor | None = None,\n        inputs_embeds: Tensor | NestedTensor | None = None,\n        labels: Tensor | None = None,\n        output_attentions: bool | None = None,\n        output_hidden_states: bool | None = None,\n        return_dict: bool | None = None,\n        **kwargs,\n    ) -&gt; Tuple[Tensor, ...] | TokenClassifierOutput:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n            `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        if kwargs:\n            warn(\n                f\"Additional keyword arguments `{', '.join(kwargs)}` are detected in \"\n                f\"`{self.__class__.__name__}.forward`, they will be ignored.\\n\"\n                \"This is provided for backward compatibility and may lead to unexpected behavior.\"\n            )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.utrlm(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        output = self.nucleotide_head(outputs, attention_mask, input_ids, labels)\n        logits, loss = output.logits, output.loss\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return TokenClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"models/utrlm/#multimolecule.models.utrlm.UtrLmForNucleotideClassification.forward","title":"<code>forward(input_ids, attention_mask=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None, output_attentions=None, output_hidden_states=None, return_dict=None, **kwargs)</code>","text":"<p>labels (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, optional):     Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ...,     config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If     <code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).</p> Source code in <code>multimolecule/models/utrlm/modeling_utrlm.py</code> Python<pre><code>def forward(\n    self,\n    input_ids: Tensor | NestedTensor,\n    attention_mask: Tensor | None = None,\n    position_ids: Tensor | None = None,\n    head_mask: Tensor | None = None,\n    inputs_embeds: Tensor | NestedTensor | None = None,\n    labels: Tensor | None = None,\n    output_attentions: bool | None = None,\n    output_hidden_states: bool | None = None,\n    return_dict: bool | None = None,\n    **kwargs,\n) -&gt; Tuple[Tensor, ...] | TokenClassifierOutput:\n    r\"\"\"\n    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n        `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).\n    \"\"\"\n    if kwargs:\n        warn(\n            f\"Additional keyword arguments `{', '.join(kwargs)}` are detected in \"\n            f\"`{self.__class__.__name__}.forward`, they will be ignored.\\n\"\n            \"This is provided for backward compatibility and may lead to unexpected behavior.\"\n        )\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n    outputs = self.utrlm(\n        input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        head_mask=head_mask,\n        inputs_embeds=inputs_embeds,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n    output = self.nucleotide_head(outputs, attention_mask, input_ids, labels)\n    logits, loss = output.logits, output.loss\n\n    if not return_dict:\n        output = (logits,) + outputs[2:]\n        return ((loss,) + output) if loss is not None else output\n\n    return TokenClassifierOutput(\n        loss=loss,\n        logits=logits,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"models/utrlm/#multimolecule.models.utrlm.UtrLmForSequenceClassification","title":"<code>UtrLmForSequenceClassification</code>","text":"<p>               Bases: <code>UtrLmPreTrainedModel</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import UtrLmConfig, UtrLmModel, RnaTokenizer\n&gt;&gt;&gt; config = UtrLmConfig()\n&gt;&gt;&gt; model = UtrLmModel(config)\n&gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n&gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n&gt;&gt;&gt; output = model(**input)\n</code></pre> Source code in <code>multimolecule/models/utrlm/modeling_utrlm.py</code> Python<pre><code>class UtrLmForSequenceClassification(UtrLmPreTrainedModel):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; from multimolecule import UtrLmConfig, UtrLmModel, RnaTokenizer\n        &gt;&gt;&gt; config = UtrLmConfig()\n        &gt;&gt;&gt; model = UtrLmModel(config)\n        &gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n        &gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n        &gt;&gt;&gt; output = model(**input)\n    \"\"\"\n\n    def __init__(self, config: UtrLmConfig):\n        super().__init__(config)\n        self.num_labels = config.head.num_labels\n        self.utrlm = UtrLmModel(config, add_pooling_layer=True)\n        self.sequence_head = SequenceClassificationHead(config)\n        self.head_config = self.sequence_head.config\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(\n        self,\n        input_ids: Tensor | NestedTensor,\n        attention_mask: Tensor | None = None,\n        position_ids: Tensor | None = None,\n        head_mask: Tensor | None = None,\n        inputs_embeds: Tensor | NestedTensor | None = None,\n        labels: Tensor | None = None,\n        output_attentions: bool | None = None,\n        output_hidden_states: bool | None = None,\n        return_dict: bool | None = None,\n    ) -&gt; Tuple[Tensor, ...] | SequenceClassifierOutput:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n            `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.utrlm(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        output = self.sequence_head(outputs, labels)\n        logits, loss = output.logits, output.loss\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"models/utrlm/#multimolecule.models.utrlm.UtrLmForSequenceClassification.forward","title":"<code>forward(input_ids, attention_mask=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None, output_attentions=None, output_hidden_states=None, return_dict=None)</code>","text":"<p>labels (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, optional):     Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ...,     config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If     <code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).</p> Source code in <code>multimolecule/models/utrlm/modeling_utrlm.py</code> Python<pre><code>def forward(\n    self,\n    input_ids: Tensor | NestedTensor,\n    attention_mask: Tensor | None = None,\n    position_ids: Tensor | None = None,\n    head_mask: Tensor | None = None,\n    inputs_embeds: Tensor | NestedTensor | None = None,\n    labels: Tensor | None = None,\n    output_attentions: bool | None = None,\n    output_hidden_states: bool | None = None,\n    return_dict: bool | None = None,\n) -&gt; Tuple[Tensor, ...] | SequenceClassifierOutput:\n    r\"\"\"\n    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n        `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).\n    \"\"\"\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n    outputs = self.utrlm(\n        input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        head_mask=head_mask,\n        inputs_embeds=inputs_embeds,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n    output = self.sequence_head(outputs, labels)\n    logits, loss = output.logits, output.loss\n\n    if not return_dict:\n        output = (logits,) + outputs[2:]\n        return ((loss,) + output) if loss is not None else output\n\n    return SequenceClassifierOutput(\n        loss=loss,\n        logits=logits,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"models/utrlm/#multimolecule.models.utrlm.UtrLmForTokenClassification","title":"<code>UtrLmForTokenClassification</code>","text":"<p>               Bases: <code>UtrLmPreTrainedModel</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import UtrLmConfig, UtrLmModel, RnaTokenizer\n&gt;&gt;&gt; config = UtrLmConfig()\n&gt;&gt;&gt; model = UtrLmModel(config)\n&gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n&gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n&gt;&gt;&gt; output = model(**input)\n</code></pre> Source code in <code>multimolecule/models/utrlm/modeling_utrlm.py</code> Python<pre><code>class UtrLmForTokenClassification(UtrLmPreTrainedModel):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; from multimolecule import UtrLmConfig, UtrLmModel, RnaTokenizer\n        &gt;&gt;&gt; config = UtrLmConfig()\n        &gt;&gt;&gt; model = UtrLmModel(config)\n        &gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n        &gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n        &gt;&gt;&gt; output = model(**input)\n    \"\"\"\n\n    def __init__(self, config: UtrLmConfig):\n        super().__init__(config)\n        self.num_labels = config.head.num_labels\n        self.utrlm = UtrLmModel(config, add_pooling_layer=False)\n        self.token_head = TokenClassificationHead(config)\n        self.head_config = self.token_head.config\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(\n        self,\n        input_ids: Tensor | NestedTensor,\n        attention_mask: Tensor | None = None,\n        position_ids: Tensor | None = None,\n        head_mask: Tensor | None = None,\n        inputs_embeds: Tensor | NestedTensor | None = None,\n        labels: Tensor | None = None,\n        output_attentions: bool | None = None,\n        output_hidden_states: bool | None = None,\n        return_dict: bool | None = None,\n    ) -&gt; Tuple[Tensor, ...] | TokenClassifierOutput:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.utrlm(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        output = self.token_head(outputs, attention_mask, input_ids, labels)\n        logits, loss = output.logits, output.loss\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return TokenClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n</code></pre>"},{"location":"models/utrlm/#multimolecule.models.utrlm.UtrLmForTokenClassification.forward","title":"<code>forward(input_ids, attention_mask=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None, output_attentions=None, output_hidden_states=None, return_dict=None)</code>","text":"<p>labels (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, optional):     Labels for computing the token classification loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>.</p> Source code in <code>multimolecule/models/utrlm/modeling_utrlm.py</code> Python<pre><code>def forward(\n    self,\n    input_ids: Tensor | NestedTensor,\n    attention_mask: Tensor | None = None,\n    position_ids: Tensor | None = None,\n    head_mask: Tensor | None = None,\n    inputs_embeds: Tensor | NestedTensor | None = None,\n    labels: Tensor | None = None,\n    output_attentions: bool | None = None,\n    output_hidden_states: bool | None = None,\n    return_dict: bool | None = None,\n) -&gt; Tuple[Tensor, ...] | TokenClassifierOutput:\n    r\"\"\"\n    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n        Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n    \"\"\"\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n    outputs = self.utrlm(\n        input_ids,\n        attention_mask=attention_mask,\n        position_ids=position_ids,\n        head_mask=head_mask,\n        inputs_embeds=inputs_embeds,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n    output = self.token_head(outputs, attention_mask, input_ids, labels)\n    logits, loss = output.logits, output.loss\n\n    if not return_dict:\n        output = (logits,) + outputs[2:]\n        return ((loss,) + output) if loss is not None else output\n\n    return TokenClassifierOutput(\n        loss=loss,\n        logits=logits,\n        hidden_states=outputs.hidden_states,\n        attentions=outputs.attentions,\n    )\n</code></pre>"},{"location":"models/utrlm/#multimolecule.models.utrlm.UtrLmModel","title":"<code>UtrLmModel</code>","text":"<p>               Bases: <code>UtrLmPreTrainedModel</code></p> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import UtrLmConfig, UtrLmModel, RnaTokenizer\n&gt;&gt;&gt; config = UtrLmConfig()\n&gt;&gt;&gt; model = UtrLmModel(config)\n&gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n&gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n&gt;&gt;&gt; output = model(**input)\n</code></pre> Source code in <code>multimolecule/models/utrlm/modeling_utrlm.py</code> Python<pre><code>class UtrLmModel(UtrLmPreTrainedModel):\n    \"\"\"\n    Examples:\n        &gt;&gt;&gt; from multimolecule import UtrLmConfig, UtrLmModel, RnaTokenizer\n        &gt;&gt;&gt; config = UtrLmConfig()\n        &gt;&gt;&gt; model = UtrLmModel(config)\n        &gt;&gt;&gt; tokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rna\")\n        &gt;&gt;&gt; input = tokenizer(\"ACGUN\", return_tensors=\"pt\")\n        &gt;&gt;&gt; output = model(**input)\n    \"\"\"\n\n    def __init__(self, config: UtrLmConfig, add_pooling_layer: bool = True):\n        super().__init__(config)\n        self.pad_token_id = config.pad_token_id\n        self.embeddings = UtrLmEmbeddings(config)\n        self.encoder = UtrLmEncoder(config)\n        self.pooler = UtrLmPooler(config) if add_pooling_layer else None\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.embeddings.word_embeddings\n\n    def set_input_embeddings(self, value):\n        self.embeddings.word_embeddings = value\n\n    def _prune_heads(self, heads_to_prune):\n        \"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"\n        for layer, heads in heads_to_prune.items():\n            self.encoder.layer[layer].attention.prune_heads(heads)\n\n    def forward(\n        self,\n        input_ids: Tensor | NestedTensor,\n        attention_mask: Tensor | None = None,\n        position_ids: Tensor | None = None,\n        head_mask: Tensor | None = None,\n        inputs_embeds: Tensor | NestedTensor | None = None,\n        encoder_hidden_states: Tensor | None = None,\n        encoder_attention_mask: Tensor | None = None,\n        past_key_values: Tuple[Tuple[torch.FloatTensor, torch.FloatTensor], ...] | None = None,\n        use_cache: bool | None = None,\n        output_attentions: bool | None = None,\n        output_hidden_states: bool | None = None,\n        return_dict: bool | None = None,\n    ) -&gt; Tuple[Tensor, ...] | BaseModelOutputWithPoolingAndCrossAttentions:\n        r\"\"\"\n        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n            the model is configured as a decoder.\n        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors\n            of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n\n            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n            `past_key_values`).\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if self.config.is_decoder:\n            use_cache = use_cache if use_cache is not None else self.config.use_cache\n        else:\n            use_cache = False\n\n        if isinstance(input_ids, NestedTensor):\n            input_ids, attention_mask = input_ids.tensor, input_ids.mask\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n        if input_ids is not None:\n            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n            input_shape = input_ids.size()\n        elif inputs_embeds is not None:\n            input_shape = inputs_embeds.size()[:-1]\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n        batch_size, seq_length = input_shape\n        device = input_ids.device if input_ids is not None else inputs_embeds.device  # type: ignore[union-attr]\n\n        # past_key_values_length\n        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n\n        if attention_mask is None:\n            attention_mask = (\n                input_ids.ne(self.pad_token_id)\n                if self.pad_token_id is not None\n                else torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n            )\n\n        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n        # ourselves in which case we just need to make it broadcastable to all heads.\n        extended_attention_mask: Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n\n        # If a 2D or 3D attention mask is provided for the cross-attention\n        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n        if self.config.is_decoder and encoder_hidden_states is not None:\n            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n            if encoder_attention_mask is None:\n                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n        else:\n            encoder_extended_attention_mask = None\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n\n        embedding_output = self.embeddings(\n            input_ids=input_ids,\n            position_ids=position_ids,\n            attention_mask=attention_mask,\n            inputs_embeds=inputs_embeds,\n            past_key_values_length=past_key_values_length,\n        )\n        encoder_outputs = self.encoder(\n            embedding_output,\n            attention_mask=extended_attention_mask,\n            head_mask=head_mask,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_extended_attention_mask,\n            past_key_values=past_key_values,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        sequence_output = encoder_outputs[0]\n        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n\n        if not return_dict:\n            return (sequence_output, pooled_output) + encoder_outputs[1:]\n\n        return BaseModelOutputWithPoolingAndCrossAttentions(\n            last_hidden_state=sequence_output,\n            pooler_output=pooled_output,\n            past_key_values=encoder_outputs.past_key_values,\n            hidden_states=encoder_outputs.hidden_states,\n            attentions=encoder_outputs.attentions,\n            cross_attentions=encoder_outputs.cross_attentions,\n        )\n</code></pre>"},{"location":"models/utrlm/#multimolecule.models.utrlm.UtrLmModel.forward","title":"<code>forward(input_ids, attention_mask=None, position_ids=None, head_mask=None, inputs_embeds=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_values=None, use_cache=None, output_attentions=None, output_hidden_states=None, return_dict=None)</code>","text":"<p>encoder_hidden_states  (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, optional):     Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if     the model is configured as a decoder. encoder_attention_mask (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, optional):     Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in     the cross-attention if the model is configured as a decoder. Mask values selected in <code>[0, 1]</code>:</p> Text Only<pre><code>- 1 for tokens that are **not masked**,\n- 0 for tokens that are **masked**.\n</code></pre> <p>past_key_values (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors     of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>):     Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.</p> Text Only<pre><code>If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\ndon't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n`decoder_input_ids` of shape `(batch_size, sequence_length)`.\n</code></pre> <p>use_cache (<code>bool</code>, optional):     If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see     <code>past_key_values</code>).</p> Source code in <code>multimolecule/models/utrlm/modeling_utrlm.py</code> Python<pre><code>def forward(\n    self,\n    input_ids: Tensor | NestedTensor,\n    attention_mask: Tensor | None = None,\n    position_ids: Tensor | None = None,\n    head_mask: Tensor | None = None,\n    inputs_embeds: Tensor | NestedTensor | None = None,\n    encoder_hidden_states: Tensor | None = None,\n    encoder_attention_mask: Tensor | None = None,\n    past_key_values: Tuple[Tuple[torch.FloatTensor, torch.FloatTensor], ...] | None = None,\n    use_cache: bool | None = None,\n    output_attentions: bool | None = None,\n    output_hidden_states: bool | None = None,\n    return_dict: bool | None = None,\n) -&gt; Tuple[Tensor, ...] | BaseModelOutputWithPoolingAndCrossAttentions:\n    r\"\"\"\n    encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n        Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n        the model is configured as a decoder.\n    encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n        Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n        the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n\n        - 1 for tokens that are **not masked**,\n        - 0 for tokens that are **masked**.\n    past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors\n        of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n        Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n\n        If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n        don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n        `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n    use_cache (`bool`, *optional*):\n        If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n        `past_key_values`).\n    \"\"\"\n    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n    output_hidden_states = (\n        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n    )\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n    if self.config.is_decoder:\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n    else:\n        use_cache = False\n\n    if isinstance(input_ids, NestedTensor):\n        input_ids, attention_mask = input_ids.tensor, input_ids.mask\n    if input_ids is not None and inputs_embeds is not None:\n        raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n    if input_ids is not None:\n        self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n        input_shape = input_ids.size()\n    elif inputs_embeds is not None:\n        input_shape = inputs_embeds.size()[:-1]\n    else:\n        raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n    batch_size, seq_length = input_shape\n    device = input_ids.device if input_ids is not None else inputs_embeds.device  # type: ignore[union-attr]\n\n    # past_key_values_length\n    past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n\n    if attention_mask is None:\n        attention_mask = (\n            input_ids.ne(self.pad_token_id)\n            if self.pad_token_id is not None\n            else torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n        )\n\n    # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n    # ourselves in which case we just need to make it broadcastable to all heads.\n    extended_attention_mask: Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n\n    # If a 2D or 3D attention mask is provided for the cross-attention\n    # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n    if self.config.is_decoder and encoder_hidden_states is not None:\n        encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n        encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n        if encoder_attention_mask is None:\n            encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n        encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n    else:\n        encoder_extended_attention_mask = None\n\n    # Prepare head mask if needed\n    # 1.0 in head_mask indicate we keep the head\n    # attention_probs has shape bsz x n_heads x N x N\n    # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n    # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n    head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n\n    embedding_output = self.embeddings(\n        input_ids=input_ids,\n        position_ids=position_ids,\n        attention_mask=attention_mask,\n        inputs_embeds=inputs_embeds,\n        past_key_values_length=past_key_values_length,\n    )\n    encoder_outputs = self.encoder(\n        embedding_output,\n        attention_mask=extended_attention_mask,\n        head_mask=head_mask,\n        encoder_hidden_states=encoder_hidden_states,\n        encoder_attention_mask=encoder_extended_attention_mask,\n        past_key_values=past_key_values,\n        use_cache=use_cache,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=return_dict,\n    )\n    sequence_output = encoder_outputs[0]\n    pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n\n    if not return_dict:\n        return (sequence_output, pooled_output) + encoder_outputs[1:]\n\n    return BaseModelOutputWithPoolingAndCrossAttentions(\n        last_hidden_state=sequence_output,\n        pooler_output=pooled_output,\n        past_key_values=encoder_outputs.past_key_values,\n        hidden_states=encoder_outputs.hidden_states,\n        attentions=encoder_outputs.attentions,\n        cross_attentions=encoder_outputs.cross_attentions,\n    )\n</code></pre>"},{"location":"models/utrlm/#multimolecule.models.utrlm.UtrLmPreTrainedModel","title":"<code>UtrLmPreTrainedModel</code>","text":"<p>               Bases: <code>PreTrainedModel</code></p> <p>An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.</p> Source code in <code>multimolecule/models/utrlm/modeling_utrlm.py</code> Python<pre><code>class UtrLmPreTrainedModel(PreTrainedModel):\n    \"\"\"\n    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n    models.\n    \"\"\"\n\n    config_class = UtrLmConfig\n    base_model_prefix = \"utrlm\"\n    supports_gradient_checkpointing = True\n    _no_split_modules = [\"UtrLmLayer\", \"UtrLmEmbeddings\"]\n\n    # Copied from transformers.models.bert.modeling_bert.BertPreTrainedModel._init_weights\n    def _init_weights(self, module: nn.Module):\n        \"\"\"Initialize the weights\"\"\"\n        if isinstance(module, nn.Linear):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n</code></pre>"},{"location":"tokenisers/dna/","title":"DnaTokenizer","text":"<p>DnaTokenizer is smart, it tokenizes raw DNA nucleotides into tokens, no matter if the input is in uppercase or lowercase, uses T (Thymine) or U (Uracil), and with or without special tokens. It also supports tokenization into nmers and codons, so you don\u2019t have to write complex code to preprocess your data.</p> <p>By default, DnaTokenizer uses an extended version of the IUPAC nucleotide code. This extension includes two additional tokens, <code>X</code> and <code>*</code>.</p> <ul> <li><code>X</code>: represents Any base, it is slightly different from <code>N</code> which represents Unknown base.   In automatic word embedding conversion, the <code>X</code> will be initialized as the mean of <code>A</code>, <code>C</code>, <code>G</code>, and <code>T</code>, while <code>N</code> will not be further processed.</li> <li><code>*</code>: is not used in MultiMolecule and is reserved for future use.</li> </ul> <p>If <code>kmers</code> is greater than <code>1</code>, or <code>codon</code> is set to <code>True</code>, the tokenizer will use a minimal alphabet that includes only the five canonical nucleotides <code>A</code>, <code>C</code>, <code>G</code>, <code>T</code>, and <code>N</code>.</p>"},{"location":"tokenisers/dna/#multimolecule.tokenisers.DnaTokenizer","title":"<code>multimolecule.tokenisers.DnaTokenizer</code>","text":"<p>               Bases: <code>Tokenizer</code></p> <p>Constructs a DNA tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>alphabet</code> <code>List[str] | None</code> <p>List of tokens to use. Defaults to IUPAC nucleotide code.</p> <code>None</code> <code>nmers</code> <code>int</code> <p>Size of nmers to tokenize. Defaults to 1.</p> <code>1</code> <code>codon</code> <code>bool</code> <p>Whether to tokenize into codons. Defaults to False.</p> <code>False</code> <code>replace_U_with_T</code> <code>bool</code> <p>Whether to replace U with T. Defaults to True.</p> <code>True</code> <code>do_upper_case</code> <code>bool</code> <p>Whether to convert input to uppercase. Defaults to True.</p> <code>True</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import DnaTokenizer\n&gt;&gt;&gt; tokenizer = DnaTokenizer()\n&gt;&gt;&gt; tokenizer('&lt;pad&gt;&lt;cls&gt;&lt;eos&gt;&lt;unk&gt;&lt;mask&gt;&lt;null&gt;ACGTNXVHDBMRWSYK.*-')[\"input_ids\"]\n[1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 2]\n&gt;&gt;&gt; tokenizer('acgt')[\"input_ids\"]\n[1, 6, 7, 8, 9, 2]\n&gt;&gt;&gt; tokenizer('acgu')[\"input_ids\"]\n[1, 6, 7, 8, 9, 2]\n&gt;&gt;&gt; tokenizer = DnaTokenizer(replace_U_with_T=False)\n&gt;&gt;&gt; tokenizer('acgu')[\"input_ids\"]\n[1, 6, 7, 8, 3, 2]\n&gt;&gt;&gt; tokenizer = DnaTokenizer(nmers=3)\n&gt;&gt;&gt; tokenizer('tataaagta')[\"input_ids\"]\n[1, 84, 21, 81, 6, 8, 19, 71, 2]\n&gt;&gt;&gt; tokenizer = DnaTokenizer(codon=True)\n&gt;&gt;&gt; tokenizer('tataaagta')[\"input_ids\"]\n[1, 84, 6, 71, 2]\n&gt;&gt;&gt; tokenizer('tataaagtaa')[\"input_ids\"]\nTraceback (most recent call last):\nValueError: length of input sequence  must be a multiple of 3 for codon tokenization, but got 10\n</code></pre> Source code in <code>multimolecule/tokenisers/dna/tokenization_dna.py</code> Python<pre><code>class DnaTokenizer(Tokenizer):\n    \"\"\"\n    Constructs a DNA tokenizer.\n\n    Args:\n        alphabet (List[str] | None, optional): List of tokens to use.\n            Defaults to [IUPAC nucleotide code](https://www.bioinformatics.org/sms2/iupac.html).\n        nmers (int, optional): Size of nmers to tokenize.\n            Defaults to 1.\n        codon (bool, optional): Whether to tokenize into codons.\n            Defaults to False.\n        replace_U_with_T (bool, optional): Whether to replace U with T.\n            Defaults to True.\n        do_upper_case (bool, optional): Whether to convert input to uppercase.\n            Defaults to True.\n\n    Examples:\n        &gt;&gt;&gt; from multimolecule import DnaTokenizer\n        &gt;&gt;&gt; tokenizer = DnaTokenizer()\n        &gt;&gt;&gt; tokenizer('&lt;pad&gt;&lt;cls&gt;&lt;eos&gt;&lt;unk&gt;&lt;mask&gt;&lt;null&gt;ACGTNXVHDBMRWSYK.*-')[\"input_ids\"]\n        [1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 2]\n        &gt;&gt;&gt; tokenizer('acgt')[\"input_ids\"]\n        [1, 6, 7, 8, 9, 2]\n        &gt;&gt;&gt; tokenizer('acgu')[\"input_ids\"]\n        [1, 6, 7, 8, 9, 2]\n        &gt;&gt;&gt; tokenizer = DnaTokenizer(replace_U_with_T=False)\n        &gt;&gt;&gt; tokenizer('acgu')[\"input_ids\"]\n        [1, 6, 7, 8, 3, 2]\n        &gt;&gt;&gt; tokenizer = DnaTokenizer(nmers=3)\n        &gt;&gt;&gt; tokenizer('tataaagta')[\"input_ids\"]\n        [1, 84, 21, 81, 6, 8, 19, 71, 2]\n        &gt;&gt;&gt; tokenizer = DnaTokenizer(codon=True)\n        &gt;&gt;&gt; tokenizer('tataaagta')[\"input_ids\"]\n        [1, 84, 6, 71, 2]\n        &gt;&gt;&gt; tokenizer('tataaagtaa')[\"input_ids\"]\n        Traceback (most recent call last):\n        ValueError: length of input sequence  must be a multiple of 3 for codon tokenization, but got 10\n    \"\"\"\n\n    model_input_names = [\"input_ids\", \"attention_mask\"]\n\n    def __init__(\n        self,\n        alphabet: List[str] | None = None,\n        nmers: int = 1,\n        codon: bool = False,\n        bos_token: str = \"&lt;cls&gt;\",\n        cls_token: str = \"&lt;cls&gt;\",\n        pad_token: str = \"&lt;pad&gt;\",\n        eos_token: str = \"&lt;eos&gt;\",\n        sep_token: str = \"&lt;eos&gt;\",\n        unk_token: str = \"&lt;unk&gt;\",\n        mask_token: str = \"&lt;mask&gt;\",\n        additional_special_tokens: List | Tuple | None = None,\n        replace_U_with_T: bool = True,\n        do_upper_case: bool = True,\n        **kwargs,\n    ):\n        if codon and nmers &gt; 1:\n            raise ValueError(\"Codon and nmers cannot be used together.\")\n        if codon:\n            nmers = 3  # set to 3 to get correct vocab\n        super().__init__(\n            alphabet=get_vocab_list(alphabet, nmers),\n            bos_token=bos_token,\n            cls_token=cls_token,\n            pad_token=pad_token,\n            eos_token=eos_token,\n            sep_token=sep_token,\n            unk_token=unk_token,\n            mask_token=mask_token,\n            additional_special_tokens=additional_special_tokens,\n            do_upper_case=do_upper_case,\n            **kwargs,\n        )\n        self.replace_U_with_T = replace_U_with_T\n        self.nmers = nmers\n        self.condon = codon\n\n    def _tokenize(self, text: str, **kwargs):\n        if self.do_upper_case:\n            text = text.upper()\n        if self.replace_U_with_T:\n            text = text.replace(\"U\", \"T\")\n        if self.condon:\n            if len(text) % 3 != 0:\n                raise ValueError(\n                    f\"length of input sequence  must be a multiple of 3 for codon tokenization, but got {len(text)}\"\n                )\n            return [text[i : i + 3] for i in range(0, len(text), 3)]\n        if self.nmers &gt; 1:\n            return [text[i : i + self.nmers] for i in range(len(text) - self.nmers + 1)]  # noqa: E203\n        return list(text)\n</code></pre>"},{"location":"tokenisers/protein/","title":"ProteinTokenizer","text":"<p>ProteinTokenizer is smart, it tokenizes raw amino acids into tokens, no matter if the input is in uppercase or lowercase, and with or without special tokens.</p> <p>By default, ProteinTokenizer uses an extended version of the IUPAC amino acid code. This extension includes nine additional tokens, <code>X</code>, <code>B</code>, <code>Z</code>, <code>J</code>, <code>U</code>, <code>O</code>, <code>.</code>, <code>-</code>, and <code>*</code>.</p> <ul> <li><code>X</code>: Xxx; Any or unknown amino acid</li> <li><code>B</code>: Asx; Aspartic acid (R) or Asparagine (N)</li> <li><code>Z</code>: Glx; Glutamic acid (E) or Glutamine (Q)</li> <li><code>J</code>: Xle; Leucine (L) or Isoleucine (I)</li> <li><code>U</code>: Sec; Selenocysteine</li> <li><code>O</code>: Pyl; Pyrrolysine</li> <li><code>.</code>: is not used in MultiMolecule and is reserved for future use.</li> <li><code>-</code>: is not used in MultiMolecule and is reserved for future use.</li> <li><code>*</code>: is not used in MultiMolecule and is reserved for future use.</li> </ul>"},{"location":"tokenisers/protein/#multimolecule.tokenisers.ProteinTokenizer","title":"<code>multimolecule.tokenisers.ProteinTokenizer</code>","text":"<p>               Bases: <code>Tokenizer</code></p> <p>Constructs a Protein tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>alphabet</code> <code>List[str] | None</code> <p>List of tokens to use. Defaults to IUPAC nucleotide code.</p> <code>None</code> <code>do_upper_case</code> <code>bool</code> <p>Whether to convert input to uppercase. Defaults to True.</p> <code>True</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import ProteinTokenizer\n&gt;&gt;&gt; tokenizer = ProteinTokenizer()\n&gt;&gt;&gt; tokenizer('ACDEFGHIKLMNPQRSTVWYXBZJUO')[\"input_ids\"]\n[1, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 2]\n&gt;&gt;&gt; tokenizer('&lt;pad&gt;&lt;cls&gt;&lt;eos&gt;&lt;unk&gt;&lt;mask&gt;&lt;null&gt;.*-')[\"input_ids\"]\n[1, 0, 1, 2, 3, 4, 5, 32, 33, 34, 2]\n&gt;&gt;&gt; tokenizer('manlgcwmlv')[\"input_ids\"]\n[1, 16, 6, 17, 15, 11, 7, 24, 16, 15, 23, 2]\n</code></pre> Source code in <code>multimolecule/tokenisers/protein/tokenization_protein.py</code> Python<pre><code>class ProteinTokenizer(Tokenizer):\n    \"\"\"\n    Constructs a Protein tokenizer.\n\n    Args:\n        alphabet (List[str] | None, optional): List of tokens to use.\n            Defaults to [IUPAC nucleotide code](https://www.bioinformatics.org/sms2/iupac.html).\n        do_upper_case (bool, optional): Whether to convert input to uppercase.\n            Defaults to True.\n\n    Examples:\n        &gt;&gt;&gt; from multimolecule import ProteinTokenizer\n        &gt;&gt;&gt; tokenizer = ProteinTokenizer()\n        &gt;&gt;&gt; tokenizer('ACDEFGHIKLMNPQRSTVWYXBZJUO')[\"input_ids\"]\n        [1, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 2]\n        &gt;&gt;&gt; tokenizer('&lt;pad&gt;&lt;cls&gt;&lt;eos&gt;&lt;unk&gt;&lt;mask&gt;&lt;null&gt;.*-')[\"input_ids\"]\n        [1, 0, 1, 2, 3, 4, 5, 32, 33, 34, 2]\n        &gt;&gt;&gt; tokenizer('manlgcwmlv')[\"input_ids\"]\n        [1, 16, 6, 17, 15, 11, 7, 24, 16, 15, 23, 2]\n    \"\"\"\n\n    model_input_names = [\"input_ids\", \"attention_mask\"]\n\n    def __init__(\n        self,\n        alphabet: List[str] | None = None,\n        bos_token: str = \"&lt;cls&gt;\",\n        cls_token: str = \"&lt;cls&gt;\",\n        pad_token: str = \"&lt;pad&gt;\",\n        eos_token: str = \"&lt;eos&gt;\",\n        sep_token: str = \"&lt;eos&gt;\",\n        unk_token: str = \"&lt;unk&gt;\",\n        mask_token: str = \"&lt;mask&gt;\",\n        additional_special_tokens: List | Tuple | None = None,\n        do_upper_case: bool = True,\n        **kwargs,\n    ):\n        super().__init__(\n            alphabet=get_vocab_list(alphabet),\n            bos_token=bos_token,\n            cls_token=cls_token,\n            pad_token=pad_token,\n            eos_token=eos_token,\n            sep_token=sep_token,\n            unk_token=unk_token,\n            mask_token=mask_token,\n            additional_special_tokens=additional_special_tokens,\n            do_upper_case=do_upper_case,\n            **kwargs,\n        )\n\n    def _tokenize(self, text: str, **kwargs):\n        if self.do_upper_case:\n            text = text.upper()\n        return list(text)\n</code></pre>"},{"location":"tokenisers/rna/","title":"RnaTokenizer","text":"<p>RnaTokenizer is smart, it tokenizes raw RNA nucleotides into tokens, no matter if the input is in uppercase or lowercase, uses U (Uracil) or T (Thymine), and with or without special tokens. It also supports tokenization into nmers and codons, so you don\u2019t have to write complex code to preprocess your data.</p> <p>By default, RnaTokenizer uses an extended version of the IUPAC nucleotide code. This extension includes two additional tokens, <code>X</code> and <code>*</code>.</p> <ul> <li><code>X</code>: represents Any base, it is slightly different from <code>N</code> which represents Unknown base.   In automatic word embedding conversion, the <code>X</code> will be initialized as the mean of <code>A</code>, <code>C</code>, <code>G</code>, and <code>U</code>, while <code>N</code> will not be further processed.</li> <li><code>*</code>: is not used in MultiMolecule and is reserved for future use.</li> </ul> <p>If <code>kmers</code> is greater than <code>1</code>, or <code>codon</code> is set to <code>True</code>, the tokenizer will use a minimal alphabet that includes only the five canonical nucleotides <code>A</code>, <code>C</code>, <code>G</code>, <code>U</code>, and <code>N</code>.</p>"},{"location":"tokenisers/rna/#multimolecule.tokenisers.RnaTokenizer","title":"<code>multimolecule.tokenisers.RnaTokenizer</code>","text":"<p>               Bases: <code>Tokenizer</code></p> <p>Constructs an RNA tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>alphabet</code> <code>List[str] | None</code> <p>List of tokens to use. Defaults to IUPAC nucleotide code.</p> <code>None</code> <code>nmers</code> <code>int</code> <p>Size of nmers to tokenize. Defaults to 1.</p> <code>1</code> <code>codon</code> <code>bool</code> <p>Whether to tokenize into codons. Defaults to False.</p> <code>False</code> <code>replace_T_with_U</code> <code>bool</code> <p>Whether to replace T with U. Defaults to True.</p> <code>True</code> <code>do_upper_case</code> <code>bool</code> <p>Whether to convert input to uppercase. Defaults to True.</p> <code>True</code> <p>Examples:</p> Python Console Session<pre><code>&gt;&gt;&gt; from multimolecule import RnaTokenizer\n&gt;&gt;&gt; tokenizer = RnaTokenizer()\n&gt;&gt;&gt; tokenizer('&lt;pad&gt;&lt;cls&gt;&lt;eos&gt;&lt;unk&gt;&lt;mask&gt;&lt;null&gt;ACGUNXVHDBMRWSYK.*-')[\"input_ids\"]\n[1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 2]\n&gt;&gt;&gt; tokenizer('acgu')[\"input_ids\"]\n[1, 6, 7, 8, 9, 2]\n&gt;&gt;&gt; tokenizer('acgt')[\"input_ids\"]\n[1, 6, 7, 8, 9, 2]\n&gt;&gt;&gt; tokenizer = RnaTokenizer(replace_T_with_U=False)\n&gt;&gt;&gt; tokenizer('acgt')[\"input_ids\"]\n[1, 6, 7, 8, 3, 2]\n&gt;&gt;&gt; tokenizer = RnaTokenizer(nmers=3)\n&gt;&gt;&gt; tokenizer('uagcuuauc')[\"input_ids\"]\n[1, 83, 17, 64, 49, 96, 84, 22, 2]\n&gt;&gt;&gt; tokenizer = RnaTokenizer(codon=True)\n&gt;&gt;&gt; tokenizer('uagcuuauc')[\"input_ids\"]\n[1, 83, 49, 22, 2]\n&gt;&gt;&gt; tokenizer('uagcuuauca')[\"input_ids\"]\nTraceback (most recent call last):\nValueError: length of input sequence  must be a multiple of 3 for codon tokenization, but got 10\n</code></pre> Source code in <code>multimolecule/tokenisers/rna/tokenization_rna.py</code> Python<pre><code>class RnaTokenizer(Tokenizer):\n    \"\"\"\n    Constructs an RNA tokenizer.\n\n    Args:\n        alphabet (List[str] | None, optional): List of tokens to use.\n            Defaults to [IUPAC nucleotide code](https://www.bioinformatics.org/sms2/iupac.html).\n        nmers (int, optional): Size of nmers to tokenize.\n            Defaults to 1.\n        codon (bool, optional): Whether to tokenize into codons.\n            Defaults to False.\n        replace_T_with_U (bool, optional): Whether to replace T with U.\n            Defaults to True.\n        do_upper_case (bool, optional): Whether to convert input to uppercase.\n            Defaults to True.\n\n    Examples:\n        &gt;&gt;&gt; from multimolecule import RnaTokenizer\n        &gt;&gt;&gt; tokenizer = RnaTokenizer()\n        &gt;&gt;&gt; tokenizer('&lt;pad&gt;&lt;cls&gt;&lt;eos&gt;&lt;unk&gt;&lt;mask&gt;&lt;null&gt;ACGUNXVHDBMRWSYK.*-')[\"input_ids\"]\n        [1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 2]\n        &gt;&gt;&gt; tokenizer('acgu')[\"input_ids\"]\n        [1, 6, 7, 8, 9, 2]\n        &gt;&gt;&gt; tokenizer('acgt')[\"input_ids\"]\n        [1, 6, 7, 8, 9, 2]\n        &gt;&gt;&gt; tokenizer = RnaTokenizer(replace_T_with_U=False)\n        &gt;&gt;&gt; tokenizer('acgt')[\"input_ids\"]\n        [1, 6, 7, 8, 3, 2]\n        &gt;&gt;&gt; tokenizer = RnaTokenizer(nmers=3)\n        &gt;&gt;&gt; tokenizer('uagcuuauc')[\"input_ids\"]\n        [1, 83, 17, 64, 49, 96, 84, 22, 2]\n        &gt;&gt;&gt; tokenizer = RnaTokenizer(codon=True)\n        &gt;&gt;&gt; tokenizer('uagcuuauc')[\"input_ids\"]\n        [1, 83, 49, 22, 2]\n        &gt;&gt;&gt; tokenizer('uagcuuauca')[\"input_ids\"]\n        Traceback (most recent call last):\n        ValueError: length of input sequence  must be a multiple of 3 for codon tokenization, but got 10\n    \"\"\"\n\n    model_input_names = [\"input_ids\", \"attention_mask\"]\n\n    def __init__(\n        self,\n        alphabet: List[str] | None = None,\n        nmers: int = 1,\n        codon: bool = False,\n        replace_T_with_U: bool = True,\n        bos_token: str = \"&lt;cls&gt;\",\n        cls_token: str = \"&lt;cls&gt;\",\n        pad_token: str = \"&lt;pad&gt;\",\n        eos_token: str = \"&lt;eos&gt;\",\n        sep_token: str = \"&lt;eos&gt;\",\n        unk_token: str = \"&lt;unk&gt;\",\n        mask_token: str = \"&lt;mask&gt;\",\n        additional_special_tokens: List | Tuple | None = None,\n        do_upper_case: bool = True,\n        **kwargs,\n    ):\n        if codon and nmers &gt; 1:\n            raise ValueError(\"Codon and nmers cannot be used together.\")\n        if codon:\n            nmers = 3  # set to 3 to get correct vocab\n        super().__init__(\n            alphabet=get_vocab_list(alphabet, nmers),\n            bos_token=bos_token,\n            cls_token=cls_token,\n            pad_token=pad_token,\n            eos_token=eos_token,\n            sep_token=sep_token,\n            unk_token=unk_token,\n            mask_token=mask_token,\n            additional_special_tokens=additional_special_tokens,\n            do_upper_case=do_upper_case,\n            **kwargs,\n        )\n        self.replace_T_with_U = replace_T_with_U\n        self.nmers = nmers\n        self.condon = codon\n\n    def _tokenize(self, text: str, **kwargs):\n        if self.do_upper_case:\n            text = text.upper()\n        if self.replace_T_with_U:\n            text = text.replace(\"T\", \"U\")\n        if self.condon:\n            if len(text) % 3 != 0:\n                raise ValueError(\n                    f\"length of input sequence  must be a multiple of 3 for codon tokenization, but got {len(text)}\"\n                )\n            return [text[i : i + 3] for i in range(0, len(text), 3)]\n        if self.nmers &gt; 1:\n            return [text[i : i + self.nmers] for i in range(len(text) - self.nmers + 1)]  # noqa: E203\n        return list(text)\n</code></pre>"},{"location":"zh/#_1","title":"\u4ecb\u7ecd","text":"<p>\u200b\u6b22\u8fce\u200b\u6765\u5230\u200b MultiMolecule (\u200b\u6d66\u539f\u200b)\uff0c\u200b\u8fd9\u662f\u200b\u4e00\u4e2a\u200b\u901a\u8fc7\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u52a0\u901f\u200b\u79d1\u5b66\u7814\u7a76\u200b\u7684\u200b\u57fa\u7840\u200b\u5e93\u200b\u3002MultiMolecule \u200b\u65e8\u5728\u200b\u4e3a\u200b\u5e0c\u671b\u200b\u5728\u200b\u5de5\u4f5c\u200b\u5f53\u4e2d\u200b\u4f7f\u7528\u200b AI \u200b\u7684\u200b\u7814\u7a76\u200b\u4eba\u5458\u200b\u63d0\u4f9b\u200b\u4e00\u5957\u200b\u5168\u9762\u200b\u800c\u200b\u7075\u6d3b\u200b\u7684\u200b\u5de5\u5177\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u7406\u89e3\u200b AI4Science \u200b\u662f\u200b\u4e00\u4e2a\u200b\u5e7f\u6cdb\u200b\u7684\u200b\u9886\u57df\u200b\uff0c\u200b\u6765\u81ea\u200b\u4e0d\u540c\u200b\u5b66\u79d1\u200b\u7684\u200b\u7814\u7a76\u200b\u4eba\u5458\u200b\u4f7f\u7528\u200b\u5404\u79cd\u200b\u5b9e\u8df5\u200b\u65b9\u6cd5\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0cMultiMolecule \u200b\u8bbe\u8ba1\u200b\u65f6\u200b\u8003\u8651\u200b\u4e86\u200b\u4f4e\u200b\u8026\u5408\u200b\u6027\u200b\uff0c\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u867d\u7136\u200b\u5b83\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u5b8c\u6574\u200b\u7684\u200b\u529f\u80fd\u200b\u5957\u4ef6\u200b\uff0c\u200b\u4f46\u200b\u6bcf\u4e2a\u200b\u6a21\u5757\u200b\u90fd\u200b\u53ef\u4ee5\u200b\u72ec\u7acb\u200b\u4f7f\u7528\u200b\u3002\u200b\u8fd9\u4f7f\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u4ec5\u200b\u5c06\u200b\u6240\u200b\u9700\u200b\u7ec4\u4ef6\u200b\u96c6\u6210\u200b\u5230\u200b\u73b0\u6709\u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u800c\u200b\u4e0d\u4f1a\u200b\u589e\u52a0\u200b\u4e0d\u5fc5\u8981\u200b\u7684\u200b\u590d\u6742\u6027\u200b\u3002MultiMolecule \u200b\u63d0\u4f9b\u200b\u7684\u200b\u4e3b\u8981\u200b\u529f\u80fd\u200b\u5305\u62ec\u200b\uff1a</p> <ul> <li><code>data</code>: \u200b\u9ad8\u6548\u200b\u7684\u200b\u6570\u636e\u5904\u7406\u200b\u548c\u200b\u9884\u5904\u7406\u200b\u529f\u80fd\u200b\uff0c\u200b\u4ee5\u200b\u7b80\u5316\u200b\u79d1\u5b66\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u6444\u53d6\u200b\u548c\u200b\u8f6c\u6362\u200b\u3002</li> <li><code>modules</code>: \u200b\u65e8\u5728\u200b\u63d0\u4f9b\u200b\u7075\u6d3b\u6027\u200b\u548c\u200b\u53ef\u200b\u91cd\u7528\u200b\u6027\u200b\u7684\u200b\u6a21\u5757\u5316\u200b\u7ec4\u4ef6\u200b\uff0c\u200b\u9002\u7528\u200b\u4e8e\u200b\u5404\u79cd\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u4efb\u52a1\u200b\u3002</li> <li><code>models</code>: \u200b\u4e3a\u200b\u79d1\u5b66\u7814\u7a76\u200b\u5e94\u7528\u200b\u4f18\u5316\u200b\u7684\u200b\u6700\u200b\u5148\u8fdb\u200b\u6a21\u578b\u200b\u67b6\u6784\u200b\uff0c\u200b\u786e\u4fdd\u200b\u9ad8\u6027\u80fd\u200b\u548c\u200b\u9ad8\u200b\u51c6\u786e\u6027\u200b\u3002</li> <li><code>tokenisers</code>: \u200b\u5148\u8fdb\u200b\u7684\u200b\u5206\u8bcd\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u6709\u6548\u200b\u5904\u7406\u200b\u590d\u6742\u200b\u7684\u200b\u79d1\u5b66\u200b\u6587\u672c\u200b\u548c\u200b\u6570\u636e\u8868\u793a\u200b\u3002</li> <li><code>downstream</code>: \u200b\u7528\u4e8e\u200b\u5c06\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7ed3\u679c\u200b\u65e0\u7f1d\u200b\u96c6\u6210\u200b\u5230\u200b\u5b9e\u9645\u200b\u79d1\u5b66\u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b\u548c\u200b\u5e94\u7528\u200b\u4e2d\u200b\u7684\u200b\u5de5\u5177\u200b\u548c\u200b\u5b9e\u7528\u7a0b\u5e8f\u200b\u3002</li> <li><code>utils</code>: \u200b\u4e00\u7cfb\u5217\u200b\u5b9e\u7528\u200b\u51fd\u6570\u200b\u548c\u200b\u5de5\u5177\u200b\uff0c\u200b\u7b80\u5316\u200b\u5e38\u89c1\u200b\u4efb\u52a1\u200b\u5e76\u200b\u589e\u5f3a\u200b\u6574\u4f53\u200b\u7528\u6237\u200b\u4f53\u9a8c\u200b\u3002</li> </ul>"},{"location":"zh/#_2","title":"\u5b89\u88c5","text":"<p>\u200b\u4ece\u200b PyPI \u200b\u5b89\u88c5\u200b\u6700\u65b0\u200b\u7684\u200b\u7a33\u5b9a\u200b\u7248\u672c\u200b\uff1a</p> Bash<pre><code>pip install multimolecule\n</code></pre> <p>\u200b\u4ece\u200b\u6e90\u4ee3\u7801\u200b\u5b89\u88c5\u200b\u6700\u65b0\u200b\u7248\u672c\u200b\uff1a</p> Bash<pre><code>pip install git+https://github.com/DLS5-Omics/MultiMolecule\n</code></pre>"},{"location":"zh/#_3","title":"\u8bb8\u53ef\u8bc1","text":"<p>\u200b\u6211\u4eec\u200b\u76f8\u4fe1\u200b\u5f00\u653e\u200b\u662f\u200b\u7814\u7a76\u200b\u7684\u200b\u57fa\u7840\u200b\u3002</p> <p>MultiMolecule \u200b\u5728\u200b GNU Affero \u200b\u901a\u7528\u200b\u516c\u5171\u200b\u8bb8\u53ef\u8bc1\u200b\u4e0b\u200b\u6388\u6743\u200b\u3002</p> <p>\u200b\u8bf7\u200b\u52a0\u5165\u200b\u6211\u4eec\u200b\uff0c\u200b\u5171\u540c\u200b\u5efa\u7acb\u200b\u4e00\u4e2a\u200b\u5f00\u653e\u200b\u7684\u200b\u7814\u7a76\u200b\u793e\u533a\u200b\u3002</p> <p><code>SPDX-License-Identifier: AGPL-3.0-or-later</code></p>"},{"location":"zh/models/#models","title":"Models","text":"<p><code>models</code> \u200b\u63d0\u4f9b\u200b\u4e86\u200b\u4e00\u7cfb\u5217\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u3002</p>"},{"location":"zh/models/#_1","title":"\u4f7f\u7528","text":""},{"location":"zh/models/#_2","title":"\u76f4\u63a5\u200b\u8bbf\u95ee","text":"<p>\u200b\u6240\u6709\u200b\u6a21\u578b\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b from_pretrained \u200b\u65b9\u6cd5\u200b\u76f4\u63a5\u200b\u52a0\u8f7d\u200b\u3002</p> Python<pre><code>from multimolecule.models import RnaFmModel, RnaTokenizer\n\nmodel = RnaFmModel.from_pretrained(\"multimolecule/rnafm\")\ntokenizer = RnaTokenizer.from_pretrained(\"multimolecule/rnafm\")\n\nsequence = \"UAGCGUAUCAGACUGAUGUUG\"\noutput = model(**tokenizer(sequence, return_tensors=\"pt\"))\n</code></pre>"},{"location":"zh/models/#transformersautomodel","title":"\u4f7f\u7528\u200b <code>transformers.AutoModel</code> \u200b\u6784\u5efa","text":"<p>\u200b\u6240\u6709\u200b\u6a21\u578b\u200b\u90fd\u200b\u5df2\u200b\u6ce8\u518c\u200b\u5230\u200b transformers.AutoModel, \u200b\u5e76\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u76f8\u5e94\u200b\u7684\u200b transformers.AutoModel \u200b\u7c7b\u200b\u76f4\u63a5\u200b\u52a0\u8f7d\u200b\u3002</p> Python<pre><code>from transformers import AutoModel, AutoTokenizer\n\nimport multimolecule  # noqa: F401\n\nmodel = AutoModel.from_pretrained(\"multimolecule/mrnafm\")\ntokenizer = AutoTokenizer.from_pretrained(\"multimolecule/mrnafm\")\n\nsequence = \"UAGCGUAUCAGACUGAUGUUG\"\noutput = model(**tokenizer(sequence, return_tensors=\"pt\"))\n</code></pre> <p>\u200b\u4f7f\u7528\u200b\u524d\u5148\u200b <code>import multimolecule</code></p> <p>\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u5728\u200b\u4f7f\u7528\u200b <code>transformers.AutoModel</code> \u200b\u6784\u5efa\u200b\u6a21\u578b\u200b\u4e4b\u524d\u200b\uff0c\u200b\u5fc5\u987b\u200b\u5148\u200b <code>import multimolecule</code>\u3002 \u200b\u6a21\u578b\u200b\u7684\u200b\u6ce8\u518c\u200b\u5728\u200b <code>multimolecule</code> \u200b\u5305\u4e2d\u200b\u5b8c\u6210\u200b\uff0c\u200b\u6a21\u578b\u200b\u5728\u200b <code>transformers</code> \u200b\u5305\u4e2d\u200b\u4e0d\u53ef\u200b\u7528\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u5728\u200b\u4f7f\u7528\u200b <code>transformers.AutoModel</code> \u200b\u4e4b\u524d\u200b\u672a\u200b <code>import multimolecule</code>\uff0c\u200b\u5c06\u4f1a\u200b\u5f15\u53d1\u200b\u4ee5\u4e0b\u200b\u9519\u8bef\u200b\uff1a</p> Python<pre><code>ValueError: The checkpoint you are trying to load has model type `rnafm` but Transformers does not recognize this architecture. This could be because of an issue with the checkpoint, or because your version of Transformers is out of date.\n</code></pre>"},{"location":"zh/models/#multimoleculeautomodel","title":"\u4f7f\u7528\u200b multimolecule.AutoModel \u200b\u6784\u5efa","text":"<p>\u200b\u7c7b\u4f3c\u200b\u4e8e\u200b transformers.AutoModel, MultiMolecule \u200b\u63d0\u4f9b\u200b\u4e86\u200b\u4e00\u7cfb\u5217\u200b multimolecule.AutoModel \u200b\u4ee5\u200b\u76f4\u63a5\u200b\u8bbf\u95ee\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u5305\u62ec\u200b\uff1a</p>"},{"location":"zh/models/#multimoleculeautomodelfornucleotideclassification","title":"<code>multimolecule.AutoModelForNucleotideClassification</code>","text":"<p>\u200b\u7c7b\u4f3c\u200b\u4e8e\u200b transformers.AutoModelForTokenClassification, \u200b\u4f46\u200b\u5982\u679c\u200b\u6a21\u578b\u200b\u914d\u7f6e\u200b\u4e2d\u200b\u5b9a\u4e49\u200b\u4e86\u200b <code>&lt;bos&gt;</code> \u200b\u6216\u8005\u200b <code>&lt;eos&gt;</code> \u200b\u4ee4\u724c\u200b\uff0c\u200b\u5c06\u200b\u5176\u200b\u79fb\u9664\u200b\u3002</p> <p><code>&lt;bos&gt;</code> \u200b\u548c\u200b <code>&lt;eos&gt;</code> \u200b\u4ee4\u724c\u200b</p> <p>\u200b\u5728\u200b MultiMolecule \u200b\u63d0\u4f9b\u200b\u7684\u200b\u5206\u8bcd\u5668\u200b\u4e2d\u200b\uff0c<code>&lt;bos&gt;</code> \u200b\u4ee4\u724c\u200b\u6307\u5411\u200b <code>&lt;cls&gt;</code> \u200b\u4ee4\u724c\u200b\uff0c<code>&lt;sep&gt;</code> \u200b\u4ee4\u724c\u200b\u6307\u5411\u200b <code>&lt;eos&gt;</code> \u200b\u4ee4\u724c\u200b\u3002</p> Python<pre><code>from transformers import AutoTokenizer\n\nfrom multimolecule import AutoModelForNucleotideClassification\n\nmodel = AutoModelForNucleotideClassification.from_pretrained(\"multimolecule/rnafm\")\ntokenizer = AutoTokenizer.from_pretrained(\"multimolecule/rnafm\")\n\nsequence = \"UAGCGUAUCAGACUGAUGUUG\"\noutput = model(**tokenizer(sequence, return_tensors=\"pt\"))\n</code></pre>"},{"location":"zh/models/#_3","title":"\u521d\u59cb\u5316\u200b\u4e00\u4e2a\u200b\u9999\u8349\u200b\u6a21\u578b","text":"<p>\u200b\u4f60\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u6a21\u578b\u200b\u7c7b\u200b\u521d\u59cb\u5316\u200b\u4e00\u4e2a\u200b\u57fa\u7840\u200b\u6a21\u578b\u200b\u3002</p> Python<pre><code>from multimolecule.models import RnaFmConfig, RnaFmModel, RnaTokenizer\n\nconfig = RnaFmConfig()\nmodel = RnaFmModel(config)\ntokenizer = RnaTokenizer()\n\nsequence = \"UAGCGUAUCAGACUGAUGUUG\"\noutput = model(**tokenizer(sequence, return_tensors=\"pt\"))\n</code></pre>"},{"location":"zh/models/#_4","title":"\u53ef\u7528\u200b\u6a21\u578b","text":""},{"location":"zh/models/#_5","title":"\u6838\u7cd6\u6838\u9178","text":"<ul> <li>RNABERT</li> <li>RNA-FM</li> <li>RNA-MSM</li> <li>SpliceBERT</li> <li>3UTRBERT</li> <li>UTR-LM</li> </ul>"},{"location":"zh/models/#_6","title":"\u8131\u6c27\u6838\u7cd6\u6838\u9178","text":"<ul> <li>CaLM</li> </ul>"},{"location":"zh/tokenisers/#tokenisers","title":"Tokenisers","text":"<p><code>tokenisers</code> \u200b\u63d0\u4f9b\u200b\u4e86\u200b\u4e00\u7cfb\u5217\u200b\u9884\u5b9a\u200b\u4e49\u200b\u4ee4\u724c\u200b\u5668\u200b\u3002</p> <p>\u200b\u4ee4\u724c\u200b\u5668\u662f\u200b\u4e00\u4e2a\u200b\u5c06\u200b\u6838\u82f7\u9178\u200b\u6216\u200b\u6c28\u57fa\u9178\u200b\u5e8f\u5217\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u7d22\u5f15\u200b\u5e8f\u5217\u200b\u7684\u200b\u7c7b\u200b\u3002\u200b\u5b83\u200b\u7528\u4e8e\u200b\u5728\u200b\u5c06\u200b\u8f93\u5165\u200b\u5e8f\u5217\u200b\u9988\u9001\u200b\u5230\u200b\u6a21\u578b\u200b\u4e4b\u524d\u200b\u5bf9\u200b\u5176\u200b\u8fdb\u884c\u200b\u9884\u5904\u7406\u200b\u3002</p> <p>\u200b\u8bf7\u53c2\u9605\u200b Tokenizer \u200b\u4e86\u89e3\u200b\u66f4\u200b\u591a\u200b\u7ec6\u8282\u200b\u3002</p>"},{"location":"zh/tokenisers/#_1","title":"\u53ef\u7528\u200b\u4ee4\u724c\u200b\u5668","text":"<ul> <li>RnaTokenizer</li> <li>DnaTokenizer</li> <li>ProteinTokenizer</li> </ul>"}]}